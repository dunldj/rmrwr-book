[["index.html", "Reproducible Medical Research with R Chapter 1 Preface 1.1 Who This Book is For 1.2 Prerequisites 1.3 The Spiral of Success Structure 1.4 Motivation for this Book 1.5 The Scientific Reproducibility Crisis 1.6 Features of a Bookdown electronic book 1.7 What this Book is Not 1.8 Some Guideposts 1.9 Helpful Tools", " Reproducible Medical Research with R Peter D.R. Higgins, MD, PhD, MSc 2022-04-28 Chapter 1 Preface Welcome to Reproducible Medical Research with R (RMRWR). I hope that this book is helpful to you. 1.1 Who This Book is For This is a book for anyone in the medical field interested in analyzing the data available to them to better understand health, disease, or the delivery of care. This could include nurses, dieticians, psychologists, and PhDs in related fields, as well as medical students, residents, fellows, or doctors in practice. I expect that most learners will be using this book in their spare time at night and on weekends, as the health training curricula are already packed full of information, and there is no room to add skills in reproducible research to the standard curriculum. This book is designed for self-teaching, and many hints and solutions will be provided to avoid roadblocks and frustration. Many learners find themselves wanting to develop reproducible research skills after they have finished their training, and after they have become comfortable with their clinical role. This is the time when they identify and want to address problems faced by patients in their practice with the data they have before them. This book is for you. 1.2 Prerequisites Thank you for giving this e-book a try. This is designed for physicians and others analyzing health data who are interested in pursuing this field using the R computer language. We will assume that: You have access to a computer You have access to the internet You can download and install software from the internet to your computer How to download and install R and RStudio will be addressed, step by step, in Chapter 2. 1.3 The Spiral of Success Structure This book is structured on the concept of a “spiral of success”, with readers learning about topics like data visualization, data wrangling, data modeling, reproducible research, and communication of results in repeated passes. These will initially be at a superficial level, and at each pass of the spiral, will provide increasing depth and complexity. This means that the chapters on data wrangling will not all be together, nor the chapters on data visualization. Our goal is to build skills gradually, and return to (and remind students of) their previously built skills in one area and to add to them. The eventual goal is for learners to be able to produce, document, and communicate reproducible research to their community. 1.4 Motivation for this Book Most medical providers who learn R to do their own data analysis do it on their own time. They rarely have time for a semester-long course, as their clinical schedules usually will not allow it. Fortunately, a lot of people learn R on their own, and there is a strong and supportive R community to help new learners. A 2019 Twitter survey conducted by @RLadies found that more than half of respondents were largely self-taught, from books and online resources. There are a lot of good resources for learning R, so why one more? In part, because the needs of a medical audience are often different. There are distinct needs for protecting health information, generating a descriptive Table One, using secure data tools like REDCap, and creating standard medical journal and meeting output in Word, Powerpoint, and poster formats. Further, while learning from a textbook can be helpful, this e-book has the ability to include interactive features that are important for learning to write your own analysis code. Informative flipbook demonstrations will show you what steps in R code do, and learnr exercises will give you a chance to do your own coding to solve problems, right within this e-book. More and more, all science is becoming data science. We are able to track patients, their test results, and even the individual pixels (voxels) of their CT scans electronically, and use those data points to develop new knowledge. While one could argue that health care workers should collect data and bring it to trained statisticians, this does not work nearly as well as you might expect. Most academic statisticians are incentivized to develop new statistical methods, and are not very interested (nor incentivized) to do the hand-holding required to wrangle messy clinical data into a manuscript. There also are simply not enough statisticians to meet the needs of medical science. Having clinicians on the front lines with some data science training makes a big difference, whether in 1854 in London (John Snow) or in 2014 in Flint, Michigan (Mona Hanna-Atisha). Having more clinicians with some data science training will impact medical care, as they will identify local problems that would have otherwise never reached a statistician, and probably never been addressed with data otherwise. 1.5 The Scientific Reproducibility Crisis Beginning as far back as 1989, with the David Baltimore case, and increasingly and publicly through the 2010s, there has been a rising tide of realization that a lot of taxpayer-funded science is done sloppily, and that our standards as scientists need to be higher. The line between carelessly-done science and outright fraud is a thin one, and the case can be made that doing science in a sloppy fashion defrauds the funders, as it leads to results that can not be reproduced by the authors nor replicated by others. Particularly in medicine, where incorrect findings can cause great harm, we should take special care to do scientific research which is well-documented, reproducible, and replicable. This topic as a motivating force for doing careful medical research will be expanded upon in Chapter 1. 1.6 Features of a Bookdown electronic book 1.6.1 Icons There are several icons at the top left, to the right of the clickable RMWR link, that can be helpful: 1. The Table of Contents Sidebar - Click on the ‘hamburger’ menu icon (three horizontal lines) or the s key to toggle the sidebar (table of contents) on and off. Within the sidebar, you can click on whichever chapter or subsection you want. 2. This book is Searchable - Click on the magnifying glass or use the f key to toggle the Find box and search for whatever you need to find. 3. You can change the font size, font, and background by clicking on the A icon. 4. You can download the chapter with the download icon (downward arrow into a file tray) in PDF or EPUB formats. 1.6.2 Sharing At the top right, there are several icons for sharing links to the current chapter through social media. 1.6.3 Scrolling/Paging You can scroll up and down within a chapter with your mouse, or use the up and down arrow keys. You can page through chapters with the left and right arrow keys. 1.7 What this Book is Not 1.7.1 This Book is Not A Statistics Text This is not an introduction to statistics. I am assuming that you have learned some statistics somewhere in secondary school, undergraduate studies, graduate school, or even medical school. There are lots of statisticians with Ph.D.s who can certainly teach statistics much more effectively than I can. While I have a master’s degree in Clinical Research Design and Statistical Analysis (isn’t that a mouthful!) from the University of Michigan, I will leave formal teaching of statistics to the pros. If you need to brush up on your statistics, no worries. There are several excellent (and free!) e-books on that very topic, using R. Some good examples include (go ahead and click through the blue links to explore): Learning Statistics with R (LSR) Open Intro Statistics Modern Dive Teacup Giraffes We will cover much of the same material as these books, but with a less theoretical and more applied approach. I will focus on specific medical examples, and emphasize issues (like Protected Health Information) that are particularly important for medical data. I am assuming that you are here because you want to analyze your own data in your (probably) very limited free time. 1.7.2 This Book Does Not Provide Comprehensive Coverage of the R Universe This book is also far from comprehensive in teaching what is available in the R ecosystem. This book should be considered a launch pad. Many of the later chapters will give you a taste of what is available in certain areas, and guide you to resources (and links) that you can explore to learn more and do more beyond the scope of this book. The R computer language has expanded far beyond statistics, and allows you to do many powerful things to improve your workflow, make amazing graphics, and share results with others. 1.8 Some Guideposts Keep an eye out for helpful Guideposts, which look like this: Warnings This is a common syntax error, especially for beginners. Watch out for this. Tips This is a helpful tip for debugging. Try It Out Take what you have learned and try it yourself on your own computer. Challenge - take the next step and try a more challenging example. Try this more complicated example. Explore More - resources for learning more about a particular topic. If you want to learn more about Shiny apps, go to https://mastering-shiny.org to see an entire book on the topic. 1.9 Helpful Tools Throughout this book you will find flipbook code demonstrations and learnr code interactive exercises in which you can practice writing R code right in the book. Let’s explain how to use these demonstration flipbooks and learnr exercises. 1.9.1 Demonstrations in Flipbooks Flipbooks are windows in this book in which you can watch R code being built into pipelines, and see the results at each step. Each flipbook demonstrates some important code concepts, and often new functions in R. You can click on the window to activate it, and the fullscreen (4 arrows) icon to expand it to the full screen. Then use the left and right arrow keys to go forward and back in the code, one step at a time. You will want to go through these slowly, and make sure that you understand what is happening in each step. You may even want to take notes, particularly on the function syntax, as you will likely coding exercises with these functions shortly after the flipbook demonstration. Take a look at the example of a flipbook below. Activate it by clicking on it, and use the expand icon (4 arrows at the lower right) to make it full screen. You can step forward and backward through the pipeline of code with the right and left arrow keys. Watch the results of each step. 1.9.2 Learnr Coding Exercises Learnr coding exercises are windows in this book in which you can write your own R code to solve a problem. Each learnr exercise tests whether you have mastered important code concepts, and often new functions in R. If needed, you can reset to a fresh code window with the Start Over button. You can type lines of code into the window, then click on the Run Code button at the top right to run the code and get your results. Your code may not produce the right result the first time, and you will have to interpret the error message to figure out how to fix it. Rely on the text and your notes and the demonstrations to help you. If you are stuck, you can click on the Hint button to see an example of correct code, and compare it to your own. If you would like, you can even copy this code to the clipboard with the Copy button and Take a look at the example of a learnr exercise below. There is a dataset piped into a series of functions (‘verbs’), with a blank. Fill in the blank with ‘p_vol’ (without the quotes), which stands for the variable, prostate volume. Then run your code with the Run Code button to get a result. Practice using the Start Over button, the Hint button (there may be more than one - usually the last one is the solution), and the Copy To Clipboard button. When you get a table of data as a result from a code pipeline, it may have more columns (variables) than can be displayed easily. When this is the case, there will be a black arrow pointing rightward at the top right of the table of results. Click on this to scroll right and see more columns. A table of data as a result from a code pipeline may also have more rows (observations) than can be displayed easily. When this is the case, the table will be paginated, with 10 rows per page. At the bottom right of the table, there will be a clickable listing of pages, along with Previous and Next buttons. Click on these buttons (or the page number buttons) to see more pages of data to inspect and check your results. 1.9.3 Coding An important note on writing your own code: you should always have an internet search window open when you are writing code. No one can remember every function, nor the correct arguments and syntax of each function. A critical skill in writing code is searching for how to do something correctly. This is not a sign of weakness. Professional programmers google “how do I do x?” many times each day. This is how programming is done. You will often search for things like “how do I do x in R?” or “how to x in tidyverse”. This is completely normal, and to be expected. You do not have time to memorize hundreds of functions, and you may have days or even weeks between coding sessions (because of your day job), making it hard to remember all the details from your last coding session. This is not a problem. There are lots of websites that can help you solve specific problems, as you will find in the How to Find Help chapter. "],["getting-started-and-installing-your-tools.html", "Chapter 2 Getting Started and Installing Your Tools 2.1 Goals for this Chapter 2.2 Website links needed for this Chapter 2.3 Pathway for this Chapter 2.4 Installing R on your Computer 2.5 Windows-Specific Steps for Installing R 2.6 Mac-specific Installation of R 2.7 Installing RStudio on your Computer 2.8 Installing Git on your Computer 2.9 Getting Acquainted with the RStudio IDE", " Chapter 2 Getting Started and Installing Your Tools One of the most intimidating parts of getting started with something new is the actual getting started part. Don’t worry, I will walk you through this step-by step. 2.1 Goals for this Chapter Install R on your Computer Install RStudio on your Computer Install Git on your Computer Get Acquainted with the RStudio IDE 2.2 Website links needed for this Chapter While in many chapters, I will list the R packages you need, in this chapter, you will be downloading and installing new software, so I will list the links here for your reference. https://www.r-project.org https://rstudio.com/products/rstudio/download/ https://git-scm.com/downloads 2.3 Pathway for this Chapter This Chapter is part of the TOOLS pathway. Chapters in this pathway include Getting Started and Installing Your Tools Using the RStudio IDE Updating R, RStudio, and Your Packages Advanced Use of the RStudio IDE When You Don’t Want to Update Packages (Using renv) Major R Updates (Where Are My Packages?) 2.4 Installing R on your Computer R is a statistical programming language, designed for non-programmers (statisticians). It is optimized to work with data in rectangular tables of rows (observations) and columns (variables). It is a very fast and powerful programming engine, but it is not terribly comfortable or convenient. R itself is not terribly user-friendly. It is a lot like a drag racing car, which is basically a person with a steering wheel strapped to an airplane engine. drag racer Very aerodynamic and fast, but not comfortable for the long run (more than about 8 seconds). You will need something more like a production car, with a nice interior and a dashboard, and comfy leather seats. dashboard This equivalent of a comfy coding environment is provided by the RStudio IDE (Integrated Developer Environment). I want you to install both the R statistical language and the RStudio IDE, in that order. Let’s start with installing R. R is free and available for download on the web. Click on the following link to go to the r-project website to get started. This screen will look like this You can see from the blue link (download R) that you can use this link to download R, but you will be downloading it faster if you pick a local CRAN mirror. You might be wondering what CRAN and CRAN Mirrors are. Nothing to do with cranberries, fortunately. CRAN is the Comprehensive R Archive Network. Each site (mirror) in the network contains an archive of all R versions and packages, and the sites are scattered over the globe. A CRAN Mirror maintains an up to date copy of all of the R versions and packages on CRAN. If you use the nearest CRAN mirror, you will generally get faster downloads. At this point, you might be wondering what a package is… A package is a set of functions and/or data that you can download to upgrade and add features to R. It is like a downloadable upgrade to a Tesla vehicle that lets you play the video game Witcher 3 on your console, but more useful. Another useful analogy for packages is that they are like apps for a smartphone. When you buy a new smartphone, it only comes with the basic apps that allow it to work as a phone, and a few other things, like a notepad and a calculator. If you want to do cool things with your smartphone, you download apps that allow your smartphone to have new capabilities. That is what packages do for your installation of R. Now let’s get started. Click on the blue link that says “download R”. This will take you to a page to select your local CRAN Mirror , from which you will download R. cran Scroll down to your local country (yes, the USA is at the bottom), and a CRAN mirror near you. This is an example from the state of Michigan, in the USA. usa-mirrors Once you click on a CRAN Mirror site to select the location, you will be taken to the actual Download site. install Select the link for the operating system you want to use. We will walk through this with Windows first, then Mac. If you are using a Mac, skip forward to the Mac install directions in section 2.6. If you are computer-savvy enough to be using Linux, you can probably figure it out on your own (it will look a lot like these). 2.5 Windows-Specific Steps for Installing R If you are installing R on a Mac, jump ahead to the Mac-specific version below in section 2.6. On Windows, once you have clicked through, your next screen will look like this: install2 You want to download both base and Rtools (you might need Rtools later to build packages). The base link will take you to the latest version, which will look something like this. install3 Click on this link, and you will be able to save the latest version to a file named R-N.N.N-win.exe (N values vary depending on the latest version number, recent versions look like R-4.1.2-win.exe) to your Downloads folder. Click on the Save button to save it. install4 Now, go to your Downloads folder in Windows, and double click on the the latest version of the R installation file (something like R-N.N.N-win.exe. Recent versions have looked something like R-4.1.2.exe). Click Yes to allow this to install. install5exe Now select your language option. install_language You will be asked to accept the GNU license - do so. Click Yes to allow this to install. Then select where to install - generally use the default- a local (often C) drive - we usually do not install on a shared network drive or in the cloud. install_drive Then select the Components - generally use the defaults, but newer computers can skip the 32 bit version. install_components In the next dialog box, accept the default startup options. install_defaults You can choose the start menu folder. The default R folder is fine. install_start If you want a shortcut icon for R on your desktop, you can leave this checked. But most people start RStudio, with R running within RStudio, rather than directly starting R. You can choose to create an R shortcut on your desktop in the next dialog box. You can delete it later if you don’t like it. install_addltasks Then the Setup Wizard will appear - click Finish, and the rest of the installation will occur. install_wizard 2.5.1 Testing R on Windows Now you want to test whether your Windows installation was successful. Can you find R and make it work? This is easy if you chose to make an R shortcut. If not, hunt for your C folder, then for OS-APPS within that folder. Keep drilling down to the Program Files folder. Then the R folder, and the current version folder within that one (R-N.N.N). Within that folder will be the bin folder, and within that will be your R-N.N.N.exe file. Double click on this to run it. The example paths below can help guide you. install_path2 install_path Opening the exe file will open a classic year 2000-era terminal window, called Rterm, with 64 bit if that is what your computer uses. The version number should match what you downloaded. The messaging should end with a “&gt;” prompt. install_term At this prompt, type in: paste(“Two to the seventh power is”, 2^7) (don’t leave out the comma or the quotes) - then press the Enter key. This should produce the following: Two to the seventh power is 128 install_test Note that you have explained what is being done in the text, and computed the result and displayed it. 2.6 Mac-specific Installation of R The installation for Mac is very similar, but the windows look a bit different. If you are working with Windows, jump ahead at this point to Installing RStudio in section 2.7. At the Download Version page, you click on the Mac Download. You will then click on the link for the latest version of R, which will look something like R-N.N.N.pkg (recent versions have been something like R-4.1.2.pkg), and allow downloads from CRAN. install_path Then go to Finder, and navigate to the Downloads folder. Click on R-N.N.N.pkg You will then click on the link for R-N.N.N.pkg, and allow downloads from CRAN. install_downloadmac Click on Continue on 2 consecutive screens to download cont1_mac cont2_mac Then you need to agree with the License Agreement, mac_license then Click on Install, and provide your Mac password for permission to install. cont1_mac When the installation is complete, click on the Close button. Accept the prompt to move the installer file to the trash. 2.6.1 Testing R on the Mac If you chose to create an R desktop shortcut, double-click on this to start. If not, go to Finder, and then your Applications folder. Scroll down to the R file. Double click on this to run it. findrmac You should get this 2000-era terminal window named R Console. The version number should match what you downloaded, and the messaging should end with a “&gt;” prompt. At this prompt, type in paste(“Two to the seventh power is”, 2^7) (DON’T leave out the comma or the quotes) rconsolemac This should result in mactestR 2.6.2 Successful testing! Awesome. You are now Ready to R! ready2R 2.7 Installing RStudio on your Computer Now that R is working, we will install RStudio. This is an IDE (Integrated Development Environment), with lots of bells and whistles to help you do reproducible medical research. teslax_dash This is a lot like adding a dashboard with polished walnut panels, a large video screen map, and heated car seats with Corinthian Leather. Not absolutely necessary, but nice to have. The RStudio IDE wraps around the R engine to make your experience more comfortable and efficient. camry_dash Fortunately, RStudio is a lot cheaper than any of these cars. In fact, it is free and open source. You can download it from the web at: rstudio Click on the RStudio Desktop icon to begin. download This will take you to a new site, where you will select the Open Source Edition of RStudio Desktop open_source This will take you to a new site, where you will select the Free Version of RStudio Desktop free Now select the right version for your Operating syxtem - Windows or Mac. 2.7.1 Windows Install of RStudio If you are installing on a Mac, jump ahead now to the Mac-specfic installation instructions. Now save the RStudio.N.N.N.exe file (Ns will be digits representing the version number) to your downloads folder. winsave Now go to your downloads folder, and double click on the RStudio.N.N.N.exe file. winlaunch Allow this app to make changes. Click Next to Continue, and Agree to the Install Location. wininstall Click Install to put RStudio in the default Start Menu Folder, and when done, click the Finish button. winsave winfinish Now select your preferred language option, accept the GNU license, Click Yes to allow this to install. Select where to install. This is generally on a local (often C:) drive, and usually not a shared network drive or in the cloud. 2.7.2 Testing Windows RStudio Now you should be ready to test your Windows installation of RStudio. Open your Start menu Program list, and find RStudio. Pin it as a favorite now. Click to Open RStudio. Within the Console window of RStudio, an instance of R is started up. Check that the version number matches the version of R that you downloaded. Now run a test at the prompt (“&gt;”) in the Console window. Type in paste(\"Three to the 5th power is\", 3^5) do not leave out the quotes or the comma Then press the enter key and this should be your result: test_result35 A successful result means that you are ready to roll in RStudio and R! 2.7.3 Installing RStudio on the Mac Start at this link: RStudio Download Select the Free RStudio Desktop Version mac_download Then click on the big button to Download RStudio for Mac. mac_download2 After the Download is complete, go to Finder and the Downloads Folder. Double click on the RStudio.N.N.N.dmg file in your Downloads folder. mac_dmg This will open a window that looks like this mac_apps Use your mouse to drag the RStudio icon into the Applications folder. Now go back to Finder, then into the Applications folder. Double click on the RStudio icon, and click OK to Open. Pin your RStudio to the Dock. Double Click to run RStudio. RStudio will open an instance of R inside the Console pane of RStudio with the version number of R that you installed, and a “&gt;” prompt. 2.7.4 Testing the Mac Installation of RStudio Type in paste(\"Three to the 5th power is\", 3^5) do not leave out the quotes or the comma Then press the enter key and this should be your result. test_result35 A successful result means that you are ready to roll in RStudio and R! ready 2.7.5 Critical Setup - Tuning Your RStudio Installation You now have ~ 7 adjustments that you need to make in your RStudio Global Settings for optimal R and RStudio use. At this point, it is a good idea to jump out of RStudio and create an “Rcode” folder on your computer, in a place that is easy to find, often at the top level in your Documents folder, to make all of your future projects easy to find. Once this Rcode folder is in place, switch back to RStudio. In the top bar of RStudio Menus, go to Tools/Global Options. A new Global Options window will open up. Click on the General tab on the left. At the top, there is a small window for identifying your Default working directory. Click on the Browse button, and browse to your new “Rcode” folder and select it. From now on, your R files and Projects will all be in one place and easy to find. In the same General tab, de-select the first 3 options turn off Restore most recently opened project at startup turn off Restore previously open source documents at startup turn off Restore .RData into workspace In the same General tab, find Save workspace to .RData on exit. Click on the dropdown menu to select “Never” These tune-ups (#2 and #3) to your RStudio will mean you will always start with a clean workspace in a new RStudio session, which will avoid a lot of potential problems later. In the same General tab, at the top, click on the Advanced tab. Then select the box for Show full path to project in window title This will show your working directory at the top of your Console Pane. This can prevent confusion and problems later. On the left, click on the Rmarkdown tab. Then de-select the option for Show output inline for all Rmarkdown documents. This will put your temporary output from Code Chunks into the larger and nicer Viewer tab. Take a look at the Appearance tab. You can change your code font, the font size, and the theme. I wouldn’t make any drastic changes at this point, but it is good to know that these options are available. Any changes here are entirely optional (and cosmetic) at this point. in the RStudio menus, select Tools/Global Options/Code, then check/select two options to turn these on: In the Editing tab - select Soft Wrap Long Lines - so that your code does not get too wide In the Display tab - select Rainbow Parentheses - this option color-codes parentheses so that you can keep track of whether you have closed all of your open parentheses (a common source of errors). When your last close parenthesis is red, all is well. Now your RStudio installation is tuned and ready to go! 2.8 Installing Git on your Computer The software program, git, is a version control system. It is the most common version control system in the world. It is free and open source, and is the foundation of reproducible computing. We won’t be doing a lot with git just yet, but it is helpful to get this installation done and out of the way. It will come up a lot when we start to discuss reproducible research and collaboration. 2.8.1 Installing Git on macOS If you are using Windows, jump ahead to Installing Git on Windows. The easiest approach on the macOS is to go to the Terminal tab in the Console pane (lower left) in RStudio. A prompt will appear that ends in a $. At that prompt, type git --version note that there are 2 dashes before version. This will tell you the current version of git (2.29.2 as of January 1, 2021), or prompt you to install git. If you want the current version of git, you can install this yourself. a. First, let’s check if you have homebrew installed. Go to the Terminal tab in the Console pane (lower left) in RStudio. A prompt will appear that ends in a $. at the prompt, type command -v brew This should return “/usr/local/bin/brew” if homebrew is installed, or will tell you “brew not found” or something similar. b. Installing homebrew At the terminal prompt($), paste in the following: /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; Then press Enter to run it. This installs the homebrew program, which allows you to install software on macOS that does not come from the Apple App Store This will take a couple of minutes. c. Installing git Once you have homebrew installed, installing git is straightforward. At the Terminal prompt (the prompt looks like a dollar sign $), type brew install git and this will quickly install. You will be returned to the Terminal prompt ($) when installation is complete. Check your installation. At the Terminal prompt ($), type git --version and this should return a result like “git version 2.29.2”, depending on the most recent version number. 2.8.2 Installing Git on Windows If you are using Windows, go to the website, https://git-scm.com/download/win. This will start the download automatically Go to your downloads folder and install the downloaded .exe file by clicking on it Check your installation. At the Terminal prompt ($), type git --version and this should return a result like “git version 2.29.2”, depending on the version number. 2.8.3 Installing Git on Linux If you are using Fedora, or a related version of Linux like RHEL or CentOS, use dnf At the $ prompt, type sudo dnf install git-all If you are using a Debian-based version of Linux like Ubuntu, use apt At the $ prompt, type sudo apt install git-all For other distributions of Linux, follow the instructions at https://git-scm.com/download/linux. Check your installation. At the Terminal prompt ($), type git --version and this should return a result like “git version 2.29.2”, depending on the version number. 2.9 Getting Acquainted with the RStudio IDE When you first open the RStudio IDE (Integrated Development Environment), there will be a left side pane, with tabs for Console, Terminal, and Jobs. This is called the Console pane. Just for fun, go to the RStudio menus, and choose File/New File/RScript. This will open a new pane at the top left, which we will call the top left pane, or the Source pane. This pane will contain tabs for each active script or document, along with tabs for any datasets you have opened up to have a look at. The Console pane, with tabs for Console, Terminal, and Jobs, has now been pushed to the lower left quadrant. You will use the Console pane for interactive programming, and as a “sandbox” to test out new code. When your code works and is good enough to save, you will move it to the Source pane and save it to a Script or an Rmarkdown document. Any code that is not saved to Source will be lost (actually it will be somewhere in the History, but it can be a pain to find the version that works later - it is best to save the good stuff to an R Script or .Rmd (rmarkdown) document). The top right pane includes tabs for your Environment (objects, like datasets, functions, and variables you have defined), History (saving the past in case you forget to, but messy), and Connections tabs for connections to databases. Later a Git tab will be added for version control (backup) of your Source documents. The bottom right pane contains tabs for your Files, Plots, Packages, Help, and a Viewer for HTML output. This is material that is also well described in the “Basic Basics 1” section of RLadiesSydney. Check it out at Basic Basics 1. There is a nice ~ 15 minute video by Jen Richmond worth watching if you are just getting started. Note that a lot of the other material on this website (RYouWithMe) is very helpful for getting started with R. For a more complete introduction to the RStudio IDE, see Chapter TBD, titled Using the RStudio IDE. "],["a-tasting-menu-of-r.html", "Chapter 3 A Tasting Menu of R 3.1 Setting the Table 3.2 Goals for this Chapter 3.3 Packages needed for this Chapter 3.4 Website links needed for this Chapter 3.5 Setting up RPubs 3.6 Open a New Rmarkdown document 3.7 Knitting your Rmarkdown document 3.8 Your Turn to Write Text 3.9 Wrangle Your Data 3.10 Summarize Your Data 3.11 Visualize Your Data 3.12 Statistical Testing of Differences 3.13 Publish your work to RPubs 3.14 The Dessert Cart", " Chapter 3 A Tasting Menu of R In this chapter, we will introduce you to a lot of neat things that you can do with R and RStudio, and you will publish a simple data analysis on the Internet that you can share with friends and family. 3.1 Setting the Table In this chapter, you will get a rapid overview of the kind of things you can do with R. You will create an Rmarkdown document read in data wrangle your data create a data visualization and publish your findings. At the end of this chapter, you will publish your data analysis to RPubs, a free website site where you can share your data analyses and visualizations. 3.2 Goals for this Chapter Set up an RPubs account Open a New Rmarkdown document Read in Data from a file Wrangle Your Data Visualize Your Data Publish your work to RPubs Check out Interactive Plots Check out Animated Graphics Check out a Clinical Trial Dashboard Check out a Shiny App 3.3 Packages needed for this Chapter In this chapter, we will use the following R packages: tidyverse janitor rstatix medicaldata If you have not installed these packages on your computer already, we will walk you through installation after you open your Rmarkdown document (see below). 3.4 Website links needed for this Chapter In this chapter, you will need to access the RPubs website. https://rpubs.com/ 3.5 Setting up RPubs First you will need to set up a free account on RPubs. Start by opening a new tab in your browser, and navigating to this link RPubs link. It should look like the image below. Enter your name, email, username and password, (remember these, you will need them later) and click on the Register Now button, and you will be set up to use RPubs . This will bring you to this page. In the image below, we have set up an account for pdr. Click on the Here’s How You Get Started link (blue text). You are now all set up and ready to go. Now you have a place on the internet to share your R creations. On to the creation part! 3.6 Open a New Rmarkdown document Let’s get started in R. Turn on your computer, and open the RStudio application. You should see the familiar panes for the Console, Environment, and Files. You need to open up a new document to activate the Source pane. While in RStudio, click on File/New File/RMarkdown. It should look like this. Opening a New Rmarkdown Document Now you will see the window below. Rename the document from “Untitled” to “Tasting”. Enter your own name as the Author, and click the OK button. Adding Title and Author Now the file is open, and looks like the window below (with title “Tasting” and author “Peter Higgins”. Click on the save icon (like a floppy disk in the top left), and save this document as my_data_analysis.Rmd. What Your New Rmarkdown Document Looks Like You have created a new Rmarkdown document. An Rmarkdown document lets you mix data, code, and descriptive text. It is very helpful for presenting and explaining data and visualizations. An Rmarkdown document can be converted (Knit) to HTML for a web page, or to Microsoft Word, Powerpoint, PDF, and several other output formats. artwork by Allison Horst, @allison_horst Code chunks are in a gray color, and both start and end with 3 backticks (```), like this. code goes here Text can be body text, or can be headers and titles. The number of hashtags before some header text defines what level the header is. You can insert links, images, and even YouTube videos into Rmarkdown documents if it is helpful to explain your point. You can change the way that the Rmarkdown document is displayed with two buttons in the top right of the new Tasting document tab. The button with several horizontal lines that looks like an outline can be clicked to toggle an outline pane on and off. When the outline pane is on, you can click on the entries to go to a different part of your document quickly. The button that looks a bit like an Angstrom symbol (A with a circle on top), or perhaps it looks like a compass to you, allows you to turn on Visual Editing, or WYSIWIG (what you see is what you get) visual editing. When this is off, you can see the markdown codes that make the words “R Markdown” a level 2 header (right after the setup chunk), with two hashtags right before the text. But when the Visual Editing button is turned on, you can see the formatting as it will appear in the output document, and a new formatting bar appears at the top of the document below the Knit button. This formatting bar adds the ability to select text and change the heading level (top left), or make text bold, italic, underlined, etc. The formatting bar also lets you insert images, links, and tables into your document, much like you would with a word processor. The first code chunk in each Rmarkdown document is named setup. Find this code chunk in your “Tasting” document. The code chunk name (in this case, setup) comes after the left curly brace and the r ({r) at the beginning of the each code chunk. The letter r tells RStudio that what is coming on the next line is R code (RStudio can also use SQL, C++, python, and several other commputer languages). After a comma, you can define options for this code chunk. In the case of this setup chunk, the option include is set to FALSE, so that when this Rmarkdown document is knitted, this code chunk will run, but no output or warnings or messages will appear in the output document. 3.7 Knitting your Rmarkdown document While this is just an example template, you can see that there is some explanatory text, some formatting, and two code chunks. One code chunk has the option, echo = FALSE, which means that the code in that code chunk will not appear in the output document, but the results of the code chunk will appear. To see what the output of this Rmarkdown document looks like, click on the Knit button - this is at the top center of your Tasting document next to a blue ball of yarn with a knitting needle. When you knit the Rmarkdown document, the default result is an HTML output document. Notice that you can see the code for the cars code chunk, but that the setup code chunk and pressure code chunk do not appear. But you do get results: a summary of the cars dataset and a plot of the pressure data. You also have the option of knitting Rmarkdown documents to other kinds of output, including Microsoft Word, Microsoft Powerpoint, posters for medical meetings, books (like this book), and pdf documents. 3.7.1 Installing Packages Before we begin working on your Rmarkdown document, you will need to install a few R packages on your computer. Go to your Console tab (lower left in RStudio), and type in (or copy and paste in) the following 5 lines: install.packages(&#39;tidyverse&#39;) install.packages(&#39;janitor&#39;) install.packages(&#39;rstatix&#39;) install.packages(&#39;remotes&#39;) remotes::install_github(&#39;higgi13425/medicaldata&#39;) Press Enter to run these functions. These will install the 5 packages, {tidyverse}, {janitor}, {rstatix}, {remotes}, and {medicaldata}. Installing packages is like buying apps for your phone. But these apps are not loaded into your current R session unless you tell R and RStudio that you want them loaded in the current session. You do this with the library() function. 3.7.2 Loading Packages with library() Copy and paste to add the following 5 lines to your setup chunk in your “Tasting.Rmd” Rmarkdown document: library(tidyverse) library(janitor) library(rstatix) library(medicaldata) prostate &lt;- medicaldata::blood_storage %&gt;% clean_names() These functions will load 4 packages and read in data from a study of prostate cancer and blood storage into the prostate object. Now run these functions, by clicking on the green rightward arrow at the top right of the setup code chunk in your Rmarkdown document. The installation of the {tidyverse} package (it is actually a meta-package that contains multiple packages) will be quite chatty, telling you which packages are being attached, and when conflicts with identically-named functions in the {stat} package have occurred. When you call the functions, filter() and lag(), the versions from the {tidyverse} package will be used by default, and the versions from the {stats} package will be masked. The {janitor} package will tell you that it has 2 conflicts with the {stats} package, and will supercede (mask) the {stats} functions for chisq.test() and fisher.test(). If you really want to access the versions from the {stats} package, you can do so by using the package::function construction, e.g. stats::chisq.test(), which tells R that you want to use the version of the function from the {stats} package, rather than the {janitor} package. If you check the Environment tab in the top right pane of RStudio, you will find that you now have a prostate object under the Data header. You can click on the white-on-blue arrow to the left of the word prostate to get an overview of each variable, the variable type (numeric, string, etc.), and the first few values of each variable. You can also click on the word prostate in the Environment window to open up a View of the whole dataset in the Source pane (top left). You can scroll up and down the rows, or right and left in the columns to inspect the data. If you check the Console tab (lower left), you will see that when you clicked on prostate, this sent a function to the console to View(prostate). You can view any dataset in the Environment tab with this function. You can also look at your data in the Console, by running (type in and press Enter for each line) glimpse(prostate) summary(prostate) to provide some information on the contents of the prostate dataframe, and a summary of the data. 3.8 Your Turn to Write Text Underneath the setup chunk, change the text of the first header (“R Markdown”) to “Analysis of Prostate Data”. Now delete the next two paragraphs of Normal text and write something about the prostate dataset, based on the summary in the console . You write body text for your documents in Normal text, and you can add new headers by starting a line with 2 hashtags, a space, and text like this ## Headline about Prostate data Write a few sentences after your heading. You can add italics or bold text by wrapping the text to be highlighted in underscores or 2 asterisks, respectively. If you are using the Visual Editor mode, you can do these things more easily by selecting your text, and clicking on the bold or italic icons in the formatting bar. You can also change a line of text to a header by selecting it, then clicking the dropdown arrow next to the word “Normal” at the top left of the formatting bar. You can make this into one of 6 levels of Headings, or a code chunk. We often reserve the level 1 Heading for the title. Try adding some text and formatting to your text. 3.9 Wrangle Your Data Find the {r cars} code chunk. Edit the name “cars” to “wrangle” Delete the one line of code from the template - “summary(cars)” We will replace this with a few lines of code to improve the data in the prostate dataset. We will be modifying the prostate dataset (with the mutate function), particularly the variables aa and fam_hx into properly labeled categorical variables, called factors in R. Then we will save (assign) the result to the prostate_factors object, while retaining the previous version in the prostate object. Copy the lines of code below and paste them into the new wrangle code chunk. You can see in the code that we start with the prostate dataset, and then (the pipe symbol [%&gt;%] is read as “and then”) mutate the aa variable to a labelled factor, and then mutate the fam_hx variable to a labelled factor. Then the resulting dataframe is assigned to the new prostate_factors object. This version of the data will be helpful for plotting later. prostate %&gt;% mutate(aa = factor(aa, levels = c(0,1), labels = c(&quot;White&quot;, &quot;African-American&quot;))) %&gt;% mutate(fam_hx = factor(fam_hx, levels = c(0,1), labels = c(&quot;No Family History&quot;, &quot;FHx of Prostate Cancer&quot;))) -&gt; prostate_factors Note that R will not ask you if you want to over-write an object. This is just a reminder to be careful when you assign data to an object. You don’t want to re-use an object name (like prostate) and inadvertently over-write your previous work. It is fine if this is what you intended, but make sure it is that this is you want to do. It is generally a good practice to assign data to well-named objects, so that you know what they are, where they came from, and how they have changed since the last data wrangling iteration. It is generally not a good idea to over-write your data. 3.10 Summarize Your Data Now you will Insert a new R code chunk, after the wrangle chunk. First, click with your mouse to place your cursor on a blank line below the wrangle chunk. Now open a new code chunk. To do this, find the green code (C) button in the menu bar at the top of your Rmarkdown document. Click on it and select R as the language being used. You will get a gray code chunk with the {r} label at the top. Insert your cursor after the r, and before the closing brace. Add a space, then type the name, summarize. In this chunk, we will run some code to summarize three variables. Paste the four lines of code below into your new code chunk. prostate %&gt;% select(age, p_vol, preop_psa, aa, fam_hx) %&gt;% group_by(aa, fam_hx) %&gt;% summarize(across(age:preop_psa, mean, na.rm=TRUE)) This code starts with the prostate dataset, and then selects 5 of the variables. Then it groups the observations by African-American race and Family history of prostate cancer. Then it summarizes across 3 variables to get the mean value of each one (after removing any missing values). Try this out by clicking on the rightward green arrow at the top right of your summarize code chunk. This should produce a summary table of results for age, prostate volume, and preoperative PSA. Add some body text below the code chunk, with your interpretation of these results, and some hypotheses about these summary results, including the contrasts by family history and race. Depending on the sample size, some of these differences might be statistically significant. Which ones would you like to test? 3.11 Visualize Your Data Now let’s plot the prostate_factors data. Change the name of the previous plot chunk to “visualize”. Delete the one line of code from the example. We will produce a scatterplot, faceted by African-American race and Family History. Copy and paste the code below into your Rmarkdown document in this visualize code chunk. ggplot(prostate_factors) + aes(x = p_vol, y = preop_psa, col = aa) + geom_point() + geom_smooth(method = &quot;lm&quot;) + facet_grid(aa ~ fam_hx) + labs(x = &#39;Prostate Volume&#39;, y = &quot;Preoperative PSA&quot;, title = &#39;Relationship Between Prostate Volume and Preop PSA,\\nSubdivided by Family History and Race&#39;) + theme(legend.position = &quot;bottom&quot;) This will run the code to generate a plot. Note that these steps for a ggplot are connected with + signs, while the data wrangling steps are connected with the pipe ( %&gt;% ) symbol. Let’s walk through each step of the code. First you are stating the dataset to plot, with ggplot(prostate), and then the aesthetic mappings are stated, with p_vol mapped to x, preop_psa mapped to y, and the aa variable mapped to color. This all happens in the aes(x = p_vol, y = preop_psa, col = aa) step. Then these are plotted as scatterplot points with geom_point, and linear regression lines are added with geom_smooth(method = \"lm\") (method = “lm” is for linear model). Then the plot is faceted (broken into comparison plots) by the aa and fam_hx variables with facet_grid(aa ~ fam_hx). Then labels for x and y and the title are added. Then a theme option is used to move the legend position to the bottom. You can run the visualize code chunk and see the plot by clicking on the green arrow at the top right of the code chunk. Do you think that the slopes of the linear regression lines are all the same? Are there differences or patterns? Add some text below your plot stating your interpretation of these plots, and any hypotheses generated by this visualization of the data. 3.12 Statistical Testing of Differences Based on the previous summary, it looks like the African-American patients in this dataset may have, on average, a higher preoperative PSA level. We can test this with Student’s t test. Insert a new header for Statistical Testing (or T testing). Make sure it is in the Heading 2 format. Then insert some Normal text in which you state your hypothesis. Then insert a code chunk (green C button, select the R language). Name this new code chunk “t-test”. Paste in the three lines of code below to get the results of a t test comparing preop_psa levels between African-Americans and Whites in this dataset. prostate_factors %&gt;% t_test(formula = preop_psa ~ aa, detailed = TRUE) Now write some body text below the results, with your interpretation for this result in this dataset and sample size. 3.13 Publish your work to RPubs Now you have a complete data analysis, including data wrangling, summarizing, plotting, and statistical testing. Your report combines code, results, graphics, and text introduction of the questions, and text interpretation of the results. Go ahead and Knit this Rmarkdown document to see the final result. Use the Knit button (blue ball of yarn with knitting needle) near the top left of your Rmarkdown document. Now you can share this document with others, by publishing your results and interpretation on the web. Now that you have a knitted HTML document, look for the Publish button at the top right of the HTML output (it looks sort of like a blue eye, or two blue semicircles wrapped around a blue dot). Click on this and select RPubs. The knitted HTML document will now be uploaded to RPubs, and your result can be found at https://rpubs.com/username, based on your RPubs username. You may have to click on the username dropdown at the top right to reach View Profile, which will show the documents that you have published. Click on the thumbnail of your new document to go to the full link and see it in its full HTML glory. This page will have a specific link, which will be something like at [[https://rpubs.com/username](https://rpubs.com/username/123456)](https://rpubs.com/username)/123456 , which is a dedicated link you can share with other people who want to see your results. You can share this link via email, and anyone (collaborators, mentors, friends) can see your results report from anywhere with a web connection. You did it! You should feel like an Rmarkdown rock star! artwork by Allison Horst, @allison_horst You can also knit Rmarkdown documents to: Microsoft Word (1st draft of manuscript) Microsoft Powerpoint (1st draft of presentation) Posterdown (a poster-making package - for meeting posters) so that you don’t need to copy/paste your results or plots, and you can easily re-run your analyses and produce new outputs if you get more data. This chapter should give you a taste of the powerful tools for research reproducibility in R that can make your research work more efficient and more reproducible. 3.14 The Dessert Cart Below are some examples of more neat things you can do with medical data in R. These are more advanced approaches, but completely doable when you have more experience with R. 3.14.1 Interactive Plots Below is an interactive plot. Click on the plot to activate it. Then you can hover your mouse over each point to get more details about the data. You can also use the crosshairs to draw a zoom rectangle, or use the plotly menu bar at top right to zoom in or out of a region of the data. 3.14.2 Animated Graphics Here is an example of animated graphics that you can create in R to illustrate changes in data over time. 3.14.3 A Clinical Trial Dashboard Below is an screen capture picture of a web flexdashboard to track the data in an ongoing clinical trial (which is now completed and published). You can see the actual web dashboard here. Check out the various tabs. Imagine how useful it would be to track enrollment, exclusions, missing data, and outcomes in real time. Details on how this is done can be found here, and the underlying code here. All of this work was done in R by Jenn Thompson. 3.14.4 A Shiny App The frame below shows a publicly available Shiny web application, built with R, which can help clinicians calculate the probablity of intestinal TB vs. Crohn’s disease with available clinical data. And to determine how new test results would change this estimate. The web app can be accessed here. 3.14.5 An Example of Synergy in the R Community One of the remarkable things about the open source R community is that people build all kinds of new R functions and packages that are useful to them, and then share them publicly with tools like Github so that they can be useful to others. Often combining bits of several packages leads to emergent properties - completely new creations that can only occur because all of the parts (packages) are present. The collaborative nature of the R community, in this case on Twitter (follow the #rstats hashtag), can lead to surprising collaborations and outcomes. Go ahead and play the example below, which uses rayrendering (all coded entirely in R) to show a 3D map of John Snow’s cholera case data in 1854, which led him to identify the Broad Street water pump as the source of the cholera outbreak, and led to the removal of the pump handle and the end of outbreak. If you are not familiar with John Snow and the Broad Street pump, there is a fun series of YouTube animations (parts 1-3 and an epilogue) to explain the history. Start by clicking here. "],["introduction-to-reproducibility.html", "Chapter 4 Introduction to Reproducibility 4.1 First Steps to Research Reproducibility", " Chapter 4 Introduction to Reproducibility One of the major goals of this book is to help medical researchers make their research projects reproducible. This means being able to take the original raw data, and replicate each step of the data cleaning and analysis to produce the same end products, including tables, figures, and the full manuscript. This requires: Robust sharing of data, materials, software, and code Documenting the data collection and data analysis processes Using authenticated biomaterials, and validated measurement instruments Using valid statistical methods and study design Pre-registration of studies, including detailed protocols and statistical plans, to avoid p-hacking and HARK-ing (Hypothesizing After the Results are Known (aka post-hoc analyses)) Publishing (even if only on MedRxiv, or in the American Journal of Gastroenterology Black Issue) well-designed negative studies This book largely focuses on computational reproducibility, as this is rarely taught in medical training, and intense pressure to publish leads the untrained into dark practices. 4.1 First Steps to Research Reproducibility 4.1.1 Have a Plan The first steps toward research reproducibility come in planning for reproducibility. This means having a clear design and statistical plan, proven methods of data collection and recording, and preserving your raw data in a secure, locked state. 4.1.2 Treat Your Raw Data Like Gold Largely because you spend a lot of money (often other peoples’ \\[grant\\] money) and your own time (and the time of the people who work for you on the studies) in collecting it - you and others have invested a lot of gold to collect this. Once it is collected, it should be protected, backed up, and kept in a safe repository. Seriously consider having off-site backups for major projects to protect your data from local natural disasters. No one at Tulane expected Hurricane Katrina to destroy decades of research data. No one at NYU realized that the backup generators (in the basement) would be flooded by Hurricane Sandy. No one expects that their data and biosamples will be destroyed. But in an era (2022) when major natural disasters are increasing rapidly, it is important to plan for Black Swan events. Protect your research and protect your career. 4.1.3 Cleaning and Analyzing Your Data There are a number of levels of reproducibility, and the following figure/meme illustrates these. Levels of Reproducibility While an amusing meme, the image above does illustrate the arc of progress we hope to make as we move from ‘back of the envelope/Excel’ research to reproducible research. You don’t have to be making encapsulated research environments right away, but you can work up to that level. Working in Excel, or any other point-and-click environment, is not fundamentally reproducible. There are multiple steps that are not recorded, calculations that are not transparent, and formulas that difficult to check (thus, forensic accounting is a growing field). Working in Excel leads to many, many errors, which have caused significant damage. Excel was never designed for research. It was designed for exploring data for small businesses, and it does not scale well. going off on a tangent on the Perils of Excel, with examples Working in a free, open-source coding language like R greatly eases the transition to reproducible research. It provides tools, accessible to anyone with an internet connection, to make science reproducible and transparent. 4.1.4 The First Level of Reproducibility Working in the Console Pane in RStudio allows you to test code, trying out functions on your data. This is a good sandbox where you can do little harm, and is helpful when you are using a new package or function, and figuring out the syntax. You may need to get help for the function (?function_name), and it may take several iterations to get it to do what you want. You may jump over to a browser window to google something from RStudio Community, StackOverflow, or a good blog post to find an example. You can even save the results (a table, a figure) to a folder with functions like this in a code chunk. save(table, file = &quot;table.csv&quot;) ggsave(plot, filename = &quot;figure1.jpg&quot;, device = &quot;jpeg&quot;, width = 8, height = 5, units = &quot;inches&quot;) This is a good start, and your code is logged in the History, but it is not saved in a named file that you can go back to, and re-run. If your data changes, you have to start all over, and re-write the code. This is not very reproducible. 4.1.5 The Second Level of Reproducibility You can bridge from the sandbox of the Console pane to reproducible, saved code. There are a couple of nice features in the History tab (top right in RStudio) to help you. To get started, you need to Open a File to save your code in. As a default, we will start with RMarkdown files. In the RStudio menu, select ** File/New File/Rmarkdown… and give it a title, like “My First Analysis”. This will open a file in the RStudio Source pane (top left). Note that this title (“My First Analysis”) is the title at the top of the resulting document, not the name of the file itself. Once the file opens, you can see a header at the top, with Title, Author, Date, and Output Fields. This is called the YAML header, and it gives instructions for how the document will be processed to make a manuscript or HTML page. The YAML header is fenced with both an opening and a closing line of 3 dashes ---. To save the file itself, you can use Cmd-S(Mac)/Ctrl-S(Windows), or click on the Save icon at the top of the file, then give it a filename, which is generally in lower case, with no numbers or spaces, and no punctuation except for underscores and hyphens. Each Rmarkdown filename ends with .Rmd The first code chunk - the setup chunk - is provided for you in the Rmarkdown template, just below the YAML header. This is where you load packages, with the library(_packagename_) function. You want to load your packages at the beginning of the file, so that the functions and data from these packages are available later in the file. Notice that code chunks are set off with 3 backticks at the beginning and end. The first set of backticks is followed by a letter r inside of curly braces, like this: {r}. This tells Rstudio that the following code will be in the R language. The {r} can be followed by an (optional) chunk name (like setup), then a comma, and a number of (optional) code chunk options. Sometimes the code chunk options can get a bit ungainly, and you can enter them like comment lines at the top of the code chunk, before the code actually starts, using the hash-pipe marker, with commas between each option, like this: ```{r example} #| echo = TRUE, #| eval = FALSE medicaldata::cytomegalovirus %&gt;% head() ``` You can insert new Code chunks with the green button that has a white letter C on it, and a plus sign at the top left corner of the button, at the top of the Source pane. When you click on it, R is the first option, but other computer languages are available. Often, when you are writing code, you will start with experiments in the Console pane (lower left). Each time it does not quite work, you can use the up-arrow to pull down the most recent entry, to edit it and try again. Another helpful tip, if you want to find code that you ran a few steps ago, is to press the up arrow (and down arrow) a few times until you reach the code you want. If it was a while back, it may be helpful to enter the first few characters of the code, then use Cmd-UpArrow(Mac) or Ctrl-UpArrow(Win) use those characters to search through the history for what you want. Another helpful tip is that when you are running a function, and trying to get all the arguments set properly (between the parentheses), you can ask RStudio to prompt you for the remaining arguments. If you have your cursor between the parentheses, you can press the tab key and RStudio will give you a list of possible arguments for the function. You can select one and set it to an appropriate value. After each argument is set, you can enter a comma, and press the tab key again to get the list of arguments for that function. Try this out with the code chunk named cars in the Rmarkdown template. Insert your cursor after the word cars, before the close-parenthesis. Add a comma, then press the tab key. Set the digits argument to 2 or 5, then run the chunk (with the green arrow to the right) to see how the results change. Once you have worked out how to run a bit of code in the Console, it is helpful to click on the History tab. You will see all of your prior attempts. You can scroll up and down to find a working version of your code, and then click on buttons at the top of the window to send that line of code to either the - Console, or - Source pane If your cursor was in the code chunk you want in the Rmarkdown file, your working version will be copied from the History to your code chunk. Rmarkdown is designed to be literate code, with the default for headers and explanatory text, as shown in the Rmarkdown template, and a few code chunks inserted into the manuscript where needed. Note that there are three other options of document types to save your reproducible code. - R script (.R) - R notebook (.nb) - Quarto document (*.qmd) An R script just has code, with minimal explanation or documentation. This is limiting for reproducible research. You can add a few comments, but it is a long way from an understandable workflow or a manuscript. An R notebook is a hybrid, with an R script and HTML output as an option. The Rmarkdown documemt can be processed (knitted) to create Word documents, pdfs, Poerpoint presentations, and HTML web pages. Quarto documents are a newer version of code documentation that has a lot of the function of Rmarkdown. Future steps/goals in Reproducibility: building Projects in RStudio, using the {here} package for file paths documenting multiple code files in order with README saving interim clean/restructured data files backing up / collaborating / code sharing on GitHub code review with colleagues to check/test your analysis sharing deidentified data and code publicly preserving your project package environment with {renv} building an encapsulated coding environment with Docker and sharing on DockerHub "],["importing-your-data-into-r.html", "Chapter 5 Importing Your Data into R 5.1 Reading data with the {readr} package 5.2 Reading Excel Files with readxl 5.3 Bringing in data from other Statistical Programs (SAS, Stata, SPSS) with the {haven} package 5.4 Other strange file types with rio 5.5 Data exploration with glimpse, str, and head/tail 5.6 More exploration with skimr and DataExplorer 5.7 Practice loading data from multiple file types 5.8 Practice saving (writing to disk) data objects in formats including csv, rds, xls, xlsx and statistical program formats 5.9 How do readr and readxl parse columns? 5.10 What are the variable types? 5.11 Controlling Parsing 5.12 Chapter Challenges 5.13 Future forms of data ingestion", " Chapter 5 Importing Your Data into R For most of this book, we will be using datasets from the {medicaldata} package. These are easy to load. You just type into the Console pane medicaldata::scurvy and you get James Lind’s scurvy dataset (actually, a reconstruction of what it might have looked like for his 12 participants). If you want to save this data to an object in your work environment, you just need to assign this to a named object, like scurvy, like so: scurvy &lt;- medicaldata::scurvy # now print the columns for id and treatment scurvy %&gt;% select(study_id:treatment) ## # A tibble: 12 × 2 ## study_id treatment ## &lt;chr&gt; &lt;fct&gt; ## 1 001 cider ## 2 002 cider ## 3 003 dilute_sulfuric_acid ## 4 004 dilute_sulfuric_acid ## 5 005 vinegar ## 6 006 vinegar ## 7 007 sea_water ## 8 008 sea_water ## 9 009 citrus ## 10 010 citrus ## 11 011 purgative_mixture ## 12 012 purgative_mixture There are a number of medical datasets to explore and learn with, within the {medicaldata} package. However, at some point, you will want to use R to work on your own data. You may already be itching to get started on your own data. This is a good thing. Working with your own data, toward your own goals, will be a motivating example, and will help you learn R. As you go through the different chapters, use the example data and exercises to get you started and to learn the principles, and then try what you have learned on your own data. Reproducibility and Raw Data It is an important principle to always save an untouched copy of your raw data. You can copy it to a new object, and experiment with modifying it, cleaning it, making plots, etc., but always leave the original data file untouched. You want to create a completely reproducible, step-by-step trail from your raw data to your finished analysis and final report, and you can only do that if you preserve the original raw data. That is the cornerstone of your analysis. It is tempting to fix minor data entry errors, or other aspects of the raw data. Do not do this - leave all errors intact in your raw data, and explicitly make edits with explanations of - who made the edit - when it was made - what was changed - why it was made - provide a justification, and identify source documents to support the rationale. Every edit should be documented in your code, with who, when, what, and why. Now, on to the fun part. Let’s read in some data! 5.1 Reading data with the {readr} package Many of the standard data formats can be read with functions in the {readr} package. These include: read_csv() for comma-separated values (*.csv) files read_tsv() for tab-separated values (*.tsv) files read_delim() for files with a different delimiter that you can specify (instead of commas or tabs, there might be semicolons), or you can let {readr} guess the delimiter in readr 2.0. read_fwf() for fixed width files read_table() for tabular files where columns are separated by white-space. read_log() is specifically for web log files Let’s read a csv file. First, make sure that you have the {readr} package loaded (or the {tidyverse} meta-package, which includes {readr}). You can load {readr} with the library() function. library(readr) # or you can use library(tidyverse) # which will load 8 packages, including readr Note that this will not work if you do not already have the {readr} package installed on your computer. You will get an error, like this: Error in library(readr) : there is no package called 'readr' This is not a problem - you just have to install the package first. You only need to do this once, like buying a book, and putting it in your personal library. Each time you use the package, you have to pull the book off the shelf, with library(packagename). To install the {readr} package, we can install the whole {tidyverse} package, which will come in handy later. Just enter the following in your Console pane: install.packages('tidyverse') Note that quotes around ‘tidyverse’ are required, as tidyverse is not yet a known object or package in your working environment. Once the {tidyverse} package is installed, you can use library(tidyverse) without quotes, as it is a known (installed) package. OK, after that detour, we should be all caught up - you should be able to run library(tidyverse) or library(readr) without an error. Now that you have {readr} loaded, you can read in some csv data. Let’s start with a file named scurvy.csv in a data folder on GitHub. You will need to glue together the url_stem and “data/scurvy.csv” to get the full web address. Run the code chunk below to see the url_stem and the dataset. url_stem &lt;- &quot;https://raw.githubusercontent.com/higgi13425/rmrwr-book/master/&quot; url_stem ## [1] &quot;https://raw.githubusercontent.com/higgi13425/rmrwr-book/master/&quot; read_csv(glue(url_stem, &#39;data/scurvy.csv&#39;)) ## Rows: 12 Columns: 8 ## ── Column specification ──────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (8): study_id, treatment, dosing_regimen_for_scurvy,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 12 × 8 ## study_id treatment dosing_regimen_… gum_rot_d6 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 001 cider 1 quart per day 2_moderate ## 2 002 cider 1 quart per day 2_moderate ## 3 003 dilute_sulfuric_acid 25 drops of eli… 1_mild ## 4 004 dilute_sulfuric_acid 25 drops of eli… 2_moderate ## 5 005 vinegar two spoonfuls, … 3_severe ## 6 006 vinegar two spoonfuls, … 3_severe ## 7 007 sea_water half pint daily 3_severe ## 8 008 sea_water half pint daily 3_severe ## 9 009 citrus two lemons and … 1_mild ## 10 010 citrus two lemons and … 0_none ## 11 011 purgative_mixture a nutmeg-sized … 3_severe ## 12 012 purgative_mixture a nutmeg-sized … 3_severe ## # … with 4 more variables: skin_sores_d6 &lt;chr&gt;, ## # weakness_of_the_knees_d6 &lt;chr&gt;, lassitude_d6 &lt;chr&gt;, ## # fit_for_duty_d6 &lt;chr&gt; Let’s look at what was extracted from the csv file. This starts after the url_stem (web address) is printed out. The Console pane has a print out of the Column specification, followed by the data in rectangular format. In the Column specification, the delimiter between items of data is identified (a comma), and a listing of variables with the character (chr) data type is printed. There are no non-character data types in this particular dataset. The ‘guessing’ of what data type is appropriate for each variable is done ‘automagically’, as the read_csv() function reads the first 1000 rows, then guesses what data type is present in each column (variable). It is often conservative, and in this case, made all of these columns into character variables (also called strings). You could argue that the col_character() assignment should be numeric for the study_id variable, or that the Likert scales used for outcomes like gum_rot_d6 and skin_sores_d6 should be coded as ordinal variables, known as ordered factors in R. You will learn to control these data types during data import with the spec() argument. The second piece of output is the data itself. This is first identified as a ‘tibble’, which is a type of data table, with 12 rows and 8 columns, in # A tibble: 12 x 8. This is followed by a header row of variable names, and just below that is the data type (&lt;chr&gt; for character) for each column. Then, on the left are gray row numbers (not actually part of the data set), followed by (to the right) rows of data. A tibble, by default, only prints out 10 rows of data, and no more columns than will fill your current console window. The other columns are listed in order at the bottom of the tibble in gray type. Now, by simply reading in the data, you can look at it, but you can’t do anything with it, as you have not saved it as an object in your working Environment. If you want to do things with this data, and make them last, you have to assign the data to an object, and give it a name. To do this, you need to use an assignment arrow, as below scurvy_data &lt;- read_csv(glue(url_stem, &#39;data/scurvy.csv&#39;)) ## Rows: 12 Columns: 8 ## ── Column specification ──────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (8): study_id, treatment, dosing_regimen_for_scurvy,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Now this is saved to scurvy_data in your working Environment. You can look in the Environment tab (top right pane in RStudio) and see that scurvy_data has now appeared in the Data section, with 12 observations of 8 variables. This is not a file written (saved) to disk, but this dataset is now available in the working environment as an assigned data object. You can now print this out at any time by typing scurvy_data into the Console, or into a script. Try this out in the Console pane. 5.1.1 Test yourself on scurvy How many limes did the British seamen in the citrus arm receive each day? 32zero1.5 You can also start with the scurvy_data object, and do things to this data object, like summarize it, or graph it, or calculate total_symptom_score in a data pipeline. Once you have assigned your data to an object, it will stick around in that R session for later use. The csv (comma separated values) format is a very common data format, and most data management programs have a way to export *.csv files. The csv format is simple, is not owned by anyone, and works across platforms. However, it can occasionally be tricky if you have commas in the middle of a variable like degree, with entries like ‘md, phd’ in a column that is mostly ‘md’. The read_csv function is pretty smart, and usually gets the number of columns right, but this is something to watch out for. Notice that read_csv had no problem with the dosing of vinegar (“two spoonfuls, three times a day”) in the scurvy dataset. If you happen to come across a tab-separated values file, read_tsv() works the same way. Both of these functions have reasonable defaults, so that most of the time, you just have to use the path to your file (usually on your hard drive, rather than on the web) as the only argument. On occasion, though, you will want to take control of some of the other arguments, and use something other than the defaults. 5.1.2 What is a path? A path is the trail through the folders in your hard drive (or on the web) that the computer needs to follow to find a particupar file. Paths can look something like: C:/Documents/Rcode/my_file.R ~/User/Documents/Rcode/my_file.R and can get pretty complicated to keep track of. One particularly nice feature of Projects in RStudio is that the project directory is always your home, or root directory. You can make your life easier by using the {here} package, which memorizes the path to your project, so you can just write here(my_file.R), and not have to worry about making a typo in a long path name. When your data has no column names (headers), read_csv will (by default) assume that the first row of the data is the column names. To fix this, add the argument, col_names = FALSE. You can also assign your own col_names by setting a vector, like c(“patient_id”, “treatment”, “outcome”) to col_names, as below read_csv(file = glue(url_stem, &#39;data/scurvy.csv&#39;), col_names = c(&quot;pat_id&quot;, &quot;arm&quot;, &quot;dose&quot;, &quot;gums&quot;, &quot;skin&quot;, &quot;weak&quot;, &quot;lass&quot;, &quot;fit&quot;)) ## Rows: 13 Columns: 8 ## ── Column specification ──────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (8): pat_id, arm, dose, gums, skin, weak, lass, fit ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 13 × 8 ## pat_id arm dose gums skin weak lass fit ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 study_id treatment dosi… gum_… skin… weak… lass… fit_… ## 2 001 cider 1 qu… 2_mo… 2_mo… 2_mo… 2_mo… 0_no ## 3 002 cider 1 qu… 2_mo… 1_mi… 2_mo… 3_se… 0_no ## 4 003 dilute_sulf… 25 d… 1_mi… 3_se… 3_se… 3_se… 0_no ## 5 004 dilute_sulf… 25 d… 2_mo… 3_se… 3_se… 3_se… 0_no ## 6 005 vinegar two … 3_se… 3_se… 3_se… 3_se… 0_no ## 7 006 vinegar two … 3_se… 3_se… 3_se… 3_se… 0_no ## 8 007 sea_water half… 3_se… 3_se… 3_se… 3_se… 0_no ## 9 008 sea_water half… 3_se… 3_se… 3_se… 3_se… 0_no ## 10 009 citrus two … 1_mi… 1_mi… 0_no… 1_mi… 0_no ## 11 010 citrus two … 0_no… 0_no… 0_no… 0_no… 1_yes ## 12 011 purgative_m… a nu… 3_se… 3_se… 3_se… 3_se… 0_no ## 13 012 purgative_m… a nu… 3_se… 3_se… 3_se… 3_se… 0_no In this case, when we set our own col_names, there are now 13 rows of data, and the original column headers are now listed as the first row of data. We can fix this with the skip argument within the parentheses of the read_csv() function, which has a default of 0. We can skip as many lines as we want, which can be helpful if you have an Excel file with a lot of blank lines or commentary at the top of the spreadsheet. When we set skip = 1 in this case, we get a cleaner dataset, without variable names as data. read_csv(file = glue(url_stem, &#39;data/scurvy.csv&#39;), col_names = c(&quot;pat_id&quot;, &quot;arm&quot;, &quot;dose&quot;, &quot;gums&quot;, &quot;skin&quot;, &quot;weak&quot;, &quot;lass&quot;, &quot;fit&quot;), skip = 1) ## Rows: 12 Columns: 8 ## ── Column specification ──────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (8): pat_id, arm, dose, gums, skin, weak, lass, fit ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 12 × 8 ## pat_id arm dose gums skin weak lass fit ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 001 cider 1 qu… 2_mo… 2_mo… 2_mo… 2_mo… 0_no ## 2 002 cider 1 qu… 2_mo… 1_mi… 2_mo… 3_se… 0_no ## 3 003 dilute_sulfur… 25 d… 1_mi… 3_se… 3_se… 3_se… 0_no ## 4 004 dilute_sulfur… 25 d… 2_mo… 3_se… 3_se… 3_se… 0_no ## 5 005 vinegar two … 3_se… 3_se… 3_se… 3_se… 0_no ## 6 006 vinegar two … 3_se… 3_se… 3_se… 3_se… 0_no ## 7 007 sea_water half… 3_se… 3_se… 3_se… 3_se… 0_no ## 8 008 sea_water half… 3_se… 3_se… 3_se… 3_se… 0_no ## 9 009 citrus two … 1_mi… 1_mi… 0_no… 1_mi… 0_no ## 10 010 citrus two … 0_no… 0_no… 0_no… 0_no… 1_yes ## 11 011 purgative_mix… a nu… 3_se… 3_se… 3_se… 3_se… 0_no ## 12 012 purgative_mix… a nu… 3_se… 3_se… 3_se… 3_se… 0_no Now we don’t have extra column names as data, and we are back to 12 rows. Also note that in this code chunk, we put each argument to the function on its own line, with commas between them. This is a good practice, to make your code more readable. You can also set n_max to a particular number of rows to be read in (the default is infinity, or Inf) You might want a smaller number if you have a very large dataset and limited computer memory. Another important argument (option) for both read_csv and read_tsv is col_types, which lets you take control of the column types during the data What if you want to take more control of the import process with read_xxx()? You can add a col_types argument to the read_csv() function. You can copy the Column specifications from the first attempt at importing, and then make some edits. You can get the column specifications as guessed by {readr} by running the spec() function on the scurvy_data object. Try this out in the code chunk below. spec(scurvy_data) ## cols( ## study_id = col_character(), ## treatment = col_character(), ## dosing_regimen_for_scurvy = col_character(), ## gum_rot_d6 = col_character(), ## skin_sores_d6 = col_character(), ## weakness_of_the_knees_d6 = col_character(), ## lassitude_d6 = col_character(), ## fit_for_duty_d6 = col_character() ## ) This sets the data type for each column (variable). This is helpful if you want to change a few of these. Take a look at the next code chunk below. I have added the col_types argument to read_csv(), and set it equal to the Column specifications (copied from above). Then I edited study_id to col_integer(), and treatment to col_factor(). Run the code chunk below to see how this works. The glimpse function will give an overview of the new scurvy_cols object that I assigned the data to. scurvy_cols &lt;- read_csv( file = glue(url_stem, &#39;data/scurvy.csv&#39;), col_types = cols( study_id = col_integer(), treatment = col_factor(), dosing_regimen_for_scurvy = col_character(), gum_rot_d6 = col_character(), skin_sores_d6 = col_character(), weakness_of_the_knees_d6 = col_character(), lassitude_d6 = col_character(), fit_for_duty_d6 = col_character() ) ) glimpse(scurvy_cols) ## Rows: 12 ## Columns: 8 ## $ study_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, … ## $ treatment &lt;fct&gt; cider, cider, dilute_sul… ## $ dosing_regimen_for_scurvy &lt;chr&gt; &quot;1 quart per day&quot;, &quot;1 qu… ## $ gum_rot_d6 &lt;chr&gt; &quot;2_moderate&quot;, &quot;2_moderat… ## $ skin_sores_d6 &lt;chr&gt; &quot;2_moderate&quot;, &quot;1_mild&quot;, … ## $ weakness_of_the_knees_d6 &lt;chr&gt; &quot;2_moderate&quot;, &quot;2_moderat… ## $ lassitude_d6 &lt;chr&gt; &quot;2_moderate&quot;, &quot;3_severe&quot;… ## $ fit_for_duty_d6 &lt;chr&gt; &quot;0_no&quot;, &quot;0_no&quot;, &quot;0_no&quot;, … You can see that study_id is now considered the integer data type (&lt;int&gt;), and the treatment variable is now a factor (&lt;fct&gt;). You can choose as data types: col_integer () col_character() col_number() (handles #s with commas) col_double() (to specify decimal #s) col_logical() (only TRUE and FALSE) col_date(format = ““) - may need to define format col_time(format = ““) - col_datetime(format =”“) col_factor(levels = ““, ordered = TRUE) - you may want to set levels and ordered if ordinal. col_guess() - is the default col_skip() if you want to skip a column The read_csv() guesses may be fine, but you can take more control if needed. This col_types() approach gives you fine control of each column. But it is a lot of typing. Sometimes you want to set all the column types with a lot less typing, and you don’t need to set levels for factors, or formats for dates. You can do this by setting col_types to a string, in which each letter specifies the column type for each column. Run the example below by clicking on the green arrow at the top right of the code chunk, in which I use i for col_integer, c for col_character, and f for col_factor. scurvy_cols2 &lt;- read_csv( file = glue(url_stem, &#39;data/scurvy.csv&#39;), col_types = &quot;ifcffff&quot;) glimpse(scurvy_cols2) ## Rows: 12 ## Columns: 8 ## $ study_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, … ## $ treatment &lt;fct&gt; cider, cider, dilute_sul… ## $ dosing_regimen_for_scurvy &lt;chr&gt; &quot;1 quart per day&quot;, &quot;1 qu… ## $ gum_rot_d6 &lt;fct&gt; 2_moderate, 2_moderate, … ## $ skin_sores_d6 &lt;fct&gt; 2_moderate, 1_mild, 3_se… ## $ weakness_of_the_knees_d6 &lt;fct&gt; 2_moderate, 2_moderate, … ## $ lassitude_d6 &lt;fct&gt; 2_moderate, 3_severe, 3_… ## $ fit_for_duty_d6 &lt;chr&gt; &quot;0_no&quot;, &quot;0_no&quot;, &quot;0_no&quot;, … 5.1.3 Try it Yourself Now try this yourself with a *.tsv file. The file strep_tb.tsv is located in the same GitHub folder, and you can use the same url_stem. In the example code chunk below, there are several blanks. Copy this code chunk (use the copy button in the top right of the code chunk - hover to find it) to your RStudio Console pane. Edit it to make the two changes listed, and run the code chunk as directed below. Fill in the second part of the read_xxx() function correctly to read this file Fill in the correct file name to complete the path This version of the code chunk will read in every column as the character data type. This is OK, but not quite right. Now edit the col_types string to make: both doses numeric (n or d) (variables 3,4) gender a factor (f) (var 5) all 4 of the baseline variables into factors (var 6-9) skip over strep_resistance and radiologic_6m - set as hyphens (-) (var 10-11) rad_num into an integer (i) (var 12) improved into a logical (l) (var 13) strep_tb_cols &lt;- read_---( file = glue(url_stem, &#39;data/----.tsv&#39;), col_types = &quot;ccccccccccccc&quot;) glimpse(strep_tb_cols) 5.2 Reading Excel Files with readxl While file types like *.csv and *.tsv are common, it is also common to use Microsoft Excel or an equivalent for data entry. There are a lot of reasons that this is not a good idea (see chapter XX and video at link), but Excel is so ubiquitous, that it is often used for data entry. Fortunately, the {readxl} package provides functions for reading excel files. The read_xl() function works nearly the same as read_csv(). But there are a few bonus arguments (options) that are really helpful. The read_excel() function includes helpful arguments like skip, col_names, col_types, and n_max, much like read_csv(). In addition, read_excel() has a sheet argument, which lets you specify which sheet in an excel workbook you want to read. The default is the first worksheet, but you can set this to sheet = 4 for the 4th worksheet from the left, or sheet = “raw_data” to get the correct worksheet. You can also set the range argument to only read in a particular range of cells, like range = “B2:G14”. Below is an example of how to read in an Excel worksheet. read_excel(paulolol_xlsx, sheet = 1, skip = 0) ## New names: ## • `` -&gt; `...2` ## • `` -&gt; `...3` ## • `` -&gt; `...4` ## • `` -&gt; `...5` ## • `` -&gt; `...6` ## # A tibble: 14 × 6 ## `Data for my study` ...2 ...3 ...4 ...5 ...6 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Paul Investigator MD &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 44338 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 pat_id SBP_start SBP_… HR_s… HR_e… trea… ## 4 1 145 120 92 78 paul… ## 5 2 147 148 88 87 plac… ## 6 3 158 139 96 80 paul… ## 7 4 167 166 87 88 plac… ## 8 5 154 131 84 72 paul… ## 9 6 178 177 99 97 plac… ## 10 7 151 134 101 86 paul… ## 11 8 149 148 92 93 plac… ## 12 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; sbp hr &lt;NA&gt; ## 13 &lt;NA&gt; mean paulol… &lt;NA&gt; 131 79 &lt;NA&gt; ## 14 &lt;NA&gt; mean placebo &lt;NA&gt; 159.… 91.25 &lt;NA&gt; 5.2.1 Test yourself on strep_tb which argument in read_excel lets you jump past initial rows of commentary? jumpsheetskippath which argument in read_excel lets you pick which spreadsheet tab to read? picksheetskippath How many missing (NA) values are in this dataset (as run with skip =1)? what should the range argument be to read in these data cleanly? B2:F8A4:F11A1:L30B4:K15 5.3 Bringing in data from other Statistical Programs (SAS, Stata, SPSS) with the {haven} package It is common to have the occasional collaborator who still uses one of the older proprietary statistical packages. They will send you files with filenames like data.sas7bdat (SAS), data.dta (Stata), or data.sav (SPSS). The {haven} package makes reding in these data files straightforward. ** Set up ** haven::read_sas(glue(url_stem, &quot;data/blood_storage.sas7bdat&quot;)) ## # A tibble: 316 × 20 ## rbc_age_group median_rbc_age age aa fam_hx p_vol ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 25 72.1 0 0 54 ## 2 3 25 73.6 0 0 43.2 ## 3 3 25 67.5 0 0 103. ## 4 2 15 65.8 0 0 46 ## 5 2 15 63.2 0 0 60 ## 6 3 25 65.4 0 0 45.9 ## 7 3 25 65.5 1 0 42.6 ## 8 1 10 67.1 0 0 40.7 ## 9 1 10 63.9 0 0 45 ## 10 2 15 63 1 0 67.6 ## # … with 306 more rows, and 14 more variables: t_vol &lt;dbl&gt;, ## # t_stage &lt;dbl&gt;, b_gs &lt;dbl&gt;, bn &lt;dbl&gt;, ## # organ_confined &lt;dbl&gt;, preop_psa &lt;dbl&gt;, ## # preop_therapy &lt;dbl&gt;, units &lt;dbl&gt;, s_gs &lt;dbl&gt;, ## # any_adj_therapy &lt;dbl&gt;, adj_rad_therapy &lt;dbl&gt;, ## # recurrence &lt;dbl&gt;, censor &lt;dbl&gt;, ## # time_to_recurrence &lt;dbl&gt; ** write files, add examples for Stata, SPSS ** 5.4 Other strange file types with rio Once in a while, you will run into a strange data file that is not a csv or Excel or from a common statistical package (SAS, Stata, SPSS). These might include Systat, Minitab, RDA, or others. This is when the {rio} package comes to the rescue. The name, rio, stands for R input and output. The {rio} package looks at the file extension (like .csv, .xls, .dta) to guess the file type, and then applies the appropriate method to read in the data. The import() function in {rio} makes data import much easier. You don’t always have the fine control seen in {readr}, but {rio} is an all-purpose tool that can get nearly any data format into R. Try this out with the code chunk below. Just replace the filename in the code chunk with one of the files named below. Try to use the same import() function to read scurvy.csv strep_tb.tsv paulolol.xlsx blood_storage.sas7bdat rio::import(glue(url_stem, &quot;data/filename&quot;)) ## Error in remote_to_local(file, format = format): Unrecognized file format. Try specifying with the format argument. It can be very convenient to use {rio} for unusual file types. 5.5 Data exploration with glimpse, str, and head/tail Once you have a dataset read into your working Environment (see the Environment tab in RStudio), you will want to know more about it. There are several helpful functions and packages to get you started in exploring your data. 5.5.1 Taking a glimpse with glimpse() The glimpse() function is part of the tidyverse, and is a helpful way to see a bit of all of the variables in a dataset. Let’s try this with the scurvy dataset, which we have already assigned to the object scurvy in the working Environment. Just put the object name as an argument within the glimpse() function (inside the parentheses), as below. Run the code chunk below to get the glimpse() output. glimpse(scurvy) ## Rows: 12 ## Columns: 8 ## $ study_id &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;00… ## $ treatment &lt;fct&gt; cider, cider, dilute_sul… ## $ dosing_regimen_for_scurvy &lt;chr&gt; &quot;1 quart per day&quot;, &quot;1 qu… ## $ gum_rot_d6 &lt;fct&gt; 2_moderate, 2_moderate, … ## $ skin_sores_d6 &lt;fct&gt; 2_moderate, 1_mild, 3_se… ## $ weakness_of_the_knees_d6 &lt;fct&gt; 2_moderate, 2_moderate, … ## $ lassitude_d6 &lt;fct&gt; 2_moderate, 3_severe, 3_… ## $ fit_for_duty_d6 &lt;fct&gt; 0_no, 0_no, 0_no, 0_no, … The glimpse() function output tells you that there are 12 rows (observations) and 8 columns (variables). Then it lists each of the 8 variables, followed by the data type, and the first few values (or as much as will fit in the width of your Console pane). We can see that study_id and dosing_regimen_for_scurvy are both of the character (aka string) data type, and the other 6 variables are factors. 5.5.2 Try this out yourself. What can you learn about the strep_tb dataset with glimpse()? Edit the code chunk below to find out about strep_tb. glimpse(----) 5.5.3 Test yourself on strep_tb which variable is the logical data type? baseline_esrimprovedpatient_id which variable is the dbl numeric data type? armpatient_idrad_num How many observations are in this dataset? 5.5.4 Examining Structure with str() The str() function is part of the {utils} package in base R, and can tell you the structure of any object in R, whether a list, a dataset, a tibble, or a single variable. It is very helpful for reality-checking your data, especially when you are getting errors in your code. A common source of errors is trying to run a function that requires a particular data structure or data type on the wrong data structure or data type. Sometimes just checking the data structure will reveal the source of an error. The str() function does largely what glimpse() does, but provides a bit more detail, with less attractive formatting. Run the code chunk below to see the output of str(). str(scurvy) ## tibble [12 × 8] (S3: tbl_df/tbl/data.frame) ## $ study_id : chr [1:12] &quot;001&quot; &quot;002&quot; &quot;003&quot; &quot;004&quot; ... ## $ treatment : Factor w/ 6 levels &quot;cider&quot;,&quot;citrus&quot;,..: 1 1 3 3 6 6 5 5 2 2 ... ## $ dosing_regimen_for_scurvy: chr [1:12] &quot;1 quart per day&quot; &quot;1 quart per day&quot; &quot;25 drops of elixir of vitriol, three times a day&quot; &quot;25 drops of elixir of vitriol, three times a day&quot; ... ## $ gum_rot_d6 : Factor w/ 4 levels &quot;0_none&quot;,&quot;1_mild&quot;,..: 3 3 2 3 4 4 4 4 2 1 ... ## $ skin_sores_d6 : Factor w/ 4 levels &quot;0_none&quot;,&quot;1_mild&quot;,..: 3 2 4 4 4 4 4 4 2 1 ... ## $ weakness_of_the_knees_d6 : Factor w/ 4 levels &quot;0_none&quot;,&quot;1_mild&quot;,..: 3 3 4 4 4 4 4 4 1 1 ... ## $ lassitude_d6 : Factor w/ 4 levels &quot;0_none&quot;,&quot;1_mild&quot;,..: 3 4 4 4 4 4 4 4 2 1 ... ## $ fit_for_duty_d6 : Factor w/ 2 levels &quot;0_no&quot;,&quot;1_yes&quot;: 1 1 1 1 1 1 1 1 1 2 ... The str() output starts by telling you that scurvy is a tibble, which is a modern sort of data table. A tibble will by default only print 10 rows of data, and only the number of columns that will fit in your Console pane. Then you see [12 x 8], which means that there are 12 rows and 8 columns - the default in R is to always list rows first, then columns (R x C notation). Then you learn that this is an S3 object, that is a tbl_df (tibble), and a tbl, and also a data.frame). Then you get a listing of each variable, data type, and a bit of the data, much like glimpse(). Another extra detail provided by str() is that it tells you some of the levels of each factor variable, and then shows these as integers (how the data is actually stored). 5.5.5 Test yourself on the scurvy dataset what is the dose of cider? 25 drops1 quart per dayone-half rood how many levels of gum_rot are there? Which numeric value indicates ‘fit for duty’? Note that you can also use str() and glimpse() on a single variable. You often use this approach when you get an error message that tells you that you have the wrong data type. Try this with the strep_tb dataset variable patient_id by running the code chunk below. Imagine that you wanted to get the mean of patient_id. You got a warning that pointed out that the argument is not numeric or logical. So you run str() to find out the data structure of this variable. mean(strep_tb$patient_id) ## Warning in mean.default(strep_tb$patient_id): argument is ## not numeric or logical: returning NA ## [1] NA str(strep_tb$patient_id) ## chr [1:107] &quot;0001&quot; &quot;0002&quot; &quot;0003&quot; &quot;0004&quot; &quot;0005&quot; &quot;0006&quot; ... This shows you that patient_id is actually a character variable. If you wanted to find the mean value, you would have to change it to numeric (with as.numeric() first). The glimpse() function provides identical output to str() for a variable in a table. glimpse(strep_tb$patient_id) ## chr [1:107] &quot;0001&quot; &quot;0002&quot; &quot;0003&quot; &quot;0004&quot; &quot;0005&quot; &quot;0006&quot; ... You can choose whether you prefer the details of str() or the nicer formatting of glimpse() for yourself. 5.5.6 Examining a bit of data with head() and tail() Oftentimes, you want just a quick peek at your data, especially after a merge or a mutate, to make sure that things have gone as expected. This is where the base R functions head() and tail() can be helpful. As you might have guessed, these functions give you a quick view of the head (top 6 rows) and tail (last 6 rows) of your data. Try this out with scurvy or strep_tb. head(scurvy) ## # A tibble: 6 × 8 ## study_id treatment dosing_regimen_f… gum_rot_d6 ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; ## 1 001 cider 1 quart per day 2_moderate ## 2 002 cider 1 quart per day 2_moderate ## 3 003 dilute_sulfuric_acid 25 drops of elix… 1_mild ## 4 004 dilute_sulfuric_acid 25 drops of elix… 2_moderate ## 5 005 vinegar two spoonfuls, t… 3_severe ## 6 006 vinegar two spoonfuls, t… 3_severe ## # … with 4 more variables: skin_sores_d6 &lt;fct&gt;, ## # weakness_of_the_knees_d6 &lt;fct&gt;, lassitude_d6 &lt;fct&gt;, ## # fit_for_duty_d6 &lt;fct&gt; tail(strep_tb) ## # A tibble: 6 × 13 ## patient_id arm dose_strep_g dose_PAS_g gender ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0100 Streptomycin 2 0 M ## 2 0101 Streptomycin 2 0 F ## 3 0104 Streptomycin 2 0 M ## 4 0105 Streptomycin 2 0 F ## 5 0106 Streptomycin 2 0 F ## 6 0107 Streptomycin 2 0 F ## # … with 8 more variables: baseline_condition &lt;fct&gt;, ## # baseline_temp &lt;fct&gt;, baseline_esr &lt;fct&gt;, ## # baseline_cavitation &lt;fct&gt;, strep_resistance &lt;fct&gt;, ## # radiologic_6m &lt;fct&gt;, rad_num &lt;dbl&gt;, improved &lt;lgl&gt; Note that since these are tibbles, they will only print the columns that will fit into your Console pane. You can see all variables and the whole width (though it will wrap around to new lines) by either (1) converting these to a data frame first, to avoid tibble behavior, or (2) by using print, which has a width argument that allows you to control the number of columns (it also has an n argument that lets you print all rows). Run the code chunk below to see how this is different. head(as.data.frame(scurvy)) ## study_id treatment ## 1 001 cider ## 2 002 cider ## 3 003 dilute_sulfuric_acid ## 4 004 dilute_sulfuric_acid ## 5 005 vinegar ## 6 006 vinegar ## dosing_regimen_for_scurvy ## 1 1 quart per day ## 2 1 quart per day ## 3 25 drops of elixir of vitriol, three times a day ## 4 25 drops of elixir of vitriol, three times a day ## 5 two spoonfuls, three times daily ## 6 two spoonfuls, three times daily ## gum_rot_d6 skin_sores_d6 weakness_of_the_knees_d6 ## 1 2_moderate 2_moderate 2_moderate ## 2 2_moderate 1_mild 2_moderate ## 3 1_mild 3_severe 3_severe ## 4 2_moderate 3_severe 3_severe ## 5 3_severe 3_severe 3_severe ## 6 3_severe 3_severe 3_severe ## lassitude_d6 fit_for_duty_d6 ## 1 2_moderate 0_no ## 2 3_severe 0_no ## 3 3_severe 0_no ## 4 3_severe 0_no ## 5 3_severe 0_no ## 6 3_severe 0_no print(tail(strep_tb), width = Inf) ## # A tibble: 6 × 13 ## patient_id arm dose_strep_g dose_PAS_g gender ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0100 Streptomycin 2 0 M ## 2 0101 Streptomycin 2 0 F ## 3 0104 Streptomycin 2 0 M ## 4 0105 Streptomycin 2 0 F ## 5 0106 Streptomycin 2 0 F ## 6 0107 Streptomycin 2 0 F ## baseline_condition baseline_temp baseline_esr ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 3_Poor 2_99-99.9F 4_51+ ## 2 3_Poor 4_100F+ 4_51+ ## 3 3_Poor 4_100F+ 4_51+ ## 4 3_Poor 4_100F+ 4_51+ ## 5 3_Poor 4_100F+ 4_51+ ## 6 3_Poor 4_100F+ 4_51+ ## baseline_cavitation strep_resistance ## &lt;fct&gt; &lt;fct&gt; ## 1 yes 3_resist_100+ ## 2 yes 3_resist_100+ ## 3 yes 3_resist_100+ ## 4 yes 3_resist_100+ ## 5 yes 3_resist_100+ ## 6 yes 3_resist_100+ ## radiologic_6m rad_num improved ## &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 4_No_change 4 FALSE ## 2 2_Considerable_deterioration 2 FALSE ## 3 5_Moderate_improvement 5 TRUE ## 4 2_Considerable_deterioration 2 FALSE ## 5 1_Death 1 FALSE ## 6 6_Considerable_improvement 6 TRUE It is actually much easier to see the full width and height of a data set by scrolling, which you can do when you View() a dataset in the RStudio viewer. Try this out in the Console pane, with View(strep_tb). 5.5.7 Test yourself on the printing tibbles What function (and argument) let you print all the columns of a tibble? print(x, width=Inf)filter(starts_with(“width”))sum(na.rm=TRUE) What function (and argument) let you print all the rows of a tibble? filter(ends_with(“all”))median(na.rm=TRUE)print(x, n=Inf) If you just want a quick view of a few critical columns of your data, you can obtain this with the select() function, as in the code chunk below. Note that if you want to look at a random sample of your dataset, rather than the head or tail, you can use sample_frac() or sample_n() to do this. This sampling can be helpful if your data are sorted, or the head and tail rows are not representative of the whole dataset. See how this is used by running the code chunk below, which uses a 10% random sample of strep_tb to check that the mutate steps to generate the variables rad_num and improved worked correctly. #check that radiologic_6m, rad_num, and improved all match strep_tb %&gt;% sample_frac(0.1) %&gt;% select(radiologic_6m, rad_num, improved) ## # A tibble: 11 × 3 ## radiologic_6m rad_num improved ## &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 6_Considerable_improvement 6 TRUE ## 2 5_Moderate_improvement 5 TRUE ## 3 6_Considerable_improvement 6 TRUE ## 4 3_Moderate_deterioration 3 FALSE ## 5 3_Moderate_deterioration 3 FALSE ## 6 6_Considerable_improvement 6 TRUE ## 7 5_Moderate_improvement 5 TRUE ## 8 1_Death 1 FALSE ## 9 5_Moderate_improvement 5 TRUE ## 10 1_Death 1 FALSE ## 11 1_Death 1 FALSE 5.6 More exploration with skimr and DataExplorer Once you have your data read in, you usually want to get an overview of this new dataset. While there are many ways to explore a dataset, I will introduce two: skim() from the {skimr} package create_report() from the {DataExplorer} package You can get a more detailed look at a dataset with the {skimr} package, which has the skim() function, gives you a quick look at each variable in the dataset, with simple output in the Console. Try this out with the strep_tb dataset. Run the code chunk below, applying the skim() function to the strep_tb dataset. skimr::skim(strep_tb) Table 5.1: Data summary Name strep_tb Number of rows 107 Number of columns 13 _______________________ Column type frequency: character 1 factor 8 logical 1 numeric 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace patient_id 0 1 4 4 0 107 0 Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts arm 0 1.00 FALSE 2 Str: 55, Con: 52 gender 0 1.00 FALSE 2 F: 59, M: 48 baseline_condition 0 1.00 FALSE 3 3_P: 54, 2_F: 37, 1_G: 16 baseline_temp 0 1.00 FALSE 4 4_1: 43, 3_1: 32, 2_9: 25, 1_9: 7 baseline_esr 1 0.99 FALSE 3 4_5: 65, 3_2: 36, 2_1: 5, 1_0: 0 baseline_cavitation 0 1.00 FALSE 2 yes: 62, no: 45 strep_resistance 0 1.00 FALSE 3 1_s: 65, 3_r: 34, 2_m: 8 radiologic_6m 0 1.00 FALSE 6 6_C: 32, 5_M: 23, 1_D: 18, 3_M: 17 Variable type: logical skim_variable n_missing complete_rate mean count improved 0 1 0.51 TRU: 55, FAL: 52 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist dose_strep_g 0 1 1.03 1.00 0 0 2 2 2 ▇▁▁▁▇ dose_PAS_g 0 1 0.00 0.00 0 0 0 0 0 ▁▁▇▁▁ rad_num 0 1 3.93 1.89 1 2 5 6 6 ▇▅▁▆▇ 5.6.1 Test yourself on the skim() results How many females participated in the strep_tb study? 525948 What proportion of subjects in strep_tb were improved? 0.49350.5140.55 What is the mean value for rad_num in strep_tb? 3.931.894.7 A fancier approach is taken by the DataExplorer package, which can create a full html report with correlations and PCA analysis. Copy and run the chunk below to see what the Data Profiling Report looks like. DataExplorer::create_report(strep_tb) 5.6.2 Test yourself on the create_report() results What percentage of the baseline_esr values were missing? 9.3%0.09%0.93%1% How many total data points (‘observations’) are there in the strep_tb dataset? 10,0001,391107 You can choose which you prefer, the simpler approach of {skimr} vs the fancier reports of {DataExplorer.} 5.7 Practice loading data from multiple file types 5.8 Practice saving (writing to disk) data objects in formats including csv, rds, xls, xlsx and statistical program formats 5.9 How do readr and readxl parse columns? 5.10 What are the variable types? Variable Type Long form Abbreviation Logical (TRUE/FALSE) col_logical() l Integer col_integer() i Double col_double() d Character col_character() c Factor (nominal or ordinal) col_factor(levels, ordered) f Date col_date(format) D Time col_time(format) t Date &amp; Time col_datetime(format) T Number col_number() n Don’t import col_skip() - Default Guessing col_guess() ? 5.11 Controlling Parsing 5.12 Chapter Challenges There is a file named “paulolol.xlsx” with the path of ‘data/paulolol.xlsx’. A picture of this problematic file is shown below. paulolol Read in this file with the {readxl} package. Just for fun, try this see how this turns out with no additional arguments. Be sure to skip the problematic non-data first few rows Be sure to exclude the problematic non-data calculations below the table. Solution to Challenge 1: paulolol.xlsx Show Me Solution 1 read_excel(path = &#39;data/paulolol.xlsx&#39;, skip = 3, n_max = 8) ## # A tibble: 8 × 6 ## pat_id SBP_start SBP_end HR_start HR_end treatment ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 145 120 92 78 paulolol ## 2 2 147 148 88 87 placebo ## 3 3 158 139 96 80 paulolol ## 4 4 167 166 87 88 placebo ## 5 5 154 131 84 72 paulolol ## 6 6 178 177 99 97 placebo ## 7 7 151 134 101 86 paulolol ## 8 8 149 148 92 93 placebo Our intrepid Investigator has inserted a chart in place of his data table on sheet 1, and moved the data table to a 2nd sheet named ‘data’, and placed in the top left corner, at the suggestion of his harried statistician, in a new file with the path of ’data/paulolol2.xlsx” Try reading this file in with read_excel() read the help file help(read_excel) to figure out how to read in the data from this excel file. Solution to Challenge 2: paulolol2.xlsx Show Me Solution 2 read_excel(path = &#39;data/paulolol2.xlsx&#39;, sheet = &#39;data&#39;) ## # A tibble: 8 × 6 ## pat_id SBP_start SBP_end HR_start HR_end treatment ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 145 120 92 78 paulolol ## 2 2 147 148 88 87 placebo ## 3 3 158 139 96 80 paulolol ## 4 4 167 166 87 88 placebo ## 5 5 154 131 84 72 paulolol ## 6 6 178 177 99 97 placebo ## 7 7 151 134 101 86 paulolol ## 8 8 149 148 92 93 placebo 5.13 Future forms of data ingestion https://www.datacamp.com/community/tutorials/r-data-import-tutorial?utm_source=adwords_ppc&amp;utm_campaignid=1658343521&amp;utm_adgroupid=63833880415&amp;utm_device=c&amp;utm_keyword=%2Bread%20%2Bdata%20%2Br&amp;utm_matchtype=b&amp;utm_network=g&amp;utm_adpostion=&amp;utm_creative=469789579368&amp;utm_targetid=aud-392016246653:kwd-309793905111&amp;utm_loc_interest_ms=&amp;utm_loc_physical_ms=9016851&amp;gclid=Cj0KCQjwxJqHBhC4ARIsAChq4auwh82WzCiJsUDzDOiaABetyowW0CXmTLbUFkQmnl1pn4Op9xcCcdQaAhMWEALw_wcB reading data from the web with readr reading data from Google Sheets with googlesheets reading data from web tables with rvest "],["wrangling-rows-in-r-with-filter.html", "Chapter 6 Wrangling Rows in R with Filter 6.1 Goals for this Chapter 6.2 Packages needed for this Chapter 6.3 Pathway for this Chapter 6.4 Logical Statements in R 6.5 Filtering on Numbers - Starting with A Flipbook 6.6 Filtering on Multiple Criteria with Boolean Logic 6.7 Filtering Strings 6.8 Filtering Dates 6.9 Filtering Out or Identifying Missing Data 6.10 Filtering Out Duplicate observations 6.11 Slicing Data by Row 6.12 Randomly Sampling Your Rows 6.13 Further Challenges 6.14 Explore More about Filtering", " Chapter 6 Wrangling Rows in R with Filter In this chapter, we will introduce you ways to wrangle rows in R. You will often want to focus your analysis on particular observations, or rows, in your dataset. This chapter will show you how to include the rows you want, and exclude the rows you don’t want. The main function in the tidyverse for doing this is filter(). The filter() function uses logical statements, or conditions, to decide whether to keep or exclude rows in your dataset. Figure 6.1: Artwork by Allison Horst, https://www.allisonhorst.com 6.1 Goals for this Chapter Understand logical statements Filter rows based on numeric values Filter rows based on string values and regex Filter rows based on dates Combine filters with boolean logic symbols, like AND (&amp;) and OR(|) Filter on missing (NA) values Filter rows for duplicates/distinct observations Slicing rows based on row number/position Sampling rows as random samples 6.2 Packages needed for this Chapter {tidyverse} {lubridate} {medicaldata} 6.3 Pathway for this Chapter This Chapter is part of the DATA WRANGLING pathway. Chapters in this pathway include What is Tidy Data? Filtering Rows Counting, Grouping, and Summarizing Rows Arranging and Ranking Rows Selecting Columns Mutating to Make New Variables Rearranging Untidy data with {tidyr} and {pivotr} 6.4 Logical Statements in R Logical statements in R are important for defining something that is TRUE or FALSE for each row, or observation, in your data. A typical logical statement involves a variable, and some criteria to test each observation of that variable, resulting in a TRUE or FALSE for each row. Typical examples of logical statements include: sbp &gt; 140 troponin_i &gt; 9.0 creatinine &gt;= 2.5 gfr &lt;= 60 Each of these examples tests each row in the database with the stated criterion, and results in a vector of TRUE or FALSE values for each row (observation) of the variable in the logical statement. The filter() function will act on these TRUE and FALSE values to include (TRUE) or exclude (FALSE) the observations from the result. 6.5 Filtering on Numbers - Starting with A Flipbook This flipbook will show you step-by-step examples of how to filter rows of observations based on logical statements involving numbers. In each logical statement, a variable will be compared with a numeric value via a mathematical operator. These operators can include comparisons with greater than (&gt;) greater than or equal to (&gt;=) less than (&lt;) less than or equal to (&lt;=) equal to (==) notice two equals signs to test equality near() - an equality test that works with tiny decimal differences The general format for filter statements is: filter(variable [comparison operator] value), like filter(sbp &gt; 140) will keep only rows with systolic blood pressure values greater than 140. If you have not used a flipbook before, you can click on the frame below to activate it, then use the right and left arrow keys to move forward and back through the examples. With each forward step in the code on the left, examine the resulting output on the right. Make sure you understand how the output was produced. In many of the examples, a select() function is used to reduce the number of columns before the filter() step to make the results more clear. You saw several examples of filtering, including Example Code equality to a value filter(recurrence == 1) greater than a value filter(preop_psa &gt; 20) near a value filter(near(preop_psa, 10)) near a value with a set tolerance filter(near(preop_psa, 17, tol = 1.5)) between 2 values filter(between(preop_psa, 10, 13)) in a list of values filter(preop_psa %in% c(10,13,17)) 6.5.1 Your Turn - learnr exercises Try this out yourself, with interactive coding exercises below, written with the {learnr} package. For each exercise, you can type in the code required,by the instructions, then click on the blue Run Code button at the top right of the exercise to test your code. Watch out for a red x in the left margin - that identifies a coding error - hover over it for an explanation of what is wrong. If you get stuck, you can click on the Hint button to get a hint, and if needed, the Copy Clipboard button to copy the hint so that you can paste it into the Exercise box. 6.6 Filtering on Multiple Criteria with Boolean Logic You can use multiple filters on your data, and combine these with the boolean symbols AND (symbol &amp;) OR (symbol | ) - the vertical line, often on the keyboard above the backslash character ( \\ ) XOR - exclusive OR, so that the whole logical statement is true if either statement A is true, or statement B is true, but NOT if both are true. parentheses and combinations thereof. NEGATION - an exclamation point ( ! ) placed before an equals sign or a variable in a logical statement makes it mean the opposite. and you can use parentheses as well, to control the order of operations. You saw several examples of filtering, including Example Code AND filter(age &gt; 65 &amp; t_vol&gt;1) OR filter(age &gt; 69 | t_vol &gt; 1) exclusive OR (XOR) filter(xor(age &gt; 69, t_vol &gt; 1)) AND with negation (!) filter(age &gt; 64 &amp; aa != 1) 6.6.1 Your Turn - learnr exercises Try this out yourself, with interactive coding exercises below, written with the {learnr} package. For each exercise, you can type in the code required,by the instructions, then click on the blue Run Code button at the top right of the exercise to test your code. Watch out for a red x in the left margin - that identifies a coding error - hover over it for an explanation of what is wrong. If you get stuck, you can click on the Hint button to get a hint, and if needed, the Copy Clipboard button to copy the hint so that you can paste it into the Exercise box. 6.7 Filtering Strings You can use == to test exact equality of strings, but you can also use str_detect from the {stringr} package for more flexible matching, and combine it with the magic of regex (regex ~ Regular expressions) to do complicated filtering on character string variables in datasets. The typical formats for string filtering are filter(variable == “string”) for an exact match, or filter(str_detect(variable, pattern = “string”)) [note two parentheses at the end to completely close the parentheses with str_detect] You saw several examples of filtering strings, including Example Code matches “oma” filter(str_detect(diagnosis, “oma”)) negated match filter(!str_detect(diagnosis, pattern = “Hodgkin”)) regex for wild card filter(str_detect(diagnosis, pattern = “lympho.+ic”)) regex wild card filter(str_detect(diagnosis, “myelo.*”)) exact match filter(diagnosis == “myelofibrosis”) 6.7.1 Your Turn - learnr exercises Try matching strings yourself, in the interactive coding exercises below, written with the {learnr} package. For each exercise, you can type in the code required,by the instructions, then click on the blue Run Code button at the top right of the exercise to test your code. Watch out for a red x in the left margin - that identifies a coding error - hover over it for an explanation of what is wrong. If you get stuck, you can click on the Hint button to get a hint, and if needed, the Copy Clipboard button to copy the hint so that you can paste it into the Exercise box. 6.8 Filtering Dates You can use the {lubridate} package to format strings for logical tests, and filter your observations by date, month, year, etc. Dates are commonly formatted in ISO 8601 format, or YYYY-MM-DD, for 4-digit year, 2-digit month, and 2-digit day. The {lubridate} package can convert these to dates if the ymd() function is applied to a string formatted this way. The typical formats for date filtering are filter(date == ymd(“2002-10-06”)) for an exact match, or filter(between(datevar, ymd(“2020-01-01”, ymd(“2020-06-30”)) filter(datevar &gt; today() - years(1)) for the past year &lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream You saw several examples of filtering dates, including Example Code between 2 dates filter(between(fake_date, ymd(“2020-03-01”), ymd(“2020-03-31”) 24 months prior to today filter(fake_agvhd_date &gt; today() - months(24)) filter to weekdays 1 or 7 (weekend) filter(wday(fake_bmt_date) %in% c(1,7)) 6.8.1 Your Turn - learnr exercises Try matching dates yourself, in the interactive coding exercises below, written with the {learnr} package. For each exercise, you can type in the code required,by the instructions, then click on the blue Run Code button at the top right of the exercise to test your code. Watch out for a red x in the left margin - that identifies a coding error - hover over it for an explanation of what is wrong. If you get stuck, you can click on the Hint button to get a hint, and if needed, the Copy Clipboard button to copy the hint so that you can paste it into the Exercise box. 6.9 Filtering Out or Identifying Missing Data You can use the is.na(), drop_na() and negation with ! to help identify and filter out (or in) the missing data, or observations that are incomplete. Common formats for this include is.na(variable) - filters for observations where the variable is missing !is.na(variable) - filters for observations where the variable is not missing janitor::remove_empty(“rows”) - removes completely empty rows (helpful if you are importing a messy dataset) drop_na() - without filter, this drops any observations with at least one NA value in the row. drop_na(variable1, variable2) - without filter, this drops any observations with at least one NA value iin the variable1 or variable2 columns. 6.9.1 Working with Missing data A common need when doing DEV (Data Exploration and Validation) is to find your missing datapoints, so that you can figure out why they are missing, and whether you can fix this using other records, including the medical record. Another common scenario, when you can’t find or fix missing data, is that you want to drop observations which are missing in a particular variable. Sometimes you will need to drop whole empty rows, which usually happens because of poor formatting of the data (often in a spreadsheet). Sometimes you will need to drop observations that have missing data in any field, which is important in modeling. Linear and logistic models generally only run on complete cases, where all predictors and outcomes are non-missing. You can also impute missing data points, if there are not too many (&lt;10%), and these are truly missing completely at random (MCAR), which is often not the case. Usually there is a bias, so that particular types of study participants have more missing data than others. The packages {missForest} and {mice} can be helpful for imputation when MCAR assumptions are met. Imputation is beyond the scope of this chapter, and should generally be done in consultation with a statistician, as the assumptions involved are very important. You saw several examples of filtering missing data, including Example Code filtering for the missing observations in a variable fi lter(is.na(fake_date)) filter for the non-missing data in a variable filter (!is.na(fake_dx_date)) removing empty rows remove_empty(“rows”) removing incomplete cases drop_na() 6.9.2 Your Turn - learnr exercises 6.10 Filtering Out Duplicate observations You can use the distinct() function and the {janitor} package to help you find duplicated observations/rows for fixing or removal from your dataset. Common formats for this include distinct(dataset) - filters for rows that are distinct from all the other rows (non-duplicates). janitor::get_dupes(dataset) - filters for observations that are duplicated in all variables, counts them, and displays them in duplicated groups. You saw several examples of filtering duplicate data, including Example Code distinct observations distinct(dataset) finding duplicates get_dupes(dataset) making the complementary dataset dataset_whole %&gt;% anti_join(data_subset) 6.11 Slicing Data by Row You can use the slice() family of functions to cut out a chunk of your observations/rows by position in the dataset.. Common formats for this include slice(X:Y) - filters for rows X to Y by position (row number). slice_max(variable, n = N) - filters for observations with the maximum N values for a single variable. slice_min(variable, prop = 0.0N) - filters for observations with the minimum proportion of 0.0N values for a single variable. slice_head and slice_tail - filter for observations by position in the top (or bottom) N or top/bottom proportion. You saw several examples of slicing data, including Example Code slice by position slice(100:200) slice by top values in variable slice_max(age, n=20) slice by bottom values in variable slice_min(pan_day, prop = 0.01) slice by top or bottom position slice_tail(prop = 0.01) 6.12 Randomly Sampling Your Rows You can use the slice_sample() function to take a random subset of large datasets, or to build randomly selected training and testing sets fo modeling. Common formats for this include slice_sample(prop = 0.N) - randomly samples N % of your rows. slice_sample(n = N) - randomly samples N of your rows. You saw several examples of sampling data, including Example Code sampling a proportion slice_sample(prop = 0.3) sampling N rows slice_sample(n = 50) 6.12.1 Your Turn - learnr exercises 6.13 Further Challenges Try filtering on your own computer. Install R and Rstudio, as in Chapter 2. Then make sure you have the following packages installed, as in the Installing Packages chapter: tidyverse medicaldata Read in data from the medicaldata datasets with data(name) Then try some of the following challenges filter the cytomegalovirus dataset for rows with time to acute gvhd &lt; 6 months and CD3 dose &gt; 4 filter the opt dataset for Birthweight between 2500 grams and 3500 grams, or Apgar &gt;=6 filter the covid_testing dataset for females tested in the first 30 days of the pandemic with positive results 6.14 Explore More about Filtering Some helpful resources include: Suzan Baert’s blog series on dplyr, part 3 about filtering is found here. The tidyverse guide to filtering, found here. A blog post on filtering by Michael Toth, found here. The RYouWithMe series from RLadiesSyndey post on filtering here. "],["wrangling-columns-in-r-with-select-rename-and-relocate.html", "Chapter 7 Wrangling Columns in R with Select, Rename, and Relocate 7.1 Goals for this Chapter 7.2 Packages needed for this Chapter 7.3 Pathway for this Chapter 7.4 Tidyselect Helpers in R 7.5 Selecting a Column Variables 7.6 Selecting Columns that are Not Contiguous 7.7 Selecting Columns With Logical Operators 7.8 Further Challenges 7.9 Explore More about Filtering", " Chapter 7 Wrangling Columns in R with Select, Rename, and Relocate In this chapter, we will introduce you ways to wrangle columns in R. Data wrangling refers broadly to the many things you might do to make disorderly data more organized and tidy. You will often want to focus your analysis on particular variables, or columns, in your dataset. This chapter will show you how to include the columns you want, and exclude the columns you don’t want. You will also learn how to rename and relocate your columns in your dataset. We will also learn some good general practices for column naming, which is often more important than people realize. The main functions in the tidyverse for doing column functions are select(), rename(), rename_with(), and relocate(). The select() function can use a number of logical statements, and functional helpers, to decide whether to keep or exclude rows in your dataset. Figure 6.1: Artwork by Allison Horst, https://www.allisonhorst.com 7.1 Goals for this Chapter Understand select statements Select rows based on name and number Select rows with helpers including starts_with(), ends_with() Select rows with helpers including contains(), matches() Relocate column variables within your dataset with relocate Rename column variables within your dataset with rename() and rename_with() 7.2 Packages needed for this Chapter {tidyverse} {janitor} {medicaldata} 7.3 Pathway for this Chapter This Chapter is part of the DATA WRANGLING pathway. Chapters in this pathway include What is Tidy Data? Selecting, Renaming, and Relocating Columns Filtering Rows Counting, Grouping, and Summarizing Rows Arranging and Ranking Rows Mutating to Make New Variables Rearranging Untidy data with {tidyr} and {pivotr} 7.4 Tidyselect Helpers in R Tidyselect helpers in R are functions that help you select a specfic group of columns. A typical tidyselect helper function defines which variables match your criteria. Typical examples of tidyselect helper functions include: starts_with(“preop”) - matches preop_bp, preop_hr, preop_psa ends_with(“pain”) - matches abd_pain, extremity_pain, wound_pain contains(“bp”) - matches bp_screening, end_bp, first_bp_am matches(“week[0-9]”) - matches week4, week8, week0 everything() - all columns last_col() - the last column num_range(“wk”, 1:6) - matches wk1, wk3, wk6, but not wk7 where(is.numeric) - selects variables of numeric type Each of these examples of logical tests will each column in your dataframe with the included argument - the criteria within the parentheses, and results in a vector of TRUE or FALSE values for each column (variable) of the dataframe. The select() function will act on these TRUE and FALSE values to include (TRUE) or exclude (FALSE) the column variables from the resulting dataframe. 7.5 Selecting a Column Variables The general format for select statements is: select(variable1, variable_name3:variable_name17, variable55), which selects by name the first variable, the 3rd through the 17th, and the 55th variable (you use just the variable names, the numbers are just to illustrate how to use the colon operator), and drops the rest. You can also negatively select against variables, with the negative sign, as in select(-sbp, -hr) These logical statements can even be sequential within a select() function, to help clarify selections, as in when you want “age” but not “stage”. select(contains(\"age\"), -stage) You can also select variables by their column number, (though this can get ugly if column numbers change), as in select(1:5, 17:22) 7.5.1 Try this out Copy the code block below to your RStudio Console, and run it to get started. library(tidyverse) library(medicaldata) medicaldata::blood_storage %&gt;% select(everything()) %&gt;% head() Now replace the argument to the select statement (everything()) with 1:5. Run this - you should get only the first 5 columns, instead of all the columns. Now try this with AnyAdjTherapy:TimeToRecurrence as the argument to get the last 5 columns. You can use the colon operator to get any continguous group of columns between the start:end columns (inclusive of the start and end columns). Experiment for yourself to get specific groups of contiguous columns. 7.6 Selecting Columns that are Not Contiguous You can select any non-contiguous columns by inserting a comma between the columns selected. You can even select one column at a time. For example medicaldata::blood_storage %&gt;% select(1, bGS:sGS, 3, AA) %&gt;% head() RBC.Age.Group bGS BN+ OrganConfined PreopPSA PreopTherapy 1 3 3 0 0 14.08 1 2 3 2 0 1 10.50 0 3 3 3 0 1 6.98 1 4 2 1 0 1 4.40 0 5 2 2 0 1 21.40 0 6 3 1 0 0 5.10 0 Units sGS Age AA 1 6 1 72.1 0 2 2 3 73.6 0 3 1 1 67.5 0 4 2 3 65.8 0 5 3 3 63.2 0 6 1 3 65.4 0 You can mix single columns, ranges of columns, column names vs. numbers, and even change the order in which the columns are listed. Try this yourself, with the cytomegalovirus dataset. Copy the code chunk below and run it in your RStudio Console pane. Then edit it to select the ID, patient demographics, the prior treatment variables, and the dose variables. medicaldata::cytomegalovirus %&gt;% select(everything()) %&gt;% head() Show/Hide Solution medicaldata::cytomegalovirus %&gt;% select(ID, age:race, prior.radiation:prior.transplant, TNC.dose:TBI.dose) %&gt;% head() 7.7 Selecting Columns With Logical Operators You can select groups of columns with logical operators to combine selections. You can use c() to combine selections. An example: select(c(age:race, prior.radiation:prior.chemo)). the symbols &amp; and | to select the intersection or the union, respectively, of two sets of variables. An example: select(age:race &amp; prior.radiation:prior.chemo). the ! symbol to select the complement of a set of variables. An example: select(!c(age:race, prior.radiation:prior.chemo)). Try this yourself, with the esoph_ca dataset. Copy the code chunk below and run it in your RStudio Console pane. Then edit it to select all variables except the ncases and ncontrols, using the ! symbol. medicaldata::esoph_ca %&gt;% select(everything()) %&gt;% head() Show/Hide Solution medicaldata::esoph_ca %&gt;% select(!ncases:ncontrols) %&gt;% head() Now try editing the code chunk below to select all of the variables that are not related to dose or any kind of graft versus host disease (end in gvhd) in the cytomegalovirus dataset. Use the ! and the | symbols. medicaldata::cytomegalovirus %&gt;% select(everything()) %&gt;% head() Show/Hide Solution medicaldata::cytomegalovirus %&gt;% select(!c(TNC.dose:TBI.dose | agvhd:time.to.cgvhd)) %&gt;% head() 7.8 Further Challenges Try selecting variables on your own computer. Install R and Rstudio, as in Chapter 2. Then make sure you have the following packages installed, as in the Installing Packages chapter: tidyverse medicaldata Read in data from the medicaldata datasets with data(name) Then try some of the following challenges select in the cytomegalovirus dataset for select in the opt dataset for select in the covid_testing dataset for 7.9 Explore More about Filtering Some helpful resources include: The RYouWithMe series from RLadiesSyndey post on selecting here. The tidyverse guide to selecting variables, found here. Suzan Baert’s blog series on dplyr, part 2 about wrangling columns is found here. "],["using-mutate-to-make-new-variables-columns.html", "Chapter 8 Using Mutate to Make New Variables (Columns) 8.1 Calculating BMI 8.2 Recoding categorical or ordinal data 8.3 Calculating Glomerular Filtration Rate", " Chapter 8 Using Mutate to Make New Variables (Columns) Many datasets, especially if you were involved in the data collection, will have exactly the variables you need in exactly the right format and data type. But often we import data from the electronic medical record, a data warehouse, or the Centers for Disease Control, and the data may not be in quite the format we want. We also want to collect the most granular form of the data available, without any calculations or interpretations by either the participant, the study coordinator, or the investigator. For example, collecting the date of birth is more accurate than collecting the participant age in years (which is a rounded-off calculation). This allows us to accurately calculate the participant’s age at any particular point in time. The dataset below contains data on 4 participants, with baseline values and the dates of 2 subsequent visits. Copy this code block (with icon in top right of the code block) and paste it into your local RStudio instance to run it and create this temporary demonstration dataset on your own computer. dataset &lt;- tibble::tribble( ~studyid, ~dob, ~baseline_visit, ~visit_1, ~visit_2, ~wt_kg, ~ht_m, ~sex, ~race, ~ethnicity, ~creat, &#39;001&#39;, as.Date(&quot;1971-04-13&quot;), as.Date(&quot;2021-03-01&quot;), as.Date(&quot;2021-09-07&quot;), as.Date(&quot;2022-03-19&quot;), 64.2, 1.53, 1, 1, 0, 0.63, &#39;002&#39;, as.Date(&quot;1983-07-19&quot;), as.Date(&quot;2021-04-01&quot;), as.Date(&quot;2021-10-03&quot;), as.Date(&quot;2022-04-13&quot;), 56.3, 1.47, 2, 6, 1, 1.32, &#39;003&#39;, as.Date(&quot;1976-09-26&quot;), as.Date(&quot;2021-04-13&quot;), as.Date(&quot;2021-10-18&quot;), as.Date(&quot;2022-04-22&quot;), 84.7, 1.78, 1, 4, 0, 1.05, &#39;004&#39;, as.Date(&quot;1988-02-07&quot;), as.Date(&quot;2021-04-19&quot;), as.Date(&quot;2021-11-22&quot;), as.Date(&quot;2022-05-01&quot;), 99.2, 1.88, 2, 2, 0, 1.19) Below is an example of how to calculate the age at the baseline visit (dividing the interval days by 365.25 to get years), and then relocating this new value after the baseline visit to make it easier to find. Copy this code and run it yourself to see the result. dataset %&gt;% mutate(age_base_yrs = as.numeric( baseline_visit - dob)/365.25) %&gt;% relocate(age_base_yrs, .after = baseline_visit) Some takeaway points: - most mutate functions to produce new variables are fairly simple math - subtraction will convert dates to an interval of days - you can convert this interval to numeric to do more math (the conversion to years) - you have to assign a name to the new variable, and set it equal to your calculation - all the other variables remain in the dataset, you are just adding new ones - by default, these new variables (columns) are placed at the end (the far right) of your dataset - you can change their location to a more sensible or convenient location with the relocate() function. Arguments for this function include the variables that you want to relocate (the default is that they are inserted the front {or far left} of your dataset), and the .before= and .after= arguments, to help with specific column placement). - You can select the variables to relocate with the usual tidyselect dplyr helpers like starts_with(), where(is.numeric()), last_col(), matches(), etc. Try this yourself. Edit the code you copied above to do these calculations. Open the dataset in the Environment tab to get the correct variable names. Try these challenges: - Calculate the age at visit 2. Which participant is 45.6 years old at visit 2? Participant 001Participant 002Participant 003Participant 004 - Calculate the number of days between the baseline visit and visit 1. This gap is supposed to be 180-200 days. Which participant had a visit outside of the study window? Participant 001Participant 002Participant 003Participant 004 8.1 Calculating BMI Another common calculation is the calculation of body mass index. This requires dividing the weight in kilograms by the height in meters squared. Fortunately, our self-explanatory variable names reassure us that we have th right units for these. Run the code chunk below to see the raw data in the available small dataset. Now edit the code below to create a new variable named bmi with the mutate() function. dataset ## # A tibble: 4 × 11 ## studyid dob baseline_visit visit_1 visit_2 ## &lt;chr&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; ## 1 001 1971-04-13 2021-03-01 2021-09-07 2022-03-19 ## 2 002 1983-07-19 2021-04-01 2021-10-03 2022-04-13 ## 3 003 1976-09-26 2021-04-13 2021-10-18 2022-04-22 ## 4 004 1988-02-07 2021-04-19 2021-11-22 2022-05-01 ## # … with 6 more variables: wt_kg &lt;dbl&gt;, ht_m &lt;dbl&gt;, ## # sex &lt;dbl&gt;, race &lt;dbl&gt;, ethnicity &lt;dbl&gt;, creat &lt;dbl&gt; Show/Hide the Solution Note that rounding to 2 places was added to make bmi look nicer, and knitr::kable() was used to make a scrollable HTML table. You can use the scroll bar to move to the right to see all of the columns. dataset %&gt;% mutate(bmi = round(wt_kg/ht_m^2, 2)) %&gt;% relocate(bmi, .before = wt_kg) %&gt;% knitr::kable() studyid dob baseline_visit visit_1 visit_2 bmi wt_kg ht_m sex race ethnicity creat 001 1971-04-13 2021-03-01 2021-09-07 2022-03-19 27.43 64.2 1.53 1 1 0 0.63 002 1983-07-19 2021-04-01 2021-10-03 2022-04-13 26.05 56.3 1.47 2 6 1 1.32 003 1976-09-26 2021-04-13 2021-10-18 2022-04-22 26.73 84.7 1.78 1 4 0 1.05 004 1988-02-07 2021-04-19 2021-11-22 2022-05-01 28.07 99.2 1.88 2 2 0 1.19 8.2 Recoding categorical or ordinal data Sex data and race data are often recorded as categorical data. A variety of surveys often have response scales that are ordinal, like a happiness scale from 0_never to 10_always. The categorical data responses like male / female for sex have no inherent order, while the ordinal scales clearly do. These responses are often entered/coded as numbers, which are can be confusing, especially if there are many variables in the codebook. It is better to make variables and values self-explaining. For example, if male was coded as 1 and female was coded as 2, it would be very easy to get these reversed, or interpreted differently in different parts of your analysis. It is better to recode these to link the coded value to the definition, like 1_male, and 2_female. Then there is no confusion about what each value means. You can still extract the numeric values, if needed for calculations, by using the parse_number() function to retrieve only the numeric value. This recoding can be done with the case_when() function. The code below takes a numeric sex variable and recodes it to a new sex_cat variable with 2 categories, 1_male, and 2_female. It identifies 2 distinct cases (sex ==1 and sex ==2), and recodes the values to the desired ones. dataset %&gt;% mutate(sex_cat = case_when(sex == 1 ~ &quot;1_male&quot;, sex == 2 ~ &quot;2_female&quot;)) ## # A tibble: 4 × 12 ## studyid dob baseline_visit visit_1 visit_2 ## &lt;chr&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; ## 1 001 1971-04-13 2021-03-01 2021-09-07 2022-03-19 ## 2 002 1983-07-19 2021-04-01 2021-10-03 2022-04-13 ## 3 003 1976-09-26 2021-04-13 2021-10-18 2022-04-22 ## 4 004 1988-02-07 2021-04-19 2021-11-22 2022-05-01 ## # … with 7 more variables: wt_kg &lt;dbl&gt;, ht_m &lt;dbl&gt;, ## # sex &lt;dbl&gt;, race &lt;dbl&gt;, ethnicity &lt;dbl&gt;, creat &lt;dbl&gt;, ## # sex_cat &lt;chr&gt; Note that it is very easy to get an error with the logical tests in case_when() if you forget to use TWO equals signs, which are needed to TEST for equality. You can set a variable equal to a value with one equals sign, but to perform a logical test you will need to use 2 equals signs. Try this yourself. Try this challenge: - Recode the NIH race category from numeric to helpful self-explanatory values. Copy the code block above to use as a starting point. Use: - 1_white - 2_black - 3_asian - 4_pacific_islander - 5_native_american - 6_more_than_one_race to recode these numeric values into self-explanatory values Show/Hide the Solution Note that flextable::flextable() was used to make a pretty HTML table. You can use the scroll bar to move to the right to see all of the columns. Both kable (via kableExtra) and flextable have lots of formatting options for fonts, size, colors, alignment, etc. dataset %&gt;% mutate(race = case_when(race == 1 ~ &quot;1_white&quot;, race == 2 ~ &quot;2_black&quot;, race == 3 ~ &quot;3_asian&quot;, race == 4 ~ &quot;4_pacific_islander&quot;, race == 5 ~ &quot;5_native_american&quot;, race == 6 ~ &quot;6_more_than_one_race&quot;)) %&gt;% flextable::flextable() .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-43dbf25e{}.cl-43d6ee62{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-43d6ff1a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-43d6ff1b{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-43d7317e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-43d7317f{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-43d73188{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-43d73189{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-43d73192{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-43d73193{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} studyiddobbaseline_visitvisit_1visit_2wt_kght_msexraceethnicitycreat0011971-04-132021-03-012021-09-072022-03-1964.21.5311_white00.630021983-07-192021-04-012021-10-032022-04-1356.31.4726_more_than_one_race11.320031976-09-262021-04-132021-10-182022-04-2284.71.7814_pacific_islander01.050041988-02-072021-04-192021-11-222022-05-0199.21.8822_black01.19 8.3 Calculating Glomerular Filtration Rate A typical, but more complicated kind of mutation calculation is the calculation of GFR. This estimate of renal function is affected by sex, serum creatinine (sCr) level, and age, using the 2021 CKD-EPI Creatinine equations. These can be summarized as 2021 CKD-EPI Creatinine = 142 x (Scr/A)^B x 0.9938^age x (1.012 if female), where A and B are the following: Female Male Scr &lt;= 0.7 A = 0.7 B = -0.241 Scr &lt;= 0.9 A = 0.9 B = -0.302 Scr &gt; 0.7 A = 0.7 B = -1.2 Scr &gt; 0.9 A = 0.9 B = -1.2 which works out to 4 distinct equations. The four equations are listed here in R format to help you out. 2021 CKD-EPI Equations for GFR Case Equation Female, low Creatinine 142 * (creat/0.7)^-0.241 * 0.9938^age * 1.012 Female, high Creatinine 142 * (creat/0.7)^-1.200 * 0.9938^age * 1.012 Male, low Creatinine 142 * (creat/0.7)^-0.302 * 0.9938^age Male, high Creatinine 142 * (creat/0.7)^-1.200 * 0.9938^age Try this yourself. Try this challenge in your local version of RStudio: Use mutate and case_when to calculate gfr for each of the four cases with each equation. Note that you have to calculate (baseline) age first, in order to use it as a variable in the gfr calculation. Relocate creat, sex, age, and gfr to a location right after the studyid. Show/Hide the Solution dataset %&gt;% mutate(age = as.numeric((baseline_visit - dob)/365.25)) %&gt;% mutate(gfr = case_when( sex == 2 &amp; creat &lt;= 0.7 ~ 142 * (creat/0.7)^-0.241 * 0.9938^age * 1.012, sex == 2 &amp; creat &gt; 0.7 ~ 142 * (creat/0.7)^-1.200 * 0.9938^age * 1.012, sex == 1 &amp; creat &lt;= 0.9 ~ 142 * (creat/0.7)^-0.302 * 0.9938^age, sex == 1 &amp; creat &gt; 0.9 ~ 142 * (creat/0.7)^-1.200 * 0.9938^age )) %&gt;% relocate(creat, sex, age, gfr, .after = studyid) ## # A tibble: 4 × 13 ## studyid creat sex age gfr dob baseline_visit ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;date&gt; ## 1 001 0.63 1 49.9 107. 1971-04-13 2021-03-01 ## 2 002 1.32 2 37.7 53.1 1983-07-19 2021-04-01 ## 3 003 1.05 1 44.5 66.2 1976-09-26 2021-04-13 ## 4 004 1.19 2 33.2 61.8 1988-02-07 2021-04-19 ## # … with 6 more variables: visit_1 &lt;date&gt;, visit_2 &lt;date&gt;, ## # wt_kg &lt;dbl&gt;, ht_m &lt;dbl&gt;, race &lt;dbl&gt;, ethnicity &lt;dbl&gt; "],["interpreting-error-messages.html", "Chapter 9 Interpreting Error Messages 9.1 The Common Errors Table 9.2 Examples of Common Errors and How to fix them 9.3 Errors Beyond This List 9.4 When Things Get Weird 9.5 References:", " Chapter 9 Interpreting Error Messages Especially when you are starting out, it can be very difficult to interpret error messages, because these can be quite jargon-y. Let’s start with a table of the most common error messages, and the likely cause in each case. Note that when reading an error message, there are two parts - the part before the colon, which identifies in which function the error occurred, and the part after the colon, which names the error. A typical error message is usually in the format: Error in Where the error occurred : what the error was here is an example Error in as_flextable(.) : object 'errors' not found On the left, you are being told that the error occurred when the as_flextable() function was called. This can be helpful if you have run a long pipeline of functions, as it helps you isolate the problem. On the right, you are being told what the error was. In this case, the function looked for the object errors in the working environment (see your Environment tab at the top right in RStudio), and could not find it. Note that sometimes syntax errors caused by missing components (a missing comma, a missing parenthesis, a missing pipe symbol %&gt;% , or a missing + sign in a ggplot pipe) will cause an error in the next function in the pipeline. Watch out for this, especially when the function where the error is found looks fine - often it occurs because there is a missing piece just before this function. Then we will walk through examples of how to create each error, and how to fix them, one by one. 9.1 The Common Errors Table Examine the error message from R, particularly the part that comes after the colon (:). The error messages listed in the left column will be what appears after the colon (:) Common Error Messages in R Error Message What it Means could not find function This usually means that you made a typographical error in the function name (including Capitalization - R is case-sensitive), or that the package you are intending to use (which contains the function) is not installed - with `install.packages(‘package_name’)` or the package is not loaded - with `library(package)` object ‘object-name’ not found This usually means that the function looked for an object (like a data frame or a vector) in your working environment (check your Environment pane) and could not find it. This commonly happens when you mistype the name of the object (double-check this, easy to fix), or you did not actually create or save this object to your working environment - confirm by checking your Environment tab at the top right in RStudio. filename does not exist in current working directory (‘path/to/working/directory’) This usually means one of three things: you mistyped the name of the file, or part of the path, you are not in the directory where the file is, or the file you thought you had saved does not exist (check your Files tab in the lower right pane in RStudio). error in if This usually means that you have an *if* statement that is trying to make a branch-point decision, but the logical statement that you wrote is not providing either a TRUE or a FALSE value. The most common reasons are typographical errors s in the logical statement, or an NA in one of the underlying values, which yields an NA from the logical statement. You may need to use a `na.rm = TRUE` option in your logical statement (the na.rm issue often comes up with means and medians). error in eval This usually occurs when you are trying to run a function on an object that does not exist in your environment. Check to make sure in your Environment pane, and consider that you may not have saved/assigned the object in a previous step. Alternatively, you may have a typographical error in the object name. Worth checking. cannot open This usually occurs when you are attempting to access or read a file that either does not exist, or is not in the folder that you thought it was. Check your working directory and find the file in your file structure. This can often be prevented by working in RStudio projects and using the here() function for paths to files. no applicable method This usually occurs when you are using a function that expects a particular data structure (vector, list, dataframe), but you have given it a different data structure as the input. Check the data structure of your object, and check the documentation for your function. For example, if you want to use a function that acts on vectors, this function will not work on a dataframe object. You may have to use the `pull(variable)`function to “pull” this variable out of the dataframe into a vector before using this function. subscript out of bounds you are trying to access an item in an environment object (like a vector, dataframe, or list) that does not exist, like the 9th item in a vector that is 7 items long, or the negative 2nd row of a dataframe. Check the length of the item, and the math that you used to count the item number (loops that go too long are often a culprit) replacement has [x] rows, data has [y] rows This usually occurs when you are trying to code for a new variable, or replace a variable in a dataframe. But somehow (missing values, NAs), what you are trying to add to the dataframe is not the same length (number of rows) as the rest of the existing dataframe. Use a length() function to check your building of this vector at each step, to figure out where your length went wrong. package not available for R version x.y.z This occurs when you are trying to install a package, and your R version is newly updated. The problem is that the package version available on CRAN has not caught up to your shiny new version of R. This can happen after an R update when the package developer is working on updating their package, but the new version has not made it onto CRAN yet. This is often fixable if you know where the developer stores their development code (usually on GitHub). For example, if the package is {medicaldata}, and the developer’s Github userid is higgi13425, then you can install the development version of this package with remotes::install_github('higgi13425/medicaldata'). This assumes that you have already installed and loaded the {remotes} package. non-numeric argument to a binary operator A binary operator, like + or *, is a mathematical operation that takes two values (operands) and produces another value. It gets grumpy when trying to do math on things that are not numbers. A typical input to produce this error would be 1 + 'one' - one operand is numeric, and the other 'one' is a character string - the non-numeric argument. object of type closure is not subsettable This occurs when you try to extract a subset of something - but it is actually a function, not an object. This most commonly occurs when you try to subset a particular object that does not exist, like df$patient_id or data$sbp, when you have not created the objects df or data. The reason you get this strange error message, rather than simply Error: object 'df' not found , is that df() and data() are defined functions in base R. It is good practice to avoid naming any objects data or df for this reason. It gets very confusing, and this is best avoided. 9.2 Examples of Common Errors and How to fix them 9.2.1 Missing Parenthesis This is a very common error. It is easy to lose track of how many sets of parentheses you have open in putting together a complicated function. Here is an example, where a closing parenthesis is missing from a mutate() function. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt;65 &amp; aa == 1 ~ 1, TRUE ~0) %&gt;% filter(older_aa ==1) In this case, no output is produced, and the console does not return to the &gt; prompt. Instead, it offers a + prompt - in effect, asking you for something more. If you type in an extra closing parenthesis (after the filter function), it will give you an error. The error you get is: Error: Problem with `mutate()` input `older_aa`. x no applicable method for ‘filter_’ applied to an object of class “c(‘double’, ‘numeric’)” ℹ Input `older_aa` is ``%&gt;%`(…)`. R identifies a problem with the input “older_aa” to mutate - the parentheses are not closed. It then fails on the next function - filter, and gives you a strange error message - filter_ applied to… - because the input to the filter step (the next step after the error) was incoherent. This can be a bit confusing. But if you inspect the input older_aa, you will find the mis-matched parentheses. This is much easier to find with “rainbow parentheses” turned on in Tools/Global Options. When this option is on, you can be sure your parentheses are right when you end on red. Sometimes the very thin parentheses of some fonts can make it difficult to identify your red close parenthesis. The red vs. yellow parentheses may be easier to differentiate if you use a bold font for your code. You can change this with RStudio Tools/Global Options/Appearance. I am currently using FiraCode-Bold and the Crimson Editor theme in RStudio, which seems to help me. In this case, adding the missing parenthesis to the mutate step fixes it. Parentheses that end on red are all right. 9.2.2 An Extra Parenthesis What if you go the other way, with an extra parenthesis after some misguided copy-paste adventures? Let’s see what happens. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt;65 &amp; aa == 1 ~ 1, TRUE ~0))) %&gt;% filter(older_aa ==1) In this code block, you will end up with two red closing parentheses, and when you click to the right of the final closing parenthesis, there will be no matching highlighted open parenthesis (note that the preceding closing parentheses both have matching highlighted open parentheses. Both of these are clues that this last one is an extra. The error you get from R is Error in filter(older_aa == 1) : object ‘older_aa’ not found The left side of the error message identifies the filter step as where the error occurs, and the right side of the error message states that the error is an object not found. The error occurs when R gets to the next function. It also tells you that older_aa was not successfully created - suggesting that the problem is in the step before the filter function. In this case, removing the extra parenthesis from the mutate step fixes it. 9.2.3 Missing pipe %&gt;% in a data wrangling pipeline This is a common error. It is easy to cut out one of your %&gt;% connectors when you are editing/debugging a data wrangling pipeline. Here is an example, where a %&gt;% is missing. Can you spot it? prostate %&gt;% select(t_vol, p_vol, age, aa) mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt;65 &amp; aa == 1 ~ 1, TRUE ~0)) %&gt;% filter(older_aa ==1) In this case, the error you get is: Error in mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt; 65 &amp; aa == : object ‘t_vol’ not found The left side of the error message identifies the mutate step as where the error occurs, and the right side of the error message states that the error is an object not found. This is a bit misleading, as the problem is not in the mutate step. But mutate is where the pipeline crashes, as it can not find the variable t_vol. You have to backtrack upwards line-by-line to find the error. Every line of a data wrangling pipeline should end in %&gt;%. Since this is such a common error, this should be one of your “usual suspects”. And the select line, just above the mutate line, is where the problem is. In this case, adding the missing %&gt;% to the end of the select step fixes your data wrangling pipeline. Use one function per line in a pipeline. Check every data wrangling pipeline to make sure each step (except the last) ends in a pipe %&gt;% 9.2.4 Missing + in a ggplot pipeline This is a common error. It is easy to cut out one of your + connectors when you are editing/debugging a ggplot. Here is an example, where a + is missing in the middle of a ggplot pipeline. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% ggplot(aes(x = factor(t_vol), y =p_vol)) geom_boxplot() + labs(x = &quot;tumor volume&quot;, y = &quot;prostate volume&quot;) + theme_minimal() In this case, you get a ggplot output, but without any boxplots. It is also missing your custom labels for the x and y axes, and the theme you wanted. Essentially, the code stops running after the initial ggplot() statement and the remaining lines of code are ignored. This can be pretty puzzling, as you do get a plot, but not what you intended. There is a partial plot in the Plots tab, but you get a somewhat helpful error in the Console. The error you get is: Error: Cannot add ggproto objects together. Did you forget to add this object to a ggplot object? R identifies a problem with the last 3 lines of code, starting with geom_boxplot() - it can not add these ggproto objects (the components of a ggplot) to the existing plot. It asks, “Did you forget to add?” which should be a clue that there is a missing + sign between lines of ggplot code. Since the theme and labels are the defaults, and there are no boxplots, suggest that these last 3 lines were not run at all, and that the missing plus sign should be found just before these lines of code. In this case, adding the missing + to the end of the ggplot step fixes your plot. Use one function per line in a pipeline. Check every ggplot pipeline to make sure each step (except the last) ends in a plus sign + 9.2.5 Pipe %&gt;% in Place of a + This is a common error. It is easy to start with your dataset, do some data wrangling steps with the pipe %&gt;% and keep piping out of habit, even after you start your ggplot. Unfortunately, once you start to ggplot, you have to use + as your code connector. Having a pipe instead will cause an error. Here is an example, where a %&gt;% is used instead of + in a ggplot pipeline. It usually happens at the beginning of the ggplot, when you are still in piping mode. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% ggplot(aes(x = factor(t_vol), y =p_vol)) %&gt;% geom_boxplot() + labs(x = &quot;tumor volume&quot;, y = &quot;prostate volume&quot;) + theme_minimal() In this case, you will not get a ggplot output, and you will get an error in the console. The error you get is: Error: `mapping` must be created by `aes()` Did you use %&gt;% instead of +? The error message identifies the aes() step as where the error occurs. R identifies a problem that causes the aes function to fail to create a mapping. The first line is not very helpful (other than identifying aes() as a problem), but in the next line, R asks, “Did you use %&gt;% instead of +?” which is very helpful. Once you know this, look at the line where aes() failed. This is where there is a pipe in place of a plus. In this case, replacing the %&gt;% with a + fixes your plot. 9.2.6 Missing Comma Within a Function() This is a common error. It is easy to start a series of arguments to a function, like multiple variables in a mutate step, and miss a comma between them. Here is an example, where a comma is missing in a series of mutate steps. Note that it is a good habit to put one mutate step on each line, with each line ending in a comma. This will help you find the missing comma if (no, when) you make this mistake. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt;65 &amp; aa == 1 ~ 1, TRUE ~0) age_decade = floor(age / 10)) %&gt;% filter(older_aa ==1) In this case, you will not get a tibble output, and you will get an error in the console. The error you get is: Error in filter(older_aa == 1) : object ‘older_aa’ not found The left side of the error message identifies the filter step as where the error occurs, and the right side of the error message states that the error is an object not found. R identifies a problem that causes the filter function to fail, but this is actually a problem in the line prior. The variable older_aa was not created and is not available to filter. It should have been created in the mutate step, but this step is where the failure occurred. Because you formatted the mutate step with one mutate statement per line, it is easy to check each line for a comma - and the older_aa line is missing its comma. In this case, adding a comma at the end of the older_aa line (after “TRUE ~0)” fixes your data wrangling pipeline. 9.2.7 A Missing Object This is a common error. You may have created or modified a dataframe, but forgot to assign it to a new object name. Or maybe you did this assignment in a different session, but have not done it in your current session. Or maybe you made a typographical error in calling the object (“covvid” instead of “covid”). Either way, this object is not yet loaded into your computing environment (the Environment tab). In this example, we request data from the {medicaldata} package, but forget to assign it to an object. So it does not exist when we try to use it to start a pipeline. This does not work. medicaldata::covid_testing covid %&gt;% select(subject_id, age, result, ct_result, patient_class) %&gt;% mutate(high_titer = case_when(ct_result &lt; 18, TRUE ~ 0), age_decade = floor(age / 10)) %&gt;% filter(age &gt;50) In this case, you will not get a tibble output, and you will get an error in the console. The error you get is: Error in select(., subject_id, age, result, ct_result, patient_class) : object ‘covid’ not found The portion to the left of the comma identifies where the error occurs - in the select step. The portion to the right of the comma identifies the error. This one is easy. The object ‘covid’ was not found. You can check your Environment pane, and it will not be there. What the coder intended was to call medicaldata::covid_testing and assign it (with an arrow) to a new object named covid. But that assignment did not happen, and R is unable to guess what you meant. In this case, adding an assignment arrow -&gt; to the end of the medicaldata::covid_testing line and then covid completes the assignment, creates the covid object, and fixes your data wrangling pipeline. 9.2.8 One Equals Sign When you Need Two This is a very common error. The equals sign is commonly used in two ways in R. To assign a parameter or argument of a function, like x = p_vol, or ratio = p_vol/t_vol, or color = “blue”. In all of these assignment cases, you use one equals sign. To test a logical statement, like age == 60, or fam_hx == 1, or location == “Outpatient”. In all of these logical tests, you use two equals signs. It is very common to use one equals sign in a logical statement. This causes errors. Watch the last filter step below. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt;65 &amp; aa == 1 ~ 1, TRUE ~0), age_decade = floor(age / 10)) %&gt;% filter(older_aa ==1) In this case, the error you receive is very helpful: Error: Problem with `filter()` input `..1`. x Input `..1` is named. ℹ This usually means that you’ve used `=` instead of `==`. ℹ Did you mean `older_aa == 1`? The problem is with the filter step. The error starts out very jargon-y. “input `..1`. x Input `..1` is named” - means the input to filter is actually named (an assignment). But then it gets a lot more helpful. It recognizes that you have made a common error, and suggests an appropriate fix. In this case, adding a 2nd equals sign in the filter step fixes your data wrangling pipeline. Testing for equality with == is a big problem with real numbers, rather than integers. Computers use algorithms to do math which are not quite exact, leading to small differences in decimals. The == equality test is very strict, so that something like sqrt(2)^2 == 2 is FALSE because of small differences far to the right of the decimal point, which can trip you up. You can see these if you run the modulo 2: sqrt(2)^2 %% 2, which gives you the remainder after you divide by 2, which is the very tiny 0.0000000000000004440892. In this situation, you should use the near() function, as near(sqrt(2)^2, 2) is TRUE. The near function has a built-in tolerance of 0.00000001490116, which will be able to handle any computer-generated small, stray decimals. You can set your own tolerance argument if needed. 9.2.9 Non-numeric argument to a binary operator This happens when you try to do math on things that are not numbers. It usually occurs when you have a variable(column) that looks like it is numeric (it contains numbers), but somewhere along the way it became a character string variable. This often occurs when data are being entered into a spreadsheet, and one value in the column has characters in it. This often happens when you have a column of systolic blood pressures, and one value is entered as “this was not done”, or “102, but taken standing up”. Having comments, even if only one character string in a column in Excel makes the whole column into the character string data type. This is not apparent until you try to do math with this variable, as in data %&gt;% mutate(mean_art_pressure = sbp/3 + 2/3* dbp) This will give you the error: Error in mutate(mean_art_pressure: non-numeric argument to binary operator To fix this, you will have to Determine which variable, sbp or dbp, is non-numeric (glimpse(data) will help). Review the values of the problem variable (possibly with table()) to find which is non-numeric. Fix these values manually in your code, and document with comments Which values are being fixed (e.g. sbp for subject 007, at visit 2) data$sbp[subject == 007 &amp; visit == 2] &lt;- 102 What the original value was, and what the new value will be Who made the change to the data Why the data change was made On what date the data change was made Never over-write your original data - keep a complete audit trail! 9.3 Errors Beyond This List This is where the internet comes in handy. Whatever errors you can create, someone has already run into. And they have asked for help on the internet, and most of the time, someone has helped them solve their error. You should copy your entire error message, and paste it into a web search. Google will often yield multiple similar examples, with various ways to solve the problem. In a future chapter, we will explore more effective ways to seek help, using a minimal reprex to describe your problem accurately to other people on sites like community.rstudio.com. Remember that the error may have occurred because of a problem in the previous line of code (missing parenthesis, comma, etc.), so don’t forget to check one line above. The Add-One-Line debugging strategy is another good strategy. Select the code for your pipeline from the beginning to 2 lines of code before the error. If that runs without errors, add one line to your selection, and run it. Keep adding lines to your selection and running until you hit the error. Then try to find the problem and fix it. 9.4 When Things Get Weird 9.4.1 Restart your R Session (Shift-Cmd-F10) If you are running code that has worked before, and it is not working now, it is possible that you have created something odd in your working Environment that is interfering with your code. Sometimes it is an old object from a previous session (it is always better to start from a clean slate). Completely restart your R session (click on Session/Restart R, or use the keyboard shortcut), make sure the Environment is clean, then run your code from start to finish to give it a new try. Sometimes a clean slate will make all the difference. 9.5 References: These are some helpful general references on troubleshooting in R, particularly focused on error messages encountered by beginners to R. Click on some of the links to check these out. https://bookdown.org/yih_huynh/Guide-to-R-Book/trouble.html https://medium.com/analytics-vidhya/common-errors-in-r-and-debugging-techniques-f11af3f1c7d3 https://rpubs.com/Altruimetavasi/Troubleshooting-in-R https://github.com/noamross/zero-dependency-problems/blob/master/misc/stack-overflow-common-r-errors.md https://www.r-bloggers.com/2016/06/common-r-programming-errors-faced-by-beginners/ https://www.r-bloggers.com/2015/03/the-most-common-r-error-messages/ https://rpubs.com/Altruimetavasi/Troubleshooting-in-R https://statisticsglobe.com/errors-warnings-r "],["the-building-blocks-of-r-data-types-data-structures-functions-and-packages..html", "Chapter 10 The Building Blocks of R: data types, data structures, functions, and packages. 10.1 Data Types 10.2 Data Structures 10.3 Examining Data Types and Data Structures 10.4 Functions 10.5 Packages 10.6 The Building Blocks of R", " Chapter 10 The Building Blocks of R: data types, data structures, functions, and packages. In this chapter, we will learn about some critical building blocks in R that can lead to a lot of frustration if you do not understand these building blocks and how they are used. Once you understand how these building blocks work, you will be able to avoid these frustrations and build some pretty amazing data analysis pipelines. 10.1 Data Types There are actually six data types in R, though we usually use only use 4 of these for data analysis. logical (TRUE or FALSE) integer (1L, 2L) - these take up less memory than a double. You specify a number as an integer (rather than a double) by adding a capital L after the number. double (aka real or decimal - 2.4, 4.7, pi). Double means that the value is stored in 64 bits (aka double precision) note that both integers and doubles have the class of numeric character (“myeloma” or “michigan”) complex ( 1+3i, 4-7i) - used under the hood for calculations raw (hexadecimal codes like B3 or FF) - used to communicate with devices like printers Note that there is not a true data type for categorical variables, like gender, or race, or ethnicity. These are encoded as factors, which behave mostly like a data type, but are technically a data class. For the purposes of dataframes, factors can be thought of as the 5th data type, but the result of typeof() on a factor variable will be “integer”, as that is how factors are stored, with text attributes for the level names. Note also that there is not a true data type for dates, or date:times, as these are stored as doubles in R. Their typeof() will be double, but their class() will be date or dttm. Storing these as doubles does make it easier to do math to calculate intervals of time. Note that you can test the type of an object in R with the typeof() function, which will return the data type. You can also test for a specific data type - is this the right kind of input, with the is.x functions, like is.character(), is.logical(), or is.numeric(). These will return the logical values TRUE or FALSE. This can be important, as many functions will report (or “throw”) an error when they receive the wrong kind of input data. When needed, you can also convert between data types, by manually coercing an object to a different data type with the as.x functions, like as.character(), as.logical(), as.integer(), or as.numeric() [as.double() ~~ as.numeric()]. Here are 4 examples of testing vectors with the typeof() function. Guess what the returned value will be before you run the code (click on the green arrow at the top right). # you can test vectors with typeof typeof(c(1.7,5.3,9.2)) [1] &quot;double&quot; typeof(c(&#39;hypertension&#39;, &quot;diabetes&quot;, &quot;atherosclerosis&quot;)) [1] &quot;character&quot; typeof(c(2L, 4L)) [1] &quot;integer&quot; typeof(c(TRUE, FALSE, TRUE)) [1] &quot;logical&quot; table(InsectSprays$spray) A B C D E F 12 12 12 12 12 12 typeof(InsectSprays$spray) [1] &quot;integer&quot; Here are 3 examples of coverting vectors from one type to another with as.x() functions. Guess what the returned value will be before you run the code (click on the green arrow at the top right). # you can convert vectors with as.x numeric &lt;- (c(1.7,5.3,9.2)) newvec1 &lt;- as.character(numeric) newvec1 [1] &quot;1.7&quot; &quot;5.3&quot; &quot;9.2&quot; typeof(newvec1) [1] &quot;character&quot; logical &lt;- c(TRUE, FALSE, TRUE) newvec2 &lt;- as.integer(logical) newvec2 [1] 1 0 1 typeof(newvec2) [1] &quot;integer&quot; character &lt;- c(&quot;2.4&quot;, &quot;5.3&quot;, &quot;7.2&quot;) newvec3 &lt;- as.numeric(character) newvec3 [1] 2.4 5.3 7.2 typeof(newvec3) [1] &quot;double&quot; 10.2 Data Structures Data structures are the nouns of programming in R. The data items of different types are organized into data structures. R has many data structures, and these include vector - the most common and basic data structure in R. Most functions in R operate on vectors. These vectorized functions act on every item in the vector, without you having to write a loop. Every item in a vector must be of the same data type. If the items added to a vector are of different data types, they will be silently and automatically coerced to a common data type. Sometimes this is helpful, but sometimes it can surprise you. &lt;Demo&gt; constructing vectors with concatenate, recycling of values, seq and seqalong, length (vs nchar). Use letters and LETTERS - two built in vectors in base R. Automatic coercion can be a common source of problems, especially for beginners. R tries to “helpfully” convert vectors from more specific types to more general types automatically and silently when you have mixed data types in a vector/variable column. Imagine you have a numeric vector of potassium values, like k &lt;- c(4.2, 3.9, 4.5, 4.3, 4.1) This is fine. k is a numeric class vector, with typeof(k) = double. But if the next value from the lab comes back as “sample lysed”, k &lt;- c(4.2, 3.9, 4.5, 4.3, 4.1. “sample lysed”), R will automatically and silently convert this vector to a more general class and type (“character”) The ordering of coercion (from more specific to more general) is: logical -&gt; integer -&gt; numeric -&gt; character -&gt; list Watch out for: data of the wrong type in your variable vectors, and silent changing of your data type - check your variable data types with glimpse() before you dive into data analysis. factors - factors are special vectors used for categorical data. These factors can be ordinal or nominal (unordered), and are important in lots of clinical data, and for modeling or plotting different categories. Factors are essentially integers with special labels (levels) attached. Factors can look like character vectors, but are actually stored as integers, which sometimes leads to unfortunate surprises. matrix - a matrix is a special case of a vector in R. A matrix must also have only a single data type, and has 2 dimensions. The matrix is filled with values by column as the default, and recycles values if needed. Matrices are often used for gene expression arrays and Omics applications to store lots of numeric results. Matrices are also often used “under the hood” for complex calculations, in the linear algebra used to fit many models. &lt;Demo matrix letters, nrow=2 vs nrow =6, dim(), matrix(letters, nrow=13), matrix(letters, nrow=2) list - a list is an all-purpose container for a variety of data types and vectors of any length. Lists are often heterogeneous in both data types and the length of vectors. Lists can even contain other lists. Lists are helpful when you have a related group of heterogeneous objects as results - vectors, dataframes, strings, etc., which can be bundled together in a list. One example of a list occurs in the starwars dataset, which has a row for each character. The column for which films they have appeared in is a list column. Each cell contains a character vectors of varying length, as each character has appeared in anywhere from 1 to 9 movies (as of 2021). starwars %&gt;% select(name, films) # A tibble: 87 × 2 name films &lt;chr&gt; &lt;list&gt; 1 Luke Skywalker &lt;chr [5]&gt; 2 C-3PO &lt;chr [6]&gt; 3 R2-D2 &lt;chr [7]&gt; 4 Darth Vader &lt;chr [4]&gt; 5 Leia Organa &lt;chr [5]&gt; 6 Owen Lars &lt;chr [3]&gt; 7 Beru Whitesun lars &lt;chr [3]&gt; 8 R5-D4 &lt;chr [1]&gt; 9 Biggs Darklighter &lt;chr [1]&gt; 10 Obi-Wan Kenobi &lt;chr [6]&gt; # … with 77 more rows starwars$films %&gt;% head() [[1]] [1] &quot;The Empire Strikes Back&quot; &quot;Revenge of the Sith&quot; [3] &quot;Return of the Jedi&quot; &quot;A New Hope&quot; [5] &quot;The Force Awakens&quot; [[2]] [1] &quot;The Empire Strikes Back&quot; &quot;Attack of the Clones&quot; [3] &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; [5] &quot;Return of the Jedi&quot; &quot;A New Hope&quot; [[3]] [1] &quot;The Empire Strikes Back&quot; &quot;Attack of the Clones&quot; [3] &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; [5] &quot;Return of the Jedi&quot; &quot;A New Hope&quot; [7] &quot;The Force Awakens&quot; [[4]] [1] &quot;The Empire Strikes Back&quot; &quot;Revenge of the Sith&quot; [3] &quot;Return of the Jedi&quot; &quot;A New Hope&quot; [[5]] [1] &quot;The Empire Strikes Back&quot; &quot;Revenge of the Sith&quot; [3] &quot;Return of the Jedi&quot; &quot;A New Hope&quot; [5] &quot;The Force Awakens&quot; [[6]] [1] &quot;Attack of the Clones&quot; &quot;Revenge of the Sith&quot; [3] &quot;A New Hope&quot; Lists can be a bit clunky to work with, as they nest more than a single value into a single cell. Sometimes this nested format can be helpful, and sometimes it is preferable to unnest() the data into a longer format. starwars %&gt;% select(name, films) # A tibble: 87 × 2 name films &lt;chr&gt; &lt;list&gt; 1 Luke Skywalker &lt;chr [5]&gt; 2 C-3PO &lt;chr [6]&gt; 3 R2-D2 &lt;chr [7]&gt; 4 Darth Vader &lt;chr [4]&gt; 5 Leia Organa &lt;chr [5]&gt; 6 Owen Lars &lt;chr [3]&gt; 7 Beru Whitesun lars &lt;chr [3]&gt; 8 R5-D4 &lt;chr [1]&gt; 9 Biggs Darklighter &lt;chr [1]&gt; 10 Obi-Wan Kenobi &lt;chr [6]&gt; # … with 77 more rows # gives you a hidden list of films starwars %&gt;% select(name, films) %&gt;% unnest(cols = c(films)) # A tibble: 173 × 2 name films &lt;chr&gt; &lt;chr&gt; 1 Luke Skywalker The Empire Strikes Back 2 Luke Skywalker Revenge of the Sith 3 Luke Skywalker Return of the Jedi 4 Luke Skywalker A New Hope 5 Luke Skywalker The Force Awakens 6 C-3PO The Empire Strikes Back 7 C-3PO Attack of the Clones 8 C-3PO The Phantom Menace 9 C-3PO Revenge of the Sith 10 C-3PO Return of the Jedi # … with 163 more rows # unnest expands to multiple rows to show detail Lists can be useful for bundling togther related data of different types, like the results of a t-test. t_test_output &lt;- list ( &quot;Welch Two Sample t-test&quot;, c(&quot;data: height by gender&quot;), data.frame(t = -1.5596, df = 37.315, p = 0.1273) ) t_test_output [[1]] [1] &quot;Welch Two Sample t-test&quot; [[2]] [1] &quot;data: height by gender&quot; [[3]] t df p 1 -1.5596 37.315 0.1273 data frame - a very important data structure in R, which corresponds to rectangular data in a spreadsheet. A data frame is a table of vectors (columns of variables) of the same length. If new vectors are added with a different length, an error will occur. Unlike a matrix, each vector (aka column, aka variable) in a dataframe can be a different data type. So you can combine character strings, integers, real numbers, logical values, and factors in a rectangular data frame in which each row is one observation, and the variables/columns/vectors are the values that are measured at that observation. A data frame can be thought of as a strict version of a list, in which each item in the list is an atomic vector (single data type) and must have the same length. You can even have list columns within a data frame. Note that if you run the typeof() function on a dataframe, it will report that it is a list. You can build a dataframe from a set of vectors (variables) by binding these together as columns/variables with the tibble() function. pat_id &lt;- c(1,1,2,2, 3,3) date &lt;- c(lubridate::ymd(&quot;2020-11-07&quot;, &quot;2020-12-03&quot;, &quot;2020-12-02&quot;, &quot;2020-12-15&quot;, &quot;2020-11-09&quot;, &quot;2020-12-02&quot;)) crp &lt;- c(5.1, 3.2, 7.6, 4.1, 4.3, 1.7) new_df &lt;- tibble(pat_id, date, crp) new_df # A tibble: 6 × 3 pat_id date crp &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; 1 1 2020-11-07 5.1 2 1 2020-12-03 3.2 3 2 2020-12-02 7.6 4 2 2020-12-15 4.1 5 3 2020-11-09 4.3 6 3 2020-12-02 1.7 tibble - a tibble is a modern upgrade to the data frame, with clear delineation of data types and printing that does not continuously spew out of your console. It also has the properties of a data frame underneath the additional features. You can convert a dataframe to a tibble with tibble(), or use tibble to build with vectors tibble(new_df) # A tibble: 6 × 3 pat_id date crp &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; 1 1 2020-11-07 5.1 2 1 2020-12-03 3.2 3 2 2020-12-02 7.6 4 2 2020-12-15 4.1 5 3 2020-11-09 4.3 6 3 2020-12-02 1.7 tibble(pat_id, date, crp) # A tibble: 6 × 3 pat_id date crp &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; 1 1 2020-11-07 5.1 2 1 2020-12-03 3.2 3 2 2020-12-02 7.6 4 2 2020-12-15 4.1 5 3 2020-11-09 4.3 6 3 2020-12-02 1.7 You can also build new row-wise tibbles with the tribble() function. tribble( ~pat_id, ~date, ~crp, 1, lubridate::ymd(&quot;2020-12-04&quot;), 5.1, 1, lubridate::ymd(&quot;2020-12-09&quot;), 3.7, 2, lubridate::ymd(&quot;2020-11-23&quot;), 3.6, 2, lubridate::ymd(&quot;2020-11-29&quot;), 1.9, 3, lubridate::ymd(&quot;2020-12-14&quot;), 1.9, 3, lubridate::ymd(&quot;2020-12-27&quot;), 0.6 ) # A tibble: 6 × 3 pat_id date crp &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; 1 1 2020-12-04 5.1 2 1 2020-12-09 3.7 3 2 2020-11-23 3.6 4 2 2020-11-29 1.9 5 3 2020-12-14 1.9 6 3 2020-12-27 0.6 R uses many other custom objects to contain things like a linear model, or the results of a t test. Many functions and packages build specific custom objects which are built upon these basic data structures. Similar to data types, functions in R are often designed to work on a specific data structure, and using the wrong data structure as input to a function will result in errors. It is important to be able to check your data structures as part of your initial DEV (Data Evaluation and Validation) process in order to avoid problems and prevent errors. 10.3 Examining Data Types and Data Structures There are a number of helpful base R functions to help you examine what the objects in your Environment actually are. This is an important step in avoiding errors, or in avoiding needing to diagnose them after the fact. assigning an object (not =) When you create a new object (dataframe, tibble, vector), it is transient. As soon as it prints to the Console, it is gone. If you want to refer back to it, or process it further later, you need to assign it to an object in your environment. When you do this, it will appear in your Environment pane in RStudio. To do this, you use an arrow from your code to the name of the new object. It is helpful to use concise names that are descriptive, in lower case with minimal punctuation (underscores and dashes are fine), no spaces, and that start with letters rather than numbers. It is important to avoid using R function names (like data or df or sum) as names for your objects. You can use either leftward arrows (Less than then dash key) &lt;- or rightward arrows -&gt; (dash key then greater than key) to assign data to an object. # example 1 tall &lt;- starwars %&gt;% filter(height &gt; 200) # example 2 starwars %&gt;% filter(height &lt;70) -&gt; short Leftward arrows are thought of as sort of like the title of a recipe. In the first example above, you can read this as, “I am going to make a tibble of tall characters. I will start with the starwars dataset, then filter by height &gt;100 to get this tall tibble”. But most people don’t think or talk this way, and it is recommended to write “literate code” - which can be read naturally by humans and by computers. These data pipelines should read like sentences. Generally, we use the literate programming format in example 2 above for data wrangling, starting with a subject (starwars dataset), then one or more (automatically indented) verbs (functions) that wrangle the data, then end with the predicate (a new noun - the new resulting dataset). To do this requires the rightward arrow. To avoid hiding the resulting dataset in a thicket of code, remove the automatic indent and put this new object on the left margin. Once you have an object, you can use several functions to examine what you have, and to make sure that it is in the right format for future data wrangling or plotting functions. The str() function, denoting structure, is a good place to start interrogating data structures. Try this out. Run str(starwars) in your RStudio console. str(starwars) tibble [87 × 14] (S3: tbl_df/tbl/data.frame) $ name : chr [1:87] &quot;Luke Skywalker&quot; &quot;C-3PO&quot; &quot;R2-D2&quot; &quot;Darth Vader&quot; ... $ height : int [1:87] 172 167 96 202 150 178 165 97 183 182 ... $ mass : num [1:87] 77 75 32 136 49 120 75 32 84 77 ... $ hair_color: chr [1:87] &quot;blond&quot; NA NA &quot;none&quot; ... $ skin_color: chr [1:87] &quot;fair&quot; &quot;gold&quot; &quot;white, blue&quot; &quot;white&quot; ... $ eye_color : chr [1:87] &quot;blue&quot; &quot;yellow&quot; &quot;red&quot; &quot;yellow&quot; ... $ birth_year: num [1:87] 19 112 33 41.9 19 52 47 NA 24 57 ... $ sex : chr [1:87] &quot;male&quot; &quot;none&quot; &quot;none&quot; &quot;male&quot; ... $ gender : chr [1:87] &quot;masculine&quot; &quot;masculine&quot; &quot;masculine&quot; &quot;masculine&quot; ... $ homeworld : chr [1:87] &quot;Tatooine&quot; &quot;Tatooine&quot; &quot;Naboo&quot; &quot;Tatooine&quot; ... $ species : chr [1:87] &quot;Human&quot; &quot;Droid&quot; &quot;Droid&quot; &quot;Human&quot; ... $ films :List of 87 ..$ : chr [1:5] &quot;The Empire Strikes Back&quot; &quot;Revenge of the Sith&quot; &quot;Return of the Jedi&quot; &quot;A New Hope&quot; ... ..$ : chr [1:6] &quot;The Empire Strikes Back&quot; &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ... ..$ : chr [1:7] &quot;The Empire Strikes Back&quot; &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ... ..$ : chr [1:4] &quot;The Empire Strikes Back&quot; &quot;Revenge of the Sith&quot; &quot;Return of the Jedi&quot; &quot;A New Hope&quot; ..$ : chr [1:5] &quot;The Empire Strikes Back&quot; &quot;Revenge of the Sith&quot; &quot;Return of the Jedi&quot; &quot;A New Hope&quot; ... ..$ : chr [1:3] &quot;Attack of the Clones&quot; &quot;Revenge of the Sith&quot; &quot;A New Hope&quot; ..$ : chr [1:3] &quot;Attack of the Clones&quot; &quot;Revenge of the Sith&quot; &quot;A New Hope&quot; ..$ : chr &quot;A New Hope&quot; ..$ : chr &quot;A New Hope&quot; ..$ : chr [1:6] &quot;The Empire Strikes Back&quot; &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ... ..$ : chr [1:3] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ..$ : chr [1:2] &quot;Revenge of the Sith&quot; &quot;A New Hope&quot; ..$ : chr [1:5] &quot;The Empire Strikes Back&quot; &quot;Revenge of the Sith&quot; &quot;Return of the Jedi&quot; &quot;A New Hope&quot; ... ..$ : chr [1:4] &quot;The Empire Strikes Back&quot; &quot;Return of the Jedi&quot; &quot;A New Hope&quot; &quot;The Force Awakens&quot; ..$ : chr &quot;A New Hope&quot; ..$ : chr [1:3] &quot;The Phantom Menace&quot; &quot;Return of the Jedi&quot; &quot;A New Hope&quot; ..$ : chr [1:3] &quot;The Empire Strikes Back&quot; &quot;Return of the Jedi&quot; &quot;A New Hope&quot; ..$ : chr &quot;A New Hope&quot; ..$ : chr [1:5] &quot;The Empire Strikes Back&quot; &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ... ..$ : chr [1:5] &quot;The Empire Strikes Back&quot; &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ... ..$ : chr [1:3] &quot;The Empire Strikes Back&quot; &quot;Attack of the Clones&quot; &quot;Return of the Jedi&quot; ..$ : chr &quot;The Empire Strikes Back&quot; ..$ : chr &quot;The Empire Strikes Back&quot; ..$ : chr [1:2] &quot;The Empire Strikes Back&quot; &quot;Return of the Jedi&quot; ..$ : chr &quot;The Empire Strikes Back&quot; ..$ : chr [1:2] &quot;Return of the Jedi&quot; &quot;The Force Awakens&quot; ..$ : chr &quot;Return of the Jedi&quot; ..$ : chr &quot;Return of the Jedi&quot; ..$ : chr &quot;Return of the Jedi&quot; ..$ : chr &quot;Return of the Jedi&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr [1:3] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr [1:2] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr [1:2] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr [1:2] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr &quot;Return of the Jedi&quot; ..$ : chr [1:3] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr [1:3] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ..$ : chr [1:3] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ..$ : chr [1:3] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ..$ : chr [1:2] &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ..$ : chr [1:2] &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ..$ : chr [1:2] &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr [1:3] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; ..$ : chr [1:2] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr [1:2] &quot;Attack of the Clones&quot; &quot;Revenge of the Sith&quot; ..$ : chr [1:2] &quot;Attack of the Clones&quot; &quot;Revenge of the Sith&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr [1:2] &quot;Attack of the Clones&quot; &quot;Revenge of the Sith&quot; ..$ : chr [1:2] &quot;Attack of the Clones&quot; &quot;Revenge of the Sith&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr &quot;The Phantom Menace&quot; ..$ : chr [1:2] &quot;Attack of the Clones&quot; &quot;Revenge of the Sith&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr &quot;Attack of the Clones&quot; ..$ : chr [1:2] &quot;Attack of the Clones&quot; &quot;Revenge of the Sith&quot; ..$ : chr &quot;Revenge of the Sith&quot; ..$ : chr &quot;Revenge of the Sith&quot; ..$ : chr [1:2] &quot;Revenge of the Sith&quot; &quot;A New Hope&quot; ..$ : chr [1:2] &quot;Attack of the Clones&quot; &quot;Revenge of the Sith&quot; ..$ : chr &quot;Revenge of the Sith&quot; ..$ : chr &quot;The Force Awakens&quot; ..$ : chr &quot;The Force Awakens&quot; ..$ : chr &quot;The Force Awakens&quot; ..$ : chr &quot;The Force Awakens&quot; ..$ : chr &quot;The Force Awakens&quot; ..$ : chr [1:3] &quot;Attack of the Clones&quot; &quot;The Phantom Menace&quot; &quot;Revenge of the Sith&quot; $ vehicles :List of 87 ..$ : chr [1:2] &quot;Snowspeeder&quot; &quot;Imperial Speeder Bike&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Imperial Speeder Bike&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Tribubble bongo&quot; ..$ : chr [1:2] &quot;Zephyr-G swoop bike&quot; &quot;XJ-6 airspeeder&quot; ..$ : chr(0) ..$ : chr &quot;AT-ST&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Snowspeeder&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Tribubble bongo&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Sith speeder&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Flitknot speeder&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Koro-2 Exodrive airspeeder&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Tsmeu-6 personal wheel bike&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) $ starships :List of 87 ..$ : chr [1:2] &quot;X-wing&quot; &quot;Imperial shuttle&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;TIE Advanced x1&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;X-wing&quot; ..$ : chr [1:5] &quot;Jedi starfighter&quot; &quot;Trade Federation cruiser&quot; &quot;Naboo star skiff&quot; &quot;Jedi Interceptor&quot; ... ..$ : chr [1:3] &quot;Trade Federation cruiser&quot; &quot;Jedi Interceptor&quot; &quot;Naboo fighter&quot; ..$ : chr(0) ..$ : chr [1:2] &quot;Millennium Falcon&quot; &quot;Imperial shuttle&quot; ..$ : chr [1:2] &quot;Millennium Falcon&quot; &quot;Imperial shuttle&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;X-wing&quot; ..$ : chr &quot;X-wing&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Slave 1&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Millennium Falcon&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;A-wing&quot; ..$ : chr(0) ..$ : chr &quot;Millennium Falcon&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Naboo Royal Starship&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Scimitar&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Jedi starfighter&quot; ..$ : chr(0) ..$ : chr &quot;Naboo fighter&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;Belbullab-22 starfighter&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr(0) ..$ : chr &quot;T-70 X-wing fighter&quot; ..$ : chr(0) ..$ : chr(0) ..$ : chr [1:3] &quot;H-type Nubian yacht&quot; &quot;Naboo star skiff&quot; &quot;Naboo fighter&quot; You can see from the output that this is a tibble, with 87 rows and 14 columns. It also meets the definitions of a table and a dataframe. Then the output shows each variable. The first 11 are either character or integer types. The last 3 (films, vehicles, starships) are all list-columns, and are more complicated. Note that glimpse(starwars) provides a prettier version of this output glimpse(starwars) Rows: 87 Columns: 14 $ name &lt;chr&gt; &quot;Luke Skywalker&quot;, &quot;C-3PO&quot;, &quot;R2-D2&quot;, &quot;Da… $ height &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 1… $ mass &lt;dbl&gt; 77, 75, 32, 136, 49, 120, 75, 32, 84, 7… $ hair_color &lt;chr&gt; &quot;blond&quot;, NA, NA, &quot;none&quot;, &quot;brown&quot;, &quot;brow… $ skin_color &lt;chr&gt; &quot;fair&quot;, &quot;gold&quot;, &quot;white, blue&quot;, &quot;white&quot;,… $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;, &quot;yellow&quot;, &quot;bro… $ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47… $ sex &lt;chr&gt; &quot;male&quot;, &quot;none&quot;, &quot;none&quot;, &quot;male&quot;, &quot;female… $ gender &lt;chr&gt; &quot;masculine&quot;, &quot;masculine&quot;, &quot;masculine&quot;, … $ homeworld &lt;chr&gt; &quot;Tatooine&quot;, &quot;Tatooine&quot;, &quot;Naboo&quot;, &quot;Tatoo… $ species &lt;chr&gt; &quot;Human&quot;, &quot;Droid&quot;, &quot;Droid&quot;, &quot;Human&quot;, &quot;Hu… $ films &lt;list&gt; &lt;&quot;The Empire Strikes Back&quot;, &quot;Revenge o… $ vehicles &lt;list&gt; &lt;&quot;Snowspeeder&quot;, &quot;Imperial Speeder Bike… $ starships &lt;list&gt; &lt;&quot;X-wing&quot;, &quot;Imperial shuttle&quot;&gt;, &lt;&gt;, &lt;&gt;… If you want more detail about a given variable inside this dataframe, you can use typeof() or class(). typeof(starwars$mass) [1] &quot;double&quot; class(starwars$mass) [1] &quot;numeric&quot; For character variables, typeof() is the same as class(). But these are different for doubles and factors. Sometimes you want a quick look at all the variable names of a dataset. The functions names() or colnames() can quickly get you a vector of these names. names(starwars) [1] &quot;name&quot; &quot;height&quot; &quot;mass&quot; &quot;hair_color&quot; [5] &quot;skin_color&quot; &quot;eye_color&quot; &quot;birth_year&quot; &quot;sex&quot; [9] &quot;gender&quot; &quot;homeworld&quot; &quot;species&quot; &quot;films&quot; [13] &quot;vehicles&quot; &quot;starships&quot; colnames(starwars) [1] &quot;name&quot; &quot;height&quot; &quot;mass&quot; &quot;hair_color&quot; [5] &quot;skin_color&quot; &quot;eye_color&quot; &quot;birth_year&quot; &quot;sex&quot; [9] &quot;gender&quot; &quot;homeworld&quot; &quot;species&quot; &quot;films&quot; [13] &quot;vehicles&quot; &quot;starships&quot; If you want the dimensions of your dataset, you can use dim(). To just get the number of rows or columns, you can use nrow() or ncol(). When you have a numeric matrix, sometimes you have a ’bonus” column of rownames, which is a special column that specifies the observation, but does not have a normal column name. This keeps character strings out of the matrix, which can only have one data type. This is kind of a pain, especially if you need to access the information in the rownames column. When this is the case, the dplyr function rownames_to_column(matrix_name) is very helpful. The default name is rowname, but you can supply a better one. Run the example below in your Console pane. rownames_to_column(mtcars, var = &quot;make_model&quot;) make_model mpg cyl disp hp drat wt qsec 1 Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 2 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 3 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 4 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 5 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 6 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 7 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 8 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 9 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 10 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 11 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 12 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 13 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 14 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 15 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 16 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 17 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 18 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 19 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 20 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 21 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 22 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 23 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 24 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 25 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 26 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 27 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 28 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 29 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 30 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 31 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 32 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 vs am gear carb 1 0 1 4 4 2 0 1 4 4 3 1 1 4 1 4 1 0 3 1 5 0 0 3 2 6 1 0 3 1 7 0 0 3 4 8 1 0 4 2 9 1 0 4 2 10 1 0 4 4 11 1 0 4 4 12 0 0 3 3 13 0 0 3 3 14 0 0 3 3 15 0 0 3 4 16 0 0 3 4 17 0 0 3 4 18 1 1 4 1 19 1 1 4 2 20 1 1 4 1 21 1 0 3 1 22 0 0 3 2 23 0 0 3 2 24 0 0 3 4 25 0 0 3 2 26 1 1 4 1 27 0 1 5 2 28 1 1 5 2 29 0 1 5 4 30 0 1 5 6 31 0 1 5 8 32 1 1 4 2 10.4 Functions Functions are the verbs of programming in R. Functions act on existing data objects to rearrange them, change them, analyze them, plot them, or report them. Functions in R can do almost anything, and can work together in pipelines (aka chains) to do more complex and interesting things. Functions can call (activate) other functions, and you can build your own custom functions, which comes in handy when you want to do something similar multiple times, like summarizing data on 5 different variables. You could run a custom summary function on each variable (the argument to the function) each time, or use a programming function like map to run the function over a vector or list of the variables needed. You can see the inner workings of a function by just entering it into the RStudio console without parentheses. For example, for sd (standard deviation). sd function (x, na.rm = FALSE) sqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), na.rm = na.rm)) &lt;bytecode: 0x7fdd2ef13768&gt; &lt;environment: namespace:stats&gt; You can see the inner workings of the sd function. This function takes the variance, then the square root of a vector, which is correct for producing a standard deviation. To run any function, you need to supply the parentheses, which normally contain the arguments (options) and identify the input data. Note that in a data pipe, the first argument is the incoming data, and does not need to be explicitly named in tidyverse functions. In older, non-tidyverse functions, data is often not the first argument, and needs to be named explicitly in a data pipeline, with the argument data = . telling the function to use the incoming data from the pipeline. To actually run any function, you need to include the parentheses at the end, even if you do not include any arguments. Two examples to try on your own are: Sys.Date() installed.packages() Give these a try in your Console pane. 10.5 Packages Packages make functions transportable and shareable. You can bundle a group of related functions into a package, and put it on Github, or even on CRAN, where other people can download it and use these functions for themselves. This can be helpful for miscellaneous functions that co-workers often use, or for a group of functions that work well together in a particular workflow. Packages can also contain data, which can be used for teaching examples, or as a way to share data. Packages can also contain tutorials related to the functions or the data in the package. Packages are like apps that you add to a phone to give it new functions. Packages enhance what you can do with R. When you start an R session in RStudio, your base R installation comes with 9 packages. You can see these listed by Clicking on Session/Restart R, then running print(.packages()) or search() in the Console pane. These packages include stats, graphics, grDevices, utils, datasets, devtools, usethis, methods, and base. You will also see the Global Environment and rstudio tools, and any autoloaded packages in this list. These are listed by their order on the search path. When you run (aka call) a function, R will search this path in order, starting with the Global Environment, then sequentially in the packages loaded, in order, for the function. This means that a function in a package loaded last (at the beginning of the search path) will mask a function with the same name in a package loaded earlier (like base, which is at the end of the search path). When you run the library() function to load a package, R will warn you about conflicts if you load a package with a function with the same name as a function in an already-loaded package. You can install more packages with the .install.packages() function, and see a pretty printed version of a list of all the installed packages with the library() function (with no arguments in the parentheses). You can get a matrix version (less pretty, but more accessible/manipulable) with the .installed.packages() function in the Console pane. You only have to install a package into your current library once. But to use a package in a current R session, you have to load it with the library(packagename) function. Let’s try this out on your own computer, within RStudio. First, within RStudio, go to the top menu bar and select Session/Restart R (Shift-Cmd-F10) to restart R and get a fresh, clean session. Then go to the Console pane and run the search() function. [Type in search() and press Return] You will see the search path, and all the currently loaded packages, in order. You can get a vector of just the loaded packages by going to the Console pane and running print(.packages()) Now load the tidyverse package by running the following in the Console pane: library(tidyverse) Note that there are 8 packages installed in addition to the {tidyverse} package (tidyverse is a meta-package, composed of multiple packages). Also note that there are two Conflicts reported. Two functions in the {dplyr} package, filter() and lag() have the same names as functions in the {stat} package. This means that when you run filter() or lag(), the default will be to run the {dplyr} versions, as these were installed last. These are earlier in the search path, and will mask the {stats} versions. You can still use the stats versions if you want to, by using the package prefix, with the format stats::filter() to make clear that you want this version, rather than dplyr::filter(). Now check what your search path looks like with search(). The nine packages from tidyverse (including tidyverse itself) should now be right after .GlobalEnv at the front of the search path, as they were installed last. If you now run print(.packages()) in the Console pane, you should now have 18 packages loaded. &lt;Demo - run library(tidyverse) - see the Conflicts warning about filter and lag.&gt;&lt;run search() again to see that tidyverse is a meta-package - with multiple new packages loaded in the search path). Packages are stored in libraries. You have a global library, and you can have project-specific libraries. You can see the file paths to your available libraries with the function .libPaths(). This will print (to the Console) the path to your package library for your specific major version of R. # You can check your own library path # Run this function in your Rstudio Console to find the current library on your computer .libPaths() [1] &quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library&quot; 10.6 The Building Blocks of R Now you know about the 4 key building blocks of R, and how they work. Being able to check data types with typeof() and data structures with str() will help you avoid running functions on data that is not in the right format for a particular function. Dataframes and tibbles will be the key data structures which you will frequently manipulate with functions to wrangle, analyze, visualize, and report your medical research. "],["tips-for-hashtag-debugging-your-pipes-and-ggplots.html", "Chapter 11 Tips for Hashtag Debugging your Pipes and GGPlots 11.1 Debugging 11.2 The Quick Screen 11.3 Systematic Hunting For Bugs in Pipes 11.4 Systematic Hunting For Bugs in Plots 11.5 Hashtag Debugging 11.6 Pipe 2 11.7 Plot 2 11.8 Plot3 11.9 Pipe 3", " Chapter 11 Tips for Hashtag Debugging your Pipes and GGPlots 11.1 Debugging While layering functions with piping in the tidyverse and the plus sign in ggplots is a great way to keep your code clear, your will invariably find some bugs in your code. This can be pretty frustrating, especially when you spend a lot of time staring at the code and it finally turns out to be something trivial, like a missing parenthesis, a pipe in place of a + (or the reverse), a missing aes() statement, or a mistyped function (fitler instead of filter). So how can you systematically approach debugging a pipe or plot, and find and fix your problem efficiently without spending tons of time? 11.2 The Quick Screen Start by checking for common errors - check for The Big 5. Click just to the right of each final parenthesis on each line. Does this result in highlighting the first parenthesis on that line of code? If not, you are probably missing a closing parenthesis. When piping, check that you have a proper pipe %&gt;% at the end of each line, except for the last line. When plotting, check that you have a plus sign + at the end of each line, except for the last line. Error message about a missing function - Error in function(arguments) : could not find function \"func\". Either you are calling a function from a library you have not loaded, or you mis-typed the function. If the library was not loaded, go back to the top and load the library that this function comes from. If you mis-typed the function (for example, /selcet), fix the typographical error. The error message will tell you which function seems to be missing. Error message about a missing object - Error: object 'xxxobj' not found. Make sure that you have typed the names of the dataframe and the variables correctly. Each of these is a data object. The error message will tell you which object seems to be missing. Your Turn: Search for The Big 5 in the pipe below (Hint - there are 5 Bugs to Be Found) You should end up with 4 columns of 10 rows, sorted by efficiency, when all of the bugs have been fixed. mtcars %&gt;% filter(cyl &gt;4) %&gt;% select(mpg, hp, displ) %&gt;% mutate(efficiency = hp/disp ) + filtre(efficiency &gt; 0.5) %&gt;% arrange(desc(efficiency) %&lt;% slice(1:10) ## Error: &lt;text&gt;:8:0: unexpected end of input ## 6: arrange(desc(efficiency) %&lt;% ## 7: slice(1:10) ## ^ mtcars %&gt;% filter(cyl &gt;4) %&gt;% select(mpg, hp, displ) %&gt;% # watch for typos in object names mutate(efficiency = hp/disp ) + # watch for misplaced + vs %&gt;% filter(efficiency &gt; 0.5) %&gt;% # watch for typos arrange(desc(efficiency) %&lt;% # watch for mistyped pipes slice(1:10) ## Error: &lt;text&gt;:8:0: unexpected end of input ## 6: arrange(desc(efficiency) %&lt;% # watch for mistyped pipes ## 7: slice(1:10) ## ^ mtcars %&gt;% filter(cyl &gt;4) %&gt;% select(mpg, hp, displ) %&gt;% mutate(efficiency = hp/disp ) %&gt;% filter(efficiency &gt; 0.5) %&gt;% arrange(desc(efficiency)) %&gt;% # watch for missing closing parentheses slice(1:10) ## Error in `select()`: ## ! Can&#39;t subset columns that don&#39;t exist. ## ✖ Column `displ` doesn&#39;t exist. 11.3 Systematic Hunting For Bugs in Pipes What if it is not one of the Big 5 Pipe Bugs, and you need to hunt systematically? Let’s start by adding a pipe to the last line (slice(1:10)), and then a return() as a new line following slice(1:10), so that you now have an 8 line pipe. Note it will be blue - this is OK. It just shows that this is an important function. Adding return() just returns the result of the previous 7 lines, which does not seem like much, but it makes it a lot easier to use our debugging MVP, the hashtag. First, in debugging a pipe, you want to be able to quickly select and run lines repeatedly. You can do this with your mouse, but it is slow and sometimes inaccurate. You can do this faster with the keyboard. To run a whole pipe, just click anywhere in the pipe and press the key combination: Cmd-Shift-Return on the Mac Ctrl-Shift-Return on PC With this key combination, you don’t have to use your mouse to select lines to run. This will run the whole pipe or plot. Now you have to take control of exactly which lines of the pipe will run. You want to run a series of experiments to isolate the bug. Start by running the pipe from the top. Run just your first line (data) by putting a hashtag just before the first pipe %&gt;% and pressing Cmd-Shift-Return while still on that line. If that works, delete the hashtag and repeat the process on the 2nd line. You can stop running the pipe after only 2 lines by placing a hashtag just before the 2nd pipe. Then press your Cmd-Shift-Return key combo to run just the first 2 lines. If that works, try running the first 3 lines, by deleting the hashtag on line 2, and putting it just before the pipe on line 3 (Copy-Paste can help). Use this approach to run successively more lines of the pipe in the code chunk below. In which line of the pipe below do you hit the first error (bug)? iris %&gt;% filter(Sepal.Length &lt;5) %&gt;% select(Sepal.Length, Sepal.Width, Species) %&gt;% mutate(Sepal.Area = Sepal.Lngth * Sepal.Width) %&gt;% filter(Sepal.Area &gt;10) %&gt;% arrange(desc(Sepal.Area) %&gt;% slice(1:10) %&gt;% return() ## Error: &lt;text&gt;:10:0: unexpected end of input ## 8: return() ## 9: ## ^ iris %&gt;% filter(Sepal.Length &lt;5) %&gt;% select(Sepal.Length, Sepal.Width, Species) %&gt;% mutate(Sepal.Area = Sepal.Lngth * Sepal.Width) %&gt;% # watch for typos filter(Sepal.Area &gt;10) %&gt;% arrange(desc(Sepal.Area) %&gt;% # watch for missing parentheses slice(1:10) %&gt;% return() ## Error: &lt;text&gt;:10:0: unexpected end of input ## 8: return() ## 9: ## ^ Great. Now you know the bug is somewhere in lines 4-7. You can selectively turn off one line at a time by putting a hashtag at the beginning of the line. Use this approach to turn off line 5 (filter). Does this fix the pipe? If not, try lines 4,6,7 individually. Turning off which one gets rid of the error/changes the error? Changing the error means that you made it at least a little bit farther before a new error occurred. Now hunt in the ‘commented out/hashtagged’ line for an error you can fix. Once you think you have fixed it, try running the pipe up to and including that line (hashtag just before pipe in that line). Does that work? If yes, you have made progress. Keep going line by line until you find and fix the next bug, until you can get the whole pipe to run. 11.4 Systematic Hunting For Bugs in Plots The Big 5 Common Errors in Plots are slightly different. Click just to the right of each final parenthesis on each line. Does this result in highlighting the first parenthesis on that line of code? If not, you are probably missing a closing parenthesis. When plotting, check that you have a plus sign + at the end of each line, except for the last line. Missing aes() mapping. It is easy to get excited about your ggplot and declare variables in the ggplot statement, and forget about wrapping the mapping of x and y in an aesthetic function. Check to make sure that every time you map variables in your data to a plot component (x,y, color, shape, size, etc.) that this occurs inside an aes() call. Error message about a missing function - Error in function(arguments) : could not find function \"func\". Either you are calling a function from a library you have not loaded, or you mis-typed the function. If the library was not loaded, go back to the top and load the library that this function comes from. If you mis-typed the function (for example, /selcet), fix the typographical error. The error message will tell you which function seems to be missing. Error message about a missing object - Error: object 'xxxobj' not found. Make sure that you have typed the names of the dataframe and the variables correctly. Each of these is a data object. The error message will tell you which object seems to be missing. ## Your Turn to Debug a Plot Before hashtag-debugging the plot below, we will cap the plot code with an additional final line, by adding a + to the final line theme_minimal() and a new following line: NULL. This functions like return does for pipes - we can now use hashtags to turn off lines without causing errors because of missing pipes. Go ahead and add the + and NULL to the plot in the code chunk below. ## Error in theme_mnimal(): could not find function &quot;theme_mnimal&quot; Now use the same hashtag approach to run the lines of the plot code sequentially, adding one line at a time, with a hashtag placed just before the pipe, and using the Cmd-Shift-Return key combination. When you hit an error, use the hashtag at the beginning of each line that is a suspect in this Bug Hunt, and turn off one line at a time until you isolate the Bug. Then try to fix it, and run the whole plot chunk one line at a time. 11.5 Hashtag Debugging The elements of Hashtag Debugging of a code chunk are simply stated in 7 steps: Add a placeholder line to the end of the code chunk %&gt;% return() for data pipes + NULL for plots Insert a hashtag just before the line extender (%&gt;% for datapipes, + for plots) near the top of the code chunk Use Cmd-Shift-Return or Ctrl-Shift-Enter to run the code up to the hashtag. Keep running more lines by moving the hashtag down one line until you hit an error. Try to find and fix the error in that line. If needed, put a hashtag at the beginning of a line to turn off that line and run the rest of the pipe. If the error IS resolved (or changes), you have fixed the first error. Now run incrementally more lines (steps 2-4) until you have found a new error or completely fixed the pipe. Use your new skills. Figure out which Star Wars characters are overweight (by human standards) Try debugging the data pipe below. Work through each step in the process. 11.6 Pipe 2 starwars %&gt;% filter(height &lt;180) %&gt;% select(name, height, mass, gender homeworld, species) %&gt;% mtate(bmi = mass^2/height) %&gt;% filter(bmi&gt;=25) %&lt;% select(name, height, mass, bmi) %&gt;% arrange(dsc(bmi)) %&gt;% slice(1:15) ## Error: &lt;text&gt;:3:37: unexpected symbol ## 2: filter(height &lt;180) %&gt;% ## 3: select(name, height, mass, gender homeworld ## ^ Good work! Now try debugging the problematic plotting code chunk below. There are multiple errors. Work through each step in the process. 11.7 Plot 2 murders %&gt;% ggplot(x = population/10^6, y = total, label = abb) + geom_abline(intercept = log10(r), lty=2, col=darkgrey) + geom_point(aes(color==region), size = 3) + geom_text_repel() + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) ## Error in layer(data = data, mapping = mapping, stat = StatIdentity, geom = GeomAbline, : object &#39;darkgrey&#39; not found ggtitle(&quot;US Gun Murders in 2010&quot;) + scale_color_discrete(name=&quot;Region&quot;) ## NULL Here is another complex plot. Work through each step to completely debug this one. When it works, it will make a heatmap of measles cases in the US by month and year, with the introduction of the measles vaccine marked as an important event. 11.8 Plot3 us_contagious_diseases %&gt;% filter(!state%in%c(&quot;Hawaii&quot;,&quot;Alaska&quot;) &amp; disease == the_disease) %&gt;% mutate(rate = count / population * 10000 * 52 / weeks_reporting) %&gt;% mutate(state = reorder(state, rate) %&gt;% ggplot(aes(year state, fill = rate)) %&gt;% geom_tile(color = &quot;grey50&quot;) + scale_x_cntinuous(expand=c(0,0)) + scale_fill_gradientn(colors = brewer.pal(9, &quot;Reds&quot;), trans = &quot;sqrt&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) + theme_minimal() + theme(panel.grid = element_blank()) + ggttle(the_disease) + ylab(&quot;&quot;) + xlab(&quot;&quot;) ## Error: &lt;text&gt;:5:19: unexpected symbol ## 4: mutate(state = reorder(state, rate) %&gt;% ## 5: ggplot(aes(year state ## ^ Now let’s try a final data pipe debugging. Use your skills to make this one work. 11.9 Pipe 3 gapminder %&gt;% filter(year == 1965) %&gt;% filter(!is.na(infant_mortality) %&gt;% mutate(adult_survival = life_expectancy/infant_mortality) %&gt;% select(country, adultsurvival, continent) %&gt;% group_by(continent) %&gt;% summarize(mean_adult_surv = mean(adult_survival), sd_adult_surv = stdev(adult_survival)) %&gt;% arrange(mean_adult_surv) ## Error: &lt;text&gt;:11:0: unexpected end of input ## 9: arrange(mean_adult_surv) ## 10: ## ^ "],["finding-help-in-r.html", "Chapter 12 Finding Help in R 12.1 Programming in R 12.2 Starting with Help! 12.3 The Magic of Vignettes 12.4 Googling the Error Message 12.5 You Know What You Want to Do, but Don’t Know What Package or Function to Use 12.6 Seeking Advanced Help with a Minimal REPREX", " Chapter 12 Finding Help in R Because the path to something valuable is never easy. Learning R programming is hard, and at times will be frustrating. You will need help frequently. But the journey is a lot easier if you learn how to ask the R community for help when you are stuck. There is a very good chance that someone else has been stuck in the same place, and that there is good advice available. 12.1 Programming in R Saving programs and data in R is critical to producing reproducible medical research. But for most people, coding is not easy, comes with lots of syntax errors and cryptic error messages, and can be frustrating. One of the key skills in programming in R is finding help when you are stuck. In this chapter, we will explain several ways to find help in R, moving from the simple to the more complex. 12.2 Starting with Help! The simplest approach to getting help in R is to use the help() function. In the console, you can type help(“lm”) or help(“geom_boxplot”) or help(“filter”) to make the reference materials appear in the Help tab in the lower right quadrant of RStudio. Note that there may be more than one match - in which case it will show you a list. The search for help(“filter”) is a good example, as many packages have function that includes ‘filter’ in the name. In the code block below, use help(‘filter’) to find the details on the filter() function from the {dplyr} package. help() You can also get help by going directly to the Help tab, and entering a term in the search box (top right of the Help window, with a magnifying glass icon), and pressing return. Help will take you directly to the documentation of a package or a function, which includes Description Usage (generic function with argument defaults) Arguments - explanation of each argument Details provided by the package author Examples (which can sometimes be cryptic) This is useful if you are just trying to remember the arguments to a function and/or their defaults, but is often not terribly helpful for beginners trying to understand how to use a package or a particular function. Let’s explore the dplyr::filter documentation a bit. There are 3 arguments listed, though … is a bit cryptic. The first argument is the .data, which is often piped in. The … argument lets you insert a variety of logical statements to filter by, and the .preserve argument defaults to recalculating grouping structure after filtering (default value is FALSE). The Details section recommends filtering before grouping for better speed (because of that recalculation of grouping structure). There are details on how grouping and rownames are affected by filter, followed by a mention of 3 variants of filter, filter_all, filter_if, and filter_at, which allow you to work on selections of variables. This is followed by some examples of the use of filter. 12.3 The Magic of Vignettes While the function documentation that you can find with help() explains the nuts and bolts (and arguments) for a function, it does not tell you much about the intended use, or the structure of the data that you should use this function on. Wrangling data into the right structure is often critical to successfully using a function. This makes package vignettes very helpful. The browseVignettes(‘package_name’) function can help you find the available vignettes for a given package. Edit the code block below to browse vignettes for the tidyr package. Remember to use quotes. this will open a webpage list to read the vignette and see examples of the use of tidyr. browseVignettes() ## starting httpd help server ... done Another useful approach is to search on the web for vignettes. You can Google “flextable vignettes” fo find examples of how to use the flextable package. Jump to a web browser and try it out. You will see nice explanations, and examples of code that you can copy and paste back into RStudio and run - either in the Console (interactively), or in a script. Try out a few of the layout and formatting examples with flextable. Often when you are first using a package, the vignette is the best place to start to get oriented to the intended use of the package and its functions. Several newer packages have dedicated documentation websites make with the pkgdown package. You can search for these by Googling packagename and “tidyverse”” for packages in the tidyverse. Google the package website for the forcats package. Read the Home text, then check out the Reference page (tab) for each function. The Articles or “Get Started” tab often contains examples. A general strategy is to Google “ package in R”. Try googling a few others, like ggpubr RVerbalExpressions sf gtsummary arsenal 12.4 Googling the Error Message It is very common to get an obscure error message. These are intended to be helpful, but often are not. A few common error messages and their usual causes are: Error Message Common Causes “could not find function x” package not installed, or function misspelled “subscript out of bounds” trying to find the 15th item in a vector when there are only 12 “error in if” an if statement trying to deal with non-logical data or NAs. “cannot open” trying to read a file that can’t be found “object x not found” using an argument that needs quotes without the quotes. If there are no quotes, R assumes that you are looking for an object already defined in the Environment tab. An R error message cheat sheet can be found [here] (http://varianceexplained.org/courses/errors/) Over time, you will learn to recognize common errors. But until you do (and even after you do), a helpful way out of a frustrating error message is to copy the error message, and paste it into a Google search. Some one has had that error before and asked for help on the internet. You can learn from their experience, and see what solutions other folks have come up with. Run the code block below to generate an error, then google the error message and see if you can figure out how to fix it. ## Error in `check_required_aesthetics()`: ## ! stat_smooth requires the following missing aesthetics: x and y The problem is that the ggplot function does not include the aesthetics layer around x and y - aes(x, y) is required inside the ggplot() function to tell ggplot that the variables time and conc should be mapped to x and y. 12.5 You Know What You Want to Do, but Don’t Know What Package or Function to Use 12.5.1 CRAN Task Views There are two general approaches to this problem. If you know a general topic area, and are looking for packages, the CRAN Task Views can be really helpful in finding a package that does what you need. CRAN Task Views are lists of packages that do useful things in a certain topic area. Pick your task, and it will supply a useful list of likely packages, with a description of what the package does and a link to the documentation. Look for packages to help you block randomize patients into clinical trials on CRAN [here] (https://cran.r-project.org/web/views/). The ClinicalTrials link will take you to an extensive list of packages that help with a variety of needs for clinical trials. You will fairly quickly find the blockrand package is one that suits your needs, though there are a few other options available. 12.5.2 Google is Your Friend Try googling “How to do” task “in R”. Try “How do do block randomization in R”. Several options come up, including the blockrand package. Try another one - google “how to put significance bars in a ggplot in R”. Several options come up, including ggsignif and ggpubr. 12.6 Seeking Advanced Help with a Minimal REPREX There is a large R community, and many experienced people are willing to help you when you are stuck. However, it can be very difficult to accurately explain your problem to someone who is not at your computer. This problem has led to the concept of the minimal REProducible EXample (minimal REPREX), and the reprex package. The reprex package helps you post a useful example on websites like the RStudio Community or Stack Overflow to ask for help. A minimal reproducible example includes: 1. All of the libraries needed 2. a small (‘toy’) dataset, with no extra columns (just the ones needed), and a limited number of rows (often 5-6). 3. Your code, which is not quite working, or producing a surprising result 4. A clear explanation of the result you are trying to get with your code (sometimes this is a jpeg of a graph, or a table of what you want the data to look like after processing). There is a nice explanation of how to reprex for beginners here. More resources and details can be found in the RStudio Community FAQ here Before we start making a reprex of our own, let’s look at a few examples on RStudio Community. https://community.rstudio.com/t/could-not-plot-geometric-point/42558 https://community.rstudio.com/t/how-to-subset-a-data-frame-by-a-rowvalue/43514/8 https://community.rstudio.com/t/pivot-wider-tidyselect-and-col-how-to-exclude-variables/41191 https://community.rstudio.com/t/new-to-r-would-like-to-find-a-way-to-find-the-mean-of-each-states/39161 Now you have a feel for what a reprex looks like, and how folks ask and answer questions on RStudio Community. So let’s imagine that you are trying to plot data on blood pressure for men and women, and you want to color the points differently for men and women. But you don’t know how. Let’s start with the 4 steps to a reprex. In a new script you need to: include all libraries needed. In this case, library(tidyverse) covers any data wrangling and ggplot2 Include your data. This should be a minimal or ‘toy’ dataset. Be SURE you are not including any fields that are Protected Health Information (PHI) or identifiers. You can do this one of several ways: use a built-in dataset (https://www.rdocumentation.org/packages/datasets/versions/3.6.1) and select() a few key variables and filter() down to a reasonable number of rows (or use head() to get 6 rows), or take your own data and select only the columns needed and use filter() or head() for a minimal number of rows. Make sure not to use any Protected Health Information (PHI).Then use dput() to add the data to the reprex and assign it to an object medicaldata::blood_storage %&gt;% select(Recurrence, PVol, TVol, AA, FamHx) %&gt;% head() -&gt; small_blood dput(small_blood) ## structure(list(Recurrence = c(1, 1, 0, 0, 0, 0), PVol = c(54, ## 43.2, 102.7, 46, 60, 45.9), TVol = c(3, 3, 1, 1, 2, 2), AA = c(0, ## 0, 0, 0, 0, 0), FamHx = c(0, 0, 0, 0, 0, 0)), row.names = c(NA, ## 6L), class = &quot;data.frame&quot;) # output of data will show up in the console # copy this and assign it to an object to start your reprex. # as below dataset &lt;- structure(list(Recurrence = c(1, 1, 0, 0, 0, 0), PVol = c(54, 43.2, 102.7, 46, 60, 45.9), TVol = c(3, 3, 1, 1, 2, 2), AA = c(0, 0, 0, 0, 0, 0), FamHx = c(0, 0, 0, 0, 0, 0)), row.names = c(NA, 6L), class = &quot;data.frame&quot;) build a toy dataset from scratch with tibble:tribble(). Each ~tilde_var gives you a variable name, each separated by commas, and each value is separated by a comma. And, remember not to put a comma after the last value (a common mistake). dataset &lt;- tibble::tribble( ~pat_id, ~sbp, ~dbp, ~hr, 001, 147, 92, 84, 002, 158, 99, 88, 003, 137, 84, 67, 004, 129, 92, 73 ) Use the datapasta package to copy in some data from a website or spreadsheet Install the package, copy a (small) amount of data. Use the add-in to paste your data, usually as a data frame or a tribble (you can choose either). Remember to assign it to an object like dataset. 12.6.0.1 Built in datasets In the code chunk below, examine the built-in dataset, infert. Then select only education, age, and parity. Then use head() to get only 6 rows. Assign this to data and print it out glimpse(datasets::infert) ## Rows: 248 ## Columns: 8 ## $ education &lt;fct&gt; 0-5yrs, 0-5yrs, 0-5yrs, 0-5yrs, 6-1… ## $ age &lt;dbl&gt; 26, 42, 39, 34, 35, 36, 23, 32, 21,… ## $ parity &lt;dbl&gt; 6, 1, 6, 4, 3, 4, 1, 2, 1, 2, 2, 4,… ## $ induced &lt;dbl&gt; 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2,… ## $ case &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ spontaneous &lt;dbl&gt; 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,… ## $ stratum &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, … ## $ pooled.stratum &lt;dbl&gt; 3, 1, 4, 2, 32, 36, 6, 22, 5, 19, 2… glimpse(datasets::infert) ## Rows: 248 ## Columns: 8 ## $ education &lt;fct&gt; 0-5yrs, 0-5yrs, 0-5yrs, 0-5yrs, 6-1… ## $ age &lt;dbl&gt; 26, 42, 39, 34, 35, 36, 23, 32, 21,… ## $ parity &lt;dbl&gt; 6, 1, 6, 4, 3, 4, 1, 2, 1, 2, 2, 4,… ## $ induced &lt;dbl&gt; 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2,… ## $ case &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ spontaneous &lt;dbl&gt; 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,… ## $ stratum &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, … ## $ pooled.stratum &lt;dbl&gt; 3, 1, 4, 2, 32, 36, 6, 22, 5, 19, 2… data &lt;- infert %&gt;% select(education, age, parity) %&gt;% head() data ## education age parity ## 1 0-5yrs 26 6 ## 2 0-5yrs 42 1 ## 3 0-5yrs 39 6 ## 4 0-5yrs 34 4 ## 5 6-11yrs 35 3 ## 6 6-11yrs 36 4 12.6.0.2 Filtering your dataframe object In the code chunk below, examine the local dataset, emerg_dept, which has counts of ED arrivals, how many breached the UK 4 hour guarantee, and how many got admitted. Then select() only org_code, attendances, breaches, and admissions. Then arrange() to have the top attendances at the top, use top_n(10) to get only the top 10 rows. Assign this to data and print it out emerg_dept ## # A tibble: 50 × 6 ## period org_code type attendances breaches admissions ## &lt;date&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018-07-01 RRK 1 32209 6499 11332 ## 2 2018-07-01 R1H 1 28357 6294 7986 ## 3 2018-07-01 RW6 1 23887 4641 6282 ## 4 2018-07-01 R0A 1 22012 4669 6818 ## 5 2018-07-01 RDU 1 21043 1941 6519 ## 6 2018-07-01 RAL 1 20481 2529 4530 ## 7 2018-07-01 RF4 1 19303 4606 5004 ## 8 2018-07-01 RWE 1 18890 4861 4522 ## 9 2018-07-01 RXF 1 18828 2731 3981 ## 10 2018-07-01 RQM 1 18560 1064 4130 ## # … with 40 more rows emerg_dept %&gt;% select(org_code, attendances:admissions) %&gt;% arrange(desc(attendances)) %&gt;% top_n(10) -&gt; data ## Selecting by admissions data ## # A tibble: 10 × 4 ## org_code attendances breaches admissions ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 RRK 32209 6499 11332 ## 2 R1H 28357 6294 7986 ## 3 RW6 23887 4641 6282 ## 4 R0A 22012 4669 6818 ## 5 RDU 21043 1941 6519 ## 6 RF4 19303 4606 5004 ## 7 RR8 17889 3507 5345 ## 8 RTG 17591 2757 5302 ## 9 RJE 16622 3758 5855 ## 10 R1K 11922 3038 6098 12.6.0.3 Toy datasets Run the code below to build a toy dataset with patient_id, sbp, dbp. Then edit the code to add 4 values for heart rate and 4 values for respiratory rate df &lt;- data.frame( patient_id = 1:4, sbp = c(151, 137, 129, 144), dbp = c(92, 85, 79, 66) ) df ## patient_id sbp dbp ## 1 1 151 92 ## 2 2 137 85 ## 3 3 129 79 ## 4 4 144 66 12.6.0.4 Fun with Datapasta Example datapasta is a package for pasting data. It is super-helpful when you just want to quickly get a bit of data from a website or a spreadsheet. install the package {datapasta}, if you don’t have it already run library(datapasta) Go to the website, https://en.wikipedia.org/wiki/Health_insurance_coverage_in_the_United_States, and find the large table named, “Percent uninsured (all persons) by state, 1999–2014”. Carefully copy the table without the title line. Then use the Addins dropdown to “Paste as Tribble” into your code file. Assign the resulting tibble to an object named ins_data. You will get funny names for the columns. Change these with names(ins_data), and assign state and 1999:2014 to the names. names(ins_data) &lt;- c('state', c(1999:2014)) Then filter to get rid of DC and United States. You should end up with 50 rows. Show/Hide Solution ins_data &lt;- tibble::tribble( ~V1, ~V2, ~V3, ~V4, ~V5, ~V6, ~V7, ~V8, ~V9, ~V10, ~V11, ~V12, ~V13, ~V14, ~V15, ~V16, ~V17, &quot;Division&quot;, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, &quot;Alabama&quot;, 12, 12.5, 12.4, 12.2, 12.5, 12, 14, 15.1, 11.7, 11.5, 16.4, 15.5, 13, 14.8, 13.6, 12.1, &quot;Alaska&quot;, 18.3, 17.4, 14.8, 18, 17.5, 15.3, 16.9, 16.4, 17.6, 19.6, 17.2, 18.1, 18.2, 19, 18.5, 17.2, &quot;Arizona&quot;, 19.4, 16.4, 16.7, 16.4, 16.4, 16.2, 19.1, 20.8, 17.8, 19.1, 18.9, 19.1, 17.3, 18, 17.1, 13.6, &quot;Arkansas&quot;, 13.9, 14.1, 16.4, 16.5, 17.2, 15.9, 17.2, 18.6, 15.7, 17.6, 19, 18.5, 17.5, 18.4, 16, 11.8, &quot;California&quot;, 19, 17.5, 18, 16.5, 17.3, 17.5, 18, 17.8, 17.5, 18.1, 19.3, 19.4, 19.7, 17.9, 17.2, 12.4, &quot;Colorado&quot;, 14.1, 12.9, 14.6, 14.5, 15.3, 15.2, 16.2, 16.5, 16, 15.4, 14.5, 12.9, 15.7, 13.7, 14.1, 10.3, &quot;Connecticut&quot;, 7.3, 8.9, 8.2, 8.6, 9.4, 10.3, 10.1, 8.7, 8.6, 9.4, 11.1, 11.2, 8.6, 8.1, 9.4, 6.9, &quot;Delaware&quot;, 9.7, 8.5, 8.5, 9.2, 9.6, 13.1, 11.6, 11.9, 10.6, 10.7, 13, 11.3, 10, 10.8, 9.1, 7.8, &quot;District of Columbia&quot;, 14, 12.8, 12.3, 13, 12.7, 12, 12.4, 10.9, 9.3, 9.4, 12.4, 12.8, 8.4, 7.9, 6.7, 5.3, &quot;Florida&quot;, 17.4, 16.2, 16.9, 15.6, 17, 18.3, 19.5, 20.3, 19.8, 19.4, 21.7, 20.7, 19.8, 21.5, 20, 16.6, &quot;Georgia&quot;, 14.2, 13.9, 14.7, 14.6, 15.2, 15.7, 17.9, 17.3, 17.2, 17.1, 20.5, 19.5, 19.2, 19.2, 18.8, 15.8, &quot;Hawaii&quot;, 9.2, 7.9, 8.2, 8.8, 8.6, 8.5, 8.1, 7.9, 6.9, 7.3, 7.4, 7.7, 7.8, 7.7, 6.7, 5.3, &quot;Idaho&quot;, 18.2, 15.4, 15.7, 16.9, 17.7, 14.5, 14.4, 15.1, 13.6, 15.4, 15.1, 19.1, 16.9, 15.9, 16.2, 13.6, &quot;Illinois&quot;, 11.9, 12, 11.8, 12.4, 13.8, 12.5, 13.2, 13.5, 13, 12.2, 14.2, 14.8, 14.7, 13.6, 12.7, 9.7, &quot;Indiana&quot;, 8.9, 10.1, 10.1, 11.5, 12.2, 12.4, 13.1, 11.3, 11, 11.3, 13.7, 13.4, 12, 13.4, 14, 11.9, &quot;Iowa&quot;, 7.8, 8.1, 6.8, 9, 10.4, 8.8, 8.1, 9.9, 8.8, 9, 10.8, 12.2, 10, 10.1, 8.1, 6.2, &quot;Kansas&quot;, 11.2, 9.6, 9.8, 9.4, 10.1, 10.6, 10, 12.1, 12.4, 11.8, 12.8, 12.6, 13.5, 12.6, 12.3, 10.2, &quot;Kentucky&quot;, 12.9, 12.7, 11.6, 12.7, 13.7, 13.9, 11.7, 15.2, 13.4, 15.7, 15.9, 14.8, 14.4, 15.7, 14.3, 8.5, &quot;Louisiana&quot;, 20.9, 16.8, 17.8, 17.2, 19, 17, 16.9, 21.1, 18, 19.5, 14.5, 19.8, 20.8, 18.3, 16.6, 8.5, &quot;Maine&quot;, 9.2, 10.4, 10.2, 10.4, 9.6, 9.3, 9.8, 8.9, 8.5, 10.2, 10, 9.3, 10, 9.5, 11.2, 10.1, &quot;Maryland&quot;, 10, 9, 11, 11.7, 12.2, 11.9, 13.1, 13.2, 12.7, 11.4, 13.3, 12.8, 13.8, 12.4, 10.2, 7.9, &quot;Massachusetts&quot;, 7.8, 7.1, 6.9, 9.5, 10.1, 9.8, 8.6, 9.6, 4.9, 5, 4.3, 5.5, 3.4, 4.1, 3.7, 3.3, &quot;Michigan&quot;, 9, 7.8, 9, 9.8, 9.3, 10.2, 9.5, 10.1, 10.8, 11.5, 13, 13, 12.5, 10.9, 11, 8.5, &quot;Minnesota&quot;, 6.6, 8, 6.9, 7.9, 8.7, 8.3, 7.6, 8.9, 8, 8.2, 8, 9.7, 9.2, 8.3, 8.2, 5.9, &quot;Mississippi&quot;, 15.7, 13.2, 17, 16.2, 17.5, 16.9, 16.5, 20.3, 18.4, 17.7, 17.3, 21, 16.2, 15.3, 17.1, 14.5, &quot;Missouri&quot;, 6.6, 8.6, 9.7, 10.8, 9.9, 11, 11.4, 13.1, 12.2, 12.4, 14.6, 13.9, 14.9, 13.3, 13, 11.7, &quot;Montana&quot;, 17.3, 16.1, 13.8, 14.3, 18.9, 17.5, 15.5, 16.9, 15, 15.7, 15.1, 18.2, 18.3, 18.1, 16.5, 14.2, &quot;Nebraska&quot;, 9, 7.9, 7.9, 9.3, 10.1, 10.3, 9.8, 12, 13, 11.1, 11.1, 13.2, 12.3, 13.3, 11.3, 9.7, &quot;Nevada&quot;, 18.3, 15.7, 14.5, 18.4, 17.6, 18.2, 16.5, 18.6, 16.9, 18.1, 20.6, 21.4, 22.6, 23.5, 20.7, 15.2, &quot;New Hampshire&quot;, 7.7, 7.9, 9.7, 8.8, 9.3, 8.7, 9.1, 10.8, 9.9, 10.1, 9.8, 10.1, 12.5, 12, 10.7, 9.2, &quot;New Jersey&quot;, 11.1, 10.2, 11.6, 12, 12.8, 12.6, 13.7, 14.8, 14.6, 13.2, 14.5, 15.6, 15.4, 14, 13.2, 10.9, &quot;New Mexico&quot;, 24, 23, 19.6, 20, 21.3, 19.3, 20.2, 22.7, 21.8, 22.8, 20.9, 21.4, 19.6, 21.9, 18.6, 14.5, &quot;New York&quot;, 14.4, 14.5, 13.9, 14, 14.3, 11.8, 12.1, 13.4, 12.3, 13.4, 14.1, 15.1, 12.2, 11.3, 10.7, 8.7, &quot;North Carolina&quot;, 12.5, 12.1, 13.3, 15.9, 16.7, 14.2, 14.5, 17.4, 16.2, 15.1, 17.8, 17.1, 16.3, 17.2, 15.6, 13.1, &quot;North Dakota&quot;, 10.2, 9.8, 8, 9.7, 10.3, 10, 10.8, 11.8, 9.5, 11.6, 10.3, 13.4, 9.1, 11.5, 10.4, 7.9, &quot;Ohio&quot;, 9.9, 9.8, 9.9, 10.4, 11.1, 10.3, 11, 9.6, 11.1, 11.2, 13.8, 13.6, 13.7, 12.3, 11, 8.4, &quot;Oklahoma&quot;, 15.4, 17.4, 17.2, 16.7, 19.1, 18.7, 17.7, 18.8, 17.6, 13.8, 17.9, 17.3, 16.9, 17.2, 17.7, 15.4, &quot;Oregon&quot;, 14.2, 11.6, 12.7, 14.3, 16, 15.4, 15.3, 17.5, 16.2, 15.9, 17.3, 16, 13.8, 15.4, 14.7, 9.7, &quot;Pennsylvania&quot;, 7.8, 7.6, 8.4, 10.2, 10, 10.1, 9.3, 9.4, 9.1, 9.6, 10.9, 10.9, 10.8, 12, 9.7, 8.5, &quot;Rhode Island&quot;, 5.9, 6.9, 7.7, 8.1, 10.4, 10, 10.7, 8.1, 10.5, 11, 12, 11.5, 12, 12.3, 11.6, 7.4, &quot;South Carolina&quot;, 14.8, 10.7, 11.1, 11.1, 13.1, 14.9, 16.3, 15.3, 15.9, 15.5, 16.8, 20.5, 19, 14.3, 15.8, 13.6, &quot;South Dakota&quot;, 10.1, 10.8, 8.3, 10.8, 10.6, 11, 11.5, 11.5, 9.9, 12.2, 13.1, 13.1, 13, 14.4, 11.3, 9.8, &quot;Tennessee&quot;, 9.3, 10.7, 10.1, 9.8, 12.2, 12.4, 13.4, 13.2, 14, 14.5, 15, 14.6, 13.3, 13.9, 13.9, 12, &quot;Texas&quot;, 21.1, 22, 22.4, 24.5, 23.6, 23.6, 22.9, 23.9, 24.7, 24.5, 25.5, 24.6, 23.8, 24.6, 22.1, 19.1, &quot;United States&quot;, 13.6, 13.1, 13.5, 13.9, 14.6, 14.3, 14.6, 15.2, 14.7, 14.9, 16.1, 16.3, 15.7, 15.4, 14.5, 11.7, &quot;Utah&quot;, 11.9, 10.8, 13.8, 12.1, 11.5, 12.8, 15.5, 16.7, 12.2, 12, 14.1, 13.8, 14.6, 14.4, 14, 12.5, &quot;Vermont&quot;, 10.1, 7.4, 8.8, 8.9, 8.4, 9.8, 11.2, 9.8, 10.1, 9.3, 9.4, 9.3, 8.6, 7, 7.2, 5, &quot;Virginia&quot;, 11.3, 9.6, 9.8, 11.8, 11.5, 13, 12.3, 12.5, 14.2, 11.8, 12.6, 14, 13.4, 12.5, 12.3, 10.9, &quot;Washington&quot;, 12.2, 13.1, 13.3, 12.3, 14.8, 12.5, 12.5, 11.5, 11, 12, 12.6, 13.9, 14.5, 13.6, 14, 9.2, &quot;West Virginia&quot;, 14.9, 13.4, 12.9, 13.8, 16.8, 15.7, 16.5, 13.3, 13.7, 14.5, 13.7, 13.4, 14.9, 14.6, 14, 8.6, &quot;Wisconsin&quot;, 9.7, 7.1, 7.3, 8.6, 9.8, 10.3, 8.8, 8, 8, 9.2, 8.9, 9.4, 10.4, 9.7, 9.1, 7.3, &quot;Wyoming&quot;, 14.5, 14.7, 14.1, 14.8, 14.8, 12.3, 14.4, 14.2, 13.2, 13.3, 15.4, 17.2, 17.8, 15.4, 13.4, 12 ) names(ins_data) &lt;- c(&quot;state&quot;, c(1999:2014)) ins_data %&gt;% slice(2:45) %&gt;% bind_rows(slice(ins_data, 47:53)) %&gt;% filter(state != &quot;District of Columbia&quot;) %&gt;% filter(state != &quot;United States&quot;) -&gt; ins_data include your minimal code - just enough to reproduce the problem, and no more. Now you need to - run this minimal code in a new script window to make sure it reproduces the problem and gets the same error. - consider adding an illustration of what you want or expect to output to look like - Install the reprex package #install.packages(&#39;reprex&#39;) Select all of the code in your new script window, including libraries, data, and code copy this with Ctrl-C (Windows) or Cmd-C (Mac) go to the Console, type in “reprex()” and enter your REPREX will be generated and will show up in your Viewer tab. This is now also on your Clipboard. Go to RStudio Community and start a new topic. Type in an introduction to your problem, state clearly what you are trying to do, and where you are stuck. Paste in the reprex. Thank people in advance for their help. Post this topic. Wait for helpful answers. You can also email a reprex to a friend, so that they can give it a try. Try making your own reprex, and emailing it to a friend. See if they can find/solve the problem. "],["the-basics-of-base-r.html", "Chapter 13 The Basics of Base R 13.1 Dimensions of Data Rectangles 13.2 Naming columns 13.3 Concatenation 13.4 Sequences 13.5 Constants 13.6 Fancier Sequences 13.7 Mathematical functions 13.8 Handling missing data (NAs) 13.9 Cutting Continuous data into Levels", " Chapter 13 The Basics of Base R While there are many great features of the tidyverse, one should not throw out the base R with the bathwater. The functions and packages of base R are stable and slow to change (unlike the dynamic packages and functions of the tidyverse), and many are helpful and important building blocks for using R. Some of the functions in base R tend to fail silently, and have unhelpful error messages, but they are embedded in a lot of R scripts. When you search for help with R on websites like RStudio Community and Stack Overflow, you will often find base R code, and you will need to know how to interpret it. There are many really basic and important functions in base R that are worth knowing about. Once you have a handle on these basic functions, you can say Obscure base R / video game meme meme details 13.1 Dimensions of Data Rectangles Whether you have a data.frame, a tibble, or a matrix, it can be helpful to know the dimensions of what you have. You can get at these dimensions with dim() nrow() ncol() You may want to know how many rows to loop over, or how many columns need names, but you will frequently need to access these numbers. The dim() function returns two numbers - the rows first, then the columns. Let’s try this on the licorice dataset from the {medicaldata} package. dim(licorice) ## [1] 235 19 This is great, as long as you know that the first number is the number of rows, and the 2nd number is the number of columns (standard notation is R x C, so rows first, columns second). But if you want to get the number of rows out, and store it in a variable, you need to use the brackets [n] notation. Brackets allow you to pull out the nth item in a vector or a list. Let’s pull out the first item (the number of rows), and the second item (the number of columns) separately. We will store these in the 2 variables rows and columns, then print them out. rows &lt;- dim(licorice)[1] rows ## [1] 235 columns &lt;- dim(licorice)[2] columns ## [1] 19 You can also do this more directly with the nrow() and ncol() functions. rows &lt;- nrow(licorice) rows ## [1] 235 columns &lt;- ncol(licorice) columns ## [1] 19 A similar approach can give you the length of a vector with the length() function. Here we will check the length of the treat vector in the licorice tibble. length(licorice$treat) ## [1] 235 The length() function works a bit differently on dataframes or tibbles - it returns the number of variables/columns. This can be surprising if you don’t expect it, and you are expecting the number of rows. length(licorice) ## [1] 19 13.2 Naming columns Sometimes you want to take a quick look at the names of all of you columns in a dataframe. The names() function is a quick solution. names(licorice) ## [1] &quot;preOp_gender&quot; &quot;preOp_asa&quot; ## [3] &quot;preOp_calcBMI&quot; &quot;preOp_age&quot; ## [5] &quot;preOp_mallampati&quot; &quot;preOp_smoking&quot; ## [7] &quot;preOp_pain&quot; &quot;treat&quot; ## [9] &quot;intraOp_surgerySize&quot; &quot;extubation_cough&quot; ## [11] &quot;pacu30min_cough&quot; &quot;pacu30min_throatPain&quot; ## [13] &quot;pacu30min_swallowPain&quot; &quot;pacu90min_cough&quot; ## [15] &quot;pacu90min_throatPain&quot; &quot;postOp4hour_cough&quot; ## [17] &quot;postOp4hour_throatPain&quot; &quot;pod1am_cough&quot; ## [19] &quot;pod1am_throatPain&quot; You can also use names() to re-set the names if you want to change a bunch of column names, by assigning a vector of names (of the same length). names(licorice) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;, &quot;K&quot;, &quot;L&quot;, &quot;M&quot;, &quot;N&quot;, &quot;O&quot;, &quot;P&quot;, &quot;Q&quot;, &quot;R&quot;, &quot;S&quot;) licorice[1:10, ] ## A B C D E F G H I J K L M N O P Q R S ## 1 0 3 32.98 67 2 1 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 2 0 2 23.66 76 2 2 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 3 0 2 26.83 58 2 1 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 4 0 2 28.39 59 2 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 5 0 1 30.45 73 1 2 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 6 0 2 35.49 61 3 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 7 0 3 25.50 66 1 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 8 0 2 31.10 61 2 1 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 9 0 3 21.22 83 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 10 0 3 27.16 69 2 3 0 1 2 0 0 0 0 0 0 0 0 0 0 Note that you can use the set_names() function in the {purrr} package to conveniently change variable/column names within a data pipeline, and the rename() function in the dplyr package to change particular variable/column names. licorice %&gt;% purrr::set_names(1:19) %&gt;% dplyr::rename(&quot;purple&quot; = 2) %&gt;% # note rename(new_name = old_name) tibble() ## # A tibble: 235 × 19 ## `1` purple `3` `4` `5` `6` `7` `8` `9` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 3 33.0 67 2 1 0 1 2 ## 2 0 2 23.7 76 2 2 0 1 1 ## 3 0 2 26.8 58 2 1 0 1 2 ## 4 0 2 28.4 59 2 1 0 1 3 ## 5 0 1 30.4 73 1 2 0 1 2 ## 6 0 2 35.5 61 3 1 0 1 3 ## 7 0 3 25.5 66 1 1 0 1 3 ## 8 0 2 31.1 61 2 1 0 1 1 ## 9 0 3 21.2 83 1 1 0 1 1 ## 10 0 3 27.2 69 2 3 0 1 2 ## # … with 225 more rows, and 10 more variables: `10` &lt;dbl&gt;, ## # `11` &lt;dbl&gt;, `12` &lt;dbl&gt;, `13` &lt;dbl&gt;, `14` &lt;dbl&gt;, ## # `15` &lt;dbl&gt;, `16` &lt;dbl&gt;, `17` &lt;dbl&gt;, `18` &lt;dbl&gt;, ## # `19` &lt;dbl&gt; licorice[1:10, ] ## A B C D E F G H I J K L M N O P Q R S ## 1 0 3 32.98 67 2 1 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 2 0 2 23.66 76 2 2 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 3 0 2 26.83 58 2 1 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 4 0 2 28.39 59 2 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 5 0 1 30.45 73 1 2 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 6 0 2 35.49 61 3 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 7 0 3 25.50 66 1 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 8 0 2 31.10 61 2 1 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 9 0 3 21.22 83 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 10 0 3 27.16 69 2 3 0 1 2 0 0 0 0 0 0 0 0 0 0 Note also that we used the bracket notation above to print just the first 10 rows of the renamed version of the licorice dataframe. This was done with brackets that define which [rows, columns] we want to use (in this case for printing). By using the sequence 1:10, we choose the first 10 rows. By putting nothing after the comma, we select all columns. You might be wondering why the column names reverted to alphabetical letters after we used set_names to change them to numbers. This is because we did set the names, and printed the result out to the console, but did not assign the result back to the licorice object with an assignment arrow, so it is transient, rather than a lasting change to the licorice object. This is a common pitfall for beginners. We can use also use brackets to choose exactly which rows and columns we want. licorice[4:7, c(2,5,10)] ## B E J ## 4 2 2 0 ## 5 1 1 0 ## 6 2 3 0 ## 7 3 1 0 Here we have selected 4 particular rows with a sequence (4:7), and 3 particular columns (by concatenating these into a vector with the c() function). 13.3 Concatenation One of the simplest, but most common early functions in R is c(). The c() function concatenates items together into a vector. This can be helpful for building a vector of items to iterate over, or to build a vector which will become a variable in a dataframe, or even a vector of options for a function. You simply write the items, separated by commas, in order inside the parentheses of c(). Remember that strings need to be enclosed in matching quotes. fib_numbers &lt;- c(1, 1, 2, 3, 5, 8, 13, 21, 34) fruit_vec &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;coconut&quot;, &quot;dragonfruit&quot;, &quot;elderberry&quot;) fib_numbers ## [1] 1 1 2 3 5 8 13 21 34 fruit_vec ## [1] &quot;apple&quot; &quot;banana&quot; &quot;coconut&quot; &quot;dragonfruit&quot; ## [5] &quot;elderberry&quot; 13.4 Sequences There are times when you want to create a sequence of numbers (i.e. 1 to 10, or 1 to 100), without manually concatenating a vector. The easiest way to do this is with the colon (:). You can assign 1:12 to an object, or 77:83, if you prefer. 1:12 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 77:83 ## [1] 77 78 79 80 81 82 83 13.5 Constants Note that base R has some handy constants that may help in making vectors - LETTERS (vector of letters (string) from A to Z) - letters (vector of letters (string) from a to z) - month.abb (vector of 3 letter (English) month abbreviations from Jan to Dec) - month.name (vector of 3 letter (English) month abbreviations from January to December) - pi (the irrational number for relating diameter to circumference) You can select subsets of these with the bracket notation, i.e letters[1:13]. You can also format number for printing as strings with sprintf() (for print formatting) to include the desired number of decimals. LETTERS[7:12] ## [1] &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; &quot;K&quot; &quot;L&quot; letters[5:10] ## [1] &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; month.abb[10:12] ## [1] &quot;Oct&quot; &quot;Nov&quot; &quot;Dec&quot; pi %&gt;% sprintf(fmt = &quot;%1.5f&quot;, .) ## [1] &quot;3.14159&quot; 13.6 Fancier Sequences You can make more complex sequences with the seq() function. The main arguments (parameters) of seq() are from (default =1) to (default =1) by (default = (to-from)/length) length You will generally need at least 3 of these to describe a sequence, or seq() will use the default by value of 1. Note that if the by increment does not match the to argument, the sequence will stop at the last number before the to number. seq_len(n) is a shortcut that gives you a sequence from 1 to n, while seq_along(vector) is a shortcut that gives you a sequence from 1 to the length of a the vector. See the examples below # leaving out &quot;length&quot; seq(from = 2, to = 18, by = 2) ## [1] 2 4 6 8 10 12 14 16 18 # leaving out argument names seq(3, 18, length=6) ## [1] 3 6 9 12 15 18 # &#39;length&#39; and &#39;to&#39; do not match seq(from = 24, to = 4, by = -6) ## [1] 24 18 12 6 # leaving out &quot;to&quot; seq(from = 5, by = 5, length = 6) ## [1] 5 10 15 20 25 30 # leaving out &quot;by&quot; seq(from = 16, to = 128, length = 8) ## [1] 16 32 48 64 80 96 112 128 seq(from = 51, by = -3, length = 17) ## [1] 51 48 45 42 39 36 33 30 27 24 21 18 15 12 9 6 3 # using the seq_len() shortcut with n seq_len(14) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # using the seq_along() shortcut with a vector seq_along(7:23) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 seq_along(licorice$C) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 ## [14] 14 15 16 17 18 19 20 21 22 23 24 25 26 ## [27] 27 28 29 30 31 32 33 34 35 36 37 38 39 ## [40] 40 41 42 43 44 45 46 47 48 49 50 51 52 ## [53] 53 54 55 56 57 58 59 60 61 62 63 64 65 ## [66] 66 67 68 69 70 71 72 73 74 75 76 77 78 ## [79] 79 80 81 82 83 84 85 86 87 88 89 90 91 ## [92] 92 93 94 95 96 97 98 99 100 101 102 103 104 ## [105] 105 106 107 108 109 110 111 112 113 114 115 116 117 ## [118] 118 119 120 121 122 123 124 125 126 127 128 129 130 ## [131] 131 132 133 134 135 136 137 138 139 140 141 142 143 ## [144] 144 145 146 147 148 149 150 151 152 153 154 155 156 ## [157] 157 158 159 160 161 162 163 164 165 166 167 168 169 ## [170] 170 171 172 173 174 175 176 177 178 179 180 181 182 ## [183] 183 184 185 186 187 188 189 190 191 192 193 194 195 ## [196] 196 197 198 199 200 201 202 203 204 205 206 207 208 ## [209] 209 210 211 212 213 214 215 216 217 218 219 220 221 ## [222] 222 223 224 225 226 227 228 229 230 231 232 233 234 ## [235] 235 13.7 Mathematical functions R has many mathematical functions, which can be used in a variety of calculations. These can be run on a vector, or on a variable in a dataframe. These include (and there are many more): mean median var sd min max range rank sum Examples are shown below mean(1:20) ## [1] 10.5 median(licorice$C) ## [1] 25.91 var(licorice$C) ## [1] 18.24933 sd(licorice$C) ## [1] 4.271923 min(licorice$C) ## [1] 15.6 max(licorice$C) ## [1] 36.33 range(licorice$C)[2] # selects 2nd value in range (max) ## [1] 36.33 rank(licorice$C)[1] # ranks first 10 values ## [1] 225 sum(licorice$C) # sum of values ## [1] 6013.99 13.8 Handling missing data (NAs) R designates missing values as the symbol NA (not available). NAs propagate through calculations, so that if you have a vector with at least one NA, and you try to calculate the mean, it will return NA. mean(licorice$J) ## [1] NA You can handle this within many functions (including mean, median, sd, and var) with the argument na.rm = TRUE. The default for these is na.rm = FALSE, so that if you are trying to do an operation on missing data, R will tell you. na.rm is an argument in a number of mathematical functions, in which na comes first, followed by the verb rm (remove). Testing whether a value or values are missing (NA) is in the reverse order. You use the is.na() function, in which the verb comes first, and then followed by NA. You might reasonably think that you can just use a normal equality test for NA values, like licorice$J == NA ## [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [19] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [37] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [55] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [73] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [91] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [109] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [127] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [145] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [163] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [181] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [199] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [217] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [235] NA but, because NAs propagate, you get just NAs, rather than TRUE or FALSE. You can use is.na() for this. licorice$J %&gt;% is.na() ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [10] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [19] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [28] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [46] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [55] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [64] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [82] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [91] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [100] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [109] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [118] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [127] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [136] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [154] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [163] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [172] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [190] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [199] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [208] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [226] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [235] FALSE There are a few TRUEs in there (value is NA), but they can be hard to count. The sum() function can help, when combined with is.na(). The which() function can help you figure out which values are missing. The anyNA() function can tell you if there are any missing values in a vector (or a variable/column in a dataframe). licorice$J %&gt;% is.na() %&gt;% sum() ## [1] 2 licorice$J %&gt;% is.na() %&gt;% which() ## [1] 113 123 licorice$C %&gt;% anyNA() ## [1] FALSE licorice$J %&gt;% anyNA() ## [1] TRUE There are two missing values, in rows 113 and 123. The na.omit() function can remove all of the rows(cases, observations) from a dataframe that have at least one missing value in any column. This can be helpful for modeling, in which cases with missing data can cause problems. It is helpful to keep track of the number of rows before and after na.omit(), to know how many cases/observations/rows you are discarding. nrow(licorice) ## [1] 235 licorice %&gt;% na.omit() %&gt;% # modeling would happen here if not too many cases discarded nrow() ## [1] 233 Note that this can also be done in the tidyverse with drop_na() in the {tidyr} package. You can include a particular column or columns as an argument in drop_na() to only drop observations if there are missing values in these particular columns. licorice %&gt;% drop_na(H:J) %&gt;% nrow() ## [1] 233 The code above takes the licorice dataset, looks for NA values in rows of (only) columns H through J, and drops 2 rows based on missing data, reducing the number of rows from 235 to 233. 13.9 Cutting Continuous data into Levels While there are good arguments for why not to do this (dichotomania, loss of granularity in data), it is common to cut continuous data into levels, like (mild, moderate, severe), or (normal weight, overweight, obese). This can, when there are already established standard levels, make the data easier to interpret. The cut() function in base R makes this easy to do. C_factor3 &lt;- cut(licorice$C, breaks = 3) table(C_factor3) ## C_factor3 ## (15.6,22.5] (22.5,29.4] (29.4,36.4] ## 59 130 46 str(C_factor3) ## Factor w/ 3 levels &quot;(15.6,22.5]&quot;,..: 3 2 2 2 3 3 2 3 1 2 ... This creates a new variable (C_factor), which is a factor with 3 levels. The levels are stored as 1, 2, 3, and range from 15.6-22.5 for 1, above 22.5-29.4 for 2, and above 29.4 to 36.4 for 3. The interval notation uses the square bracket for including the listed number, and parentheses for starting just after the listed number. It is a good practice to develop a standard way of naming these created variables, which are related to the original variable, are factors, and have a certain number of levels. One helpful shorthand is to take the original variable name, and to add the suffix “_f4” for a factor with 4 levels. A dichotomized variable would be ”varname_f2” "],["updating-r-rstudio-and-your-packages.html", "Chapter 14 Updating R, RStudio, and Your Packages 14.1 Installing Packages 14.2 Loading Packages with Library 14.3 Updating R 14.4 Updating RStudio 14.5 Updating Your Packages", " Chapter 14 Updating R, RStudio, and Your Packages 14.1 Installing Packages The most important way to update R is to add packages. Each package adds new functions and/or data to R, enabling you to do much more in the R and RStudio environment. When you open R, or start a new session, you have only the base version of R available, and it is pretty spartan. You can see how many packages you have available to you by starting RStudio and going to the menu Session/New Session, or Session/Restart R. Each of these will give you a clean workspace to start in. Once you have started a new session, or restarted R, run the following code: print(.packages()) You will find that you only have 9 packages available, including base, utils, methods, stats, graphics, grDevices, datasets, devtools, and usethis. This is what is called “base R” and is essentially the bare minimum needed to use R. In order to use more of the power of R and RStudio, you will need to install packages (a one-time task), and load them (in each session) before use with a library(package_name) function. If you Google a bit for ways to do things in R, you will find many packages that can be helpful. The most strictly validated packages are hosted on CRAN - a mirrored server. There are now over 20,000 packages on CRAN to do various specialized things in R. These were all useful for someone, so they have shared them on CRAN. To install packages from CRAN, you use the function: install.packages(\"package_name\") Notice that the package_name has to be in quotes. These can be single or double quotes. The package_name and install.packages() are case_sensitive like all objects and functions in R, so that something like Install.Packages will not work. Once the package is installed, you keep that in your R library associated with your current major version of R. You will need to update &amp; reinstall packages each time you update a major version of R. R versions are designated with R version #.#.# A change in the third number indicates a patch level change. A change in the first number (from R 3.6.2 to 4.0.0) is a major version change, while a change in the middle number (4.0.2 to 4.1.0) is a minor version change. Any major or minor version upgrade will require re-installation of your add-on packages into a version-specific package library. Let’s practice installing a package. Run the code below to install the remotes() package. install.packages(&quot;remotes&quot;) 14.1.1 Installing Packages from Github Some packages are still in development. These are often in repositories on GitHub, rather than on the CRAN servers. To install these packages, you need to know path to the repository. You can install the development version of the medicaldata package from Github (the stable version is on CRAN). Run the code below to install this package (this assumes that you already have the remotes package installed). remotes::install_github(&quot;higgi13425/medicaldata&quot;) In contrast to install.packages, the library() function can work with quotes around the package_name, but they are not required. This is because these packages are already installed in your R library, and are known quantities. In general, known objects in your R Environment pane (dataframes, vectors) do not require quotes, and novel things like new packages (or variables hidden inside of a data frame) do require quotes or a $ - though the tidyverse packages work around this quote issue for variables inside of a data frame. If you re-run print(.packages) at this point, you will not have any more packages. This is because you have installed new packages, but not loaded them with library(). Notice that you were able to use the {remotes} package function, install_github() above, without loading the package with library(remotes), because you explicitly called the function with its package name, as in remotes::install_github(). Try installing a few more packages, like “janitor”, or “gtsummary”. These should now show up with installed.packages(). Then use library() to load these packages, and see that the output of print(.packages) has changed. 14.1.2 Problems with Installing Packages 14.1.2.1 R Version Issues Sometimes you may run into a problem installing a package which was developed for a previous version of R. Especially if you have recently upgraded your R version recently, the CRAN version of a package may be a bit behind. This can often be fixed by googling for “github” and “package_name”. This will usually lead you to the github repository for that package, which will have a pathname of “github_username/package_name”. Once you know this, you can use devtools::install_github('github_username/package_name') to install the newest version of the package, which will usually be compatible with the latest version of R. 14.1.2.2 Installing from Source vs Binaries When you install or update a package that is in rapid development, you may get this message: There are binary versions available but the source versions are later: binary source needs_compilation tidyr 1.1.4 1.2.0 TRUE This means that the latest version (in this case, 1.2.0), is available, but it has to be compiled and built from source code, which is a bit slower. It is faster to install from binary files, which tend to be available on CRAN (built by the CRAN volunteer maintainer team) a little bit later. When the binary version of a package is not yet available, you will be asked to decide - you can install the source code version (installs slower, but you get the very latest version), or the binary version (installs fast, but a version behind). Most of the time, unless you have a very slow internet connection, you want to install from the source version. The compilation part is what slows the installation. Usually, you want to answer Yes to the question Do you want to install from sources the packages which need compilation? (Yes/no/cancel) Why do some packages need to be compiled from source (~ 25%) and others (~75%) don’t? Usually it is because the ones that need to be compiled include another language (like C or C++, rather than pure R) which needs to be compiled. These additional languages are usually used to improve processing speed of the resulting functions. 14.1.2.3 Dependencies Some packages are dependent on specific versions of other packages, and will ask you to update the other packages during installation. As a general rule, you should say ‘yes’ to all packages. If you are worried about over-writing an existing package in a way that would break your code in a different project, then that project needs its own project-specific library, which you can create with the {renv} package. 14.1.2.4 Extra-R Dependencies Sometimes packages require (depend upon) software that is not part of the R ecosystem. These will generally give you messages during the install process asking you to install this helper software. Common helper software packages include things like gtk+, freetype, and proj. Sometimes you will need to go to websites, or use software like Homebrew (on the Mac) to install these extra helper pieces of software. If you have never used Homebrew, here is a basic guide to get you started. The Homebrew page is here. You can search on the Homebrew page and usually find the packages you need. They can be installed (via your Terminal Application (in Utilties) with something like brew install packagename. A bit of googling with the error message as the search term will usually help you find the specific package name. Once the external-to-R package is installed, you can go back and install the R package that depends on the external software package. 14.1.2.5 Package Installation Failed Sometimes you will get a message like: Warning in install.packages : installation of package \"ragg\" had non-zero exit status This means that the package installation failed. Sometimes it is because there is a problem with the package, but more often there is a missing dependency. Often you can figure this out by scrolling upward. You will often find messages telling you that fatal error: 'ft2build.h' file not found Suggesting that you need a file named ‘ft2build.h’. You will likely need to web search for this error to find out how to fix it, usually by downloading and installing a piece of software that the problematic package needs in order to run. Another example - configure: error: Install libtiff-dev or equivalent first (or set PKG_MODULE if non-standard) ERROR: configuration failed for package 'tiff' You are missing a piece of software external to R called libtiff-dev. The error message is fairly helpful. 14.2 Loading Packages with Library Run the code chunk below to load both {tidyverse} and {medicaldata}. Note that the {tidyverse} package is actually a meta-package that contains 8 packages, and each one has its own version number. library(tidyverse) library(medicaldata) Notice that loading tidyverse led to some conflict messages. The dplyr::filter function masks the stats::filter() function. These two packages, {dplyr} and {stats}, both have a function named filter(). The more recently loaded package is assumed to be the default, so if you call a filter() command, R will use dplyr::filter(). If you want to call the stats::filter() command, you have to explicitly use the package::function() format. If you are not sure which package you loaded last, it can be wise to use the explicit format when calling functions in R. The other masked function is lag(). The function dplyr::lag() is masking stats::lag(), as {dplyr} was loaded after {stats}. Most of the time this is not a big difference, but every once in a while a conflict between package functions can get very confusing. When in doubt, use the explicit format, in which you call package::function() to make clear what you mean, as in dplyr::lag() vs. stats::lag(). Note that it is good practice to load all of your packages needed for an R script or an Rmarkdown (.Rmd) document at the beginning of the script or .Rmd. This allows someone else using your script or Rmd to check whether they have the needed packages installed, and install them if needed. In an Rmarkdown document, this is done in a special setup code chunk near the top of the document. If some of these packages are not on CRAN, it is good practice to add a comment (a statement after a hashtag) on how to install this package. For example, in a setup chunk that loads {tidyverse} and {medicaldata}, it is a good idea to add a comment on how to install {medicaldata}, which is not yet on CRAN. See the example below library(tidyverse) library(medicaldata) # the {medicaldata} package can be installed with remotes::install_github(&#39;higgi13425/medicaldata&#39;) 14.2.0.1 Explicitly Managing Function Conflicts You can also manage conflicts between identically-named functions from different packages in R with the {conflicted} package, which identifies conflicts when you have called a function without explicitly naming the package that it is from. You load library(conflicted) and it will watch out for conflicts and send an error when you call a function in an ambiguous way. If you don’t want to always explicitly specify between dplyr::filter, base::filter and rstatix::filter, you can set your default version of a function right after you load your libraries, with conflicted::conflict_prefer(\"filter\", \"dplyr\"), which tells R that you want to use the dplyr version of filter by default, and that you will specify if you want to use base::filter or rstatix::filter. 14.3 Updating R Every once in a while, you will want to update your version of R. Usually this occurs with a major version upgrade, when something important changes. You may not rush into this, as it means re-installing all of your packages, but eventually it is worth it to be up to date. In order to update R, you have to find your installed version of R and run it on its own, outside of RStudio. This is easy if you have an R desktop shortcut, but not too hard if you hunt around a bit in your Applications folder. Double click the R icon to start up R. It will open the R Console and a menu. Click on the R menu at top left, and select Check for R Updates. If you are up to date, the R Console will report “Your version of R is up to date”. If not, this process will provide windows and buttons to click to upgrade to the latest version of R. When done, quit R and start RStudio to make sure the update has carried over. You should see the new version number when RStudio starts. 14.4 Updating RStudio To update RStudio, just run RStudio, and go to the Help menu in the top menu bar (not the Help tab in the lower right quadrant). In the Help menu, select Check for Updates. It will tell you if you are using the latest version of RStudio, or will direct you to the website to download the latest version. 14.5 Updating Your Packages To update your packages, you go to the Tools menu in RStudio, and select Check for package updates. You will usually get a list of the packages that have been updated since you installed them. Generally, select Update All, and allow one restart of your R session. RStudio may ask you if you want to restart more than once, but always say no after the first session restart. You may be asked about installing some packages from source, and you should generally select Yes. In general, your Console pane will be a bit chatty as it documents all the steps in package installation, but should generally end with something like: “The downloaded source packages are in ‘/private/var/folders/93/s18zkv2d4f556fxbjvb8yglc0000gp/T/RtmpHnsvlh/downloaded_packages’” and return to your &gt; prompt. If you then re-check for Package Updates, you will get the message that all of your packages are up to date. This process is a bit different after a major version upgrade of R, which we will cover in a later chapter. You have to retreive a list of all your packages, decide which to keep, and then install these fresh in the new version of R (and its new, major-version-specific package library). "],["major-r-updates-where-are-my-packages.html", "Chapter 15 Major R Updates (Where Are My Packages?) 15.1 Preparing for a Minor or Major R Upgrade 15.2 Saving a List of Your Packages 15.3 Upgrading R (and RStudio) 15.4 Now Check your list of Packages 15.5 Updating Packages", " Chapter 15 Major R Updates (Where Are My Packages?) Versions of R have numbers attached, like R.4.1.2. The first number is the major version (4). The second number is the minor version (1). The third number is the patch level (2). When you upgrade R from one patch level to the next, from 4.1.1 to 4.1.2, the changes are very minor, and backward-compatible. However, when you upgrade from one minor version (the middle number) to the next, from 4.0.2 to 4.1.0, you will find that your carefully installed and much-beloved packages are gone. All of the add-on packages you know and love, from {tidyverse} to {rstatix}, did not come along for the upgrade. Be Mindful Do not just upgrade R to a new minor (or major) version in the middle of a project. This will take a bit of work and time, maybe an hour if you have a lot of packages installed, as this will require re-installing packages to a new library folder. 15.1 Preparing for a Minor or Major R Upgrade You want to check the current R version at r-project.org. It may have been a while since you upgraded. Let’s check your current R version, with R.Version(). If you are not in the middle of a major project, it may be a good time to upgrade. There are several new features (like the native pipe) being added to base R that are worth checking out. Before you jump into upgrading R versions and packages, make sure that any important projects are protected. Make sure that your important ongoing projects use the {renv} package to preserve their Environment, and that you have taken a recent snapshot with renv::snapshot() Also, check out your current package library, with .libPaths(). This should look something like: [1] \"/Library/Frameworks/R.framework/Versions/4.1/Resources/library\" Note that this package library is specific to the most recent major and minor version of R. If you upgrade from 4.0.x to 4.1.x, you will have a new library path to a new 4.1 folder, and will need to re-install packages in this folder. You can do this from scratch, and just install packages as you need them (which is not a bad way to clean out packages you are not using), but a lot of people prefer to re-install all of their packages in an organized way, rather than just-in-time when they want to use them. You can see a dataframe of all your currently installed packages with the base R function installed.packages(). This will help us make a list of your packages for updating. There is also a package for Windows, {installr}, which will pull the list of your current packages and install these in the correct folder for the new version of R. If you are on a Windows OS, and have installed the {installr} package, the installr::updateR() command performs the following: finding the latest R version downloading it running the installer deleting the installation file copy old packages to the new R installation updating old packages as needed for the new R installation But for MacOS and UnixOS folks, this must be done manually. Let’s do this step by step for the Mac and Linux folks. 15.2 Saving a List of Your Packages The short script below will save a vector of your current packages to a file in your root (home) directory, named installed.packages.rda. Copy this code chunk below into your Console pane in RStudio and run it. Then reopen RStudio, go to the Files tab, click on Home , and check your root (home) directory for the installed.packages.rda file. You can click on this file to load it into your Environment tab (answer yes to load). In your Environment tab, it should look like a character vector with the names of all of your packages in quotes. tmp = installed.packages() # save list to tmp as a matrix object installedpackages = as.vector(tmp[is.na(tmp[,&quot;Priority&quot;]), 1]) # filter for all rows where the &quot;Priority&quot; column is NA, and select just (column 1) the package names in quotes, and then assign this vector to the installedpackages object save(installedpackages, file=&quot;~/installed_packages.rda&quot;) # save this vector as an *.rda file in your root (home) directory rm(tmp) rm(installedpackages) # remove the tmp matrix and the installedpackages vector from your Environment 15.3 Upgrading R (and RStudio) Now that you have your saved list of packages stored in a file, you are ready to upgrade R. Quit RStudio, and go to r-project.org. The first paragraph of text under the heading Getting Started will have a link to download R. Click on this link, and select a nearby CRAN mirror to download from. If you are in the United States, you will have to scroll down a long way (alphabetical by country) to USA. Pick one that is reasonably close, click through, and Download and install a new version of R for your operating system. If you are not sure of the steps, refer to Chapter 2, Getting Started and Installing Your Tools. Once this is set, you are ready to re-open RStudio. If you haven’t updated RStudio in a while, it might be a good time to update RStudio as well (and get rainbow parentheses!) - once RStudio is open, go to Help/Check for Updates. If you are not using the latest version of RStudio, you will be directed to update and restart RStudio. 15.3.1 Reinstalling your list of Packages First, let’s chack your new R version with R.Version() - run this in the Console pane. This should be the new version. Now let’s check your library path with .libPaths() - run this in the Console Pane - this should match your R version. Now run installed.packages(). This should show that your new library is mostly empty, except for the base R packages. We will now fix that. Now you need to take advantage of your nicely-stored list of packages to reinstall all of your packages in your new (mostly empty) library. We will use the short script below to load the file installed.packages.rda to open a vector of your previous packages in your Environment pane, and then re-install all of these packages. Copy this code chunk below into your Console pane in RStudio and run it. This will take a while, especially if you have a lot of packages installed. While you are waiting, check out the code chunk below. The first step is to load the installedpackages.rda file from your root (home) directory into your Environment pane. Then it starts taking action on this vector of package names. See what it does in the next step, the for loop, and think about how it works. - It measures the length of the vector, installedpackages. - Then it counts from 1 to the length of installedpackages. - For each count, it installs the package at position [count] in the vector - Then it goes to the next [count] value, and installs the next package in the vector - Until it reaches the full length of the vector (installs the last package), and then stops. load(&quot;~/installed_packages.rda&quot;) # loads vector into your Environment pane for (count in 1:length(installedpackages)) { install.packages(installedpackages[count]) } # For each package in this vector of length [count], install them one by one until the last one is installed. 15.4 Now Check your list of Packages When all of the installation is complete, you can check your work. Review the Console text for errors or problems. You may find you need software packages external to R, or that you have tried to install some packages that are not on CRAN. Many packages that are not on CRAN are ones that you may have previously installed from Github or BioConductor. Note these by making a list, and install those from GitHub with remotes::install_github(\"username/packagename). As one example, you can install the development version of the {medicaldata} package from github with remotes::install_github(\"higgi13425/medicaldata). Note that the stable version is available on the CRAN repository. You may have similar issues with packages installed from BioConductor, which requires you to use BiocManager::install(\"packagename\"). Once you have completely installed your packages, check your work by running installed.packages() again. You should see a full list of your packages, and you are updated and ready to go! 15.5 Updating Packages If all of your current important projects are protected with an {renv} snapshot, this is probably a good time to update your tools (packages). You can do this easily within RStudio with Tools/Check for Package Updates. Select All, and let the updates begin. You will usually want to compile the newest version from source code, rather than a recent version from binaries. If you have problems with updating packages, check the troubleshooting guide at Chapter 12.1.2, Problems with Installing Packages. "],["intermediate-steps-toward-reproducibility.html", "Chapter 16 Intermediate Steps Toward Reproducibility 16.1 Level 3 Reproducibility 16.2 Code Review with a Coding Partner 16.3 Sharing code on GitHub", " Chapter 16 Intermediate Steps Toward Reproducibility Now that you have become familiar with storing your code in saved Rmarkdown documents or RScripts, let’s continue on our journey to Research Reproducibility. Levels of Reproducibility In this chapter, you will learn how to: - Use RStudio Projects - Organize multiple files and folders in a project - Use the {here} package to avoid file paths that are specific to your computer - Do code review with a coding partner - Share code on Github - Sharing deidentified data on websites like figshare and Open Science Foundation - Building project-specific libraries with {renv} and snapshots to maintain your package environment. 16.1 Level 3 Reproducibility At this point, you may have a very long Rmarkdown document, or several documents with code, for a particular data project. There are challenges to organizing your files, your data, and your outputs. RStudio Projects help greatly in organizing your work. These Projects organize a coherent research project into a single folder in your storage/file system, anchored by a unique *.Rproj file in the folder. To get started, you may want to create a particular folder for R projects on your computer. I typically put an Rcode folder inside the Documents folder, and store RStudio Projects inside of the Rcode folder. Take a moment to create your Rcode folder, if you don’t have one already. It is best not to store RStudio Projects inside of other Projects. RStudio looks for the name.Rproj file to identify a project, and it gets confused if these are nested. Avoid having a name.Rproj file in either your Documents folder or your Rcode folder, to avoid problems. Now that you have an Rcode folder, let’s set it as the default location for your files and Projects in R. In RStudio, select (Tools/Global Options). Click on the General Tab on the left. Setting Default File Location Under R Sessions, enter the Default directory as ~/Documents/Rcode, or use the Browse button to browse to this folder and select it. 16.1.1 Creating a New Project in RStudio Open a new RStudio Session (Session/New Session), and start a new project, by selecting (File/New Project). You will be presented with a box of 3 options. Most of the time you will select New Directory. If you have already started on a research project, with data and code files in a particular folder that is not yet an RStudio project, you can select Existing Directory, and browse to the correct folder, and make it an official RStudio project. If you or a collaborator have already shared a project on GitHub, you can select Version Control, select Git, enter the repository URL, and clone this project to your computer. After you select New Directory, you will be asked to select New Project (rather than R package, Shiny App, website, book, etc.), then give the project (and its directory) a short name. It is generally a good idea to check both of the boxes to (1) Create a git repository, and (2) Use renv with this project. Creating a New Project Then click the Create Project button. This creates a new folder on your computer, named new-project, within your Rcode folder. This currently only contains a file named new-project.Rproj and a folder named .Rproj.user which handles files for you. Check your Rcode folder to find the new folder. You can copy or move your data files for this project to this folder. If you have started your coding for this project, move your code files to this project folder as well. Now you need to begin to organize your Project Folder. There are a lot of opinions and workflow templates for this, but most include folders for: data_raw - untouched raw data files data_processed (numbered in order if several iterations) R (for scripts, functions) metadata - plans/aims, TODO lists, descriptions of datafiles code - analysis files (Rmarkdown, numbered in order) output - output tables, figures, manuscript and a README.Rmd file to orient someone new to the project - what are the goals, where are data found, where are the analysis scripts, and how to run the code in order to reproduce the analysis. 16.1.1.1 Naming and Numbering Files In general, it is helpful to name code files with a short useful title that describes their function, along with a leading number, so that the order of running the code files is clear, like 01-import-clean_redcap_data.Rmd 02-import-clean_data-warehouse_data.Rmd 03-merge_redcap_data-warehouse_data.Rmd These will be easily sorted and organized in your new-project/code/ folder, as the leading numbers help organize these (keeping the leading 0 on the front of 0 through 9 helps sorting a lot when you have more than 10 files). It is also helpful to number and name your data files clearly. These should usually correspond to the code files that created them. At the end of a code step, you can save/write your resulting data file to an *.rda file or a *.csv file, to be loaded at the start of the next code file. These might look like the following in the new-project/processed-data/ folder. 01-cleaned_redcap_data.csv 02-cleaned_data-warehouse_data.csv 03-merged_redcap_data-warehouse_data.csv Note that we are using ‘chunked’ naming, so that the files have a consistent naming pattern with chunks in the same order, and distinct chunks are separated by underscores, while informative chunks composed of more than one word are separated by dashes. It is surprisingly important to be thoughtful about your chunked naming scheme early on, as it can help you in many ways in organizing, sorting, and manipulating files later. 16.1.2 File paths and the {here} package One of the great frustrations in sharing an RStudio project with a collaborator is that it is common to refer to files with absolute file paths which are specific to your computer. Unfortunately, the files will inevitably not be in the same place on your collaborator’s computer, and all of the path references to load or write/save files will not work. This frustration has led reasonable people to threaten computer arson, and to propose better approaches. Computer Arson In order to have a set of file paths that work on any computer that has a copy of your Project, you need to use the {here} package. Go ahead and install it now and load it library(here) if you don’t have it already. # install.packages(&quot;here&quot;) library(here) here() ## [1] &quot;/Users/peterhiggins/Documents/RCode/rmrwr-book&quot; Then run the here() function in your Console pane. It will return the path to your current Project. It essentially figures out where your project directory is, and provides this as a home base. This can be used to reference other folders in your project in a relative way. For example, to list the files in your data-raw folder (aka directory, or dir), you can use the {fs} package and the here() function, as in: fs::dir_ls(here(&quot;data-raw&quot;)) The here() function supplies the file path details up to your RStudio Project folder, and then you can add any subfolder or filename details within the parentheses. Similarly, you can write your cleaned and merged data file to the data-processed folder with write_csv(here(&quot;data-processed/03-merged_redcap_data-warehouse_data.csv&quot;)) Which will write the csv file to the data-processed folder within your Project. Now when you share the Project with a collaborator, the file paths for reading and writing files will now all work. Move a data file and a code file into your Project Folder. Create folders like data-raw, data-processed, metadata, code. You can do this by ‘hand’ with your computer’s operating system, or with the fs package, using fs::create_dir(\"name\") Use fs:dir_ls(here()) to list the files in the top level of your project Move the files into the appropriate folders re-run fs:dir_ls(here()) to see the change in the listing Now run fs:dir_ls(here(\"foldername\")) to see the contents of each of the folders (by name) that you created. 16.2 Code Review with a Coding Partner Checking code is a largely thankless job, but it is extremely important to prevent errors. It also encourages you to document your thinking and your code more thoroughly if you know someone else will be reviewing and checking it. If they can’t understand what you are doing, they can’t adequately review your code. It is important to find someone, often at a parallel position in their career, who is doing data coding for their research, so that you can become code review partners. Each of you can help the other become a better, clearer coder, and each of you can be more confident that your code does not contain mistakes. It can be very helpful to do this in several stages, to prevent problems later, rather than trying to review an entire project’s code in one go. after data import, cleaning, relabeling and merging After initial analysis After all figures, tables, manuscript ready 16.2.1 Checklist for Code Review This is a general and fairly comprehensive guidance. Use the items you (and the code creator) wish to focus on. Review a small, manageable chunk of code at a time. This takes a lot of focus, and often a lot of solving mysteries (poorly documented code). Make sure you understand the goal of the code, and/or the hypothesis being tested Is the README clear enough on where the data are, where the code is located, and what to run in what order? Is the code itself, with comments &amp; text, readable and understandable? Does it stand on its own without a lot of over-the-shoulder explanations? Is it structured in a project with {here} and {renv} to have consistent file paths and package versions? Are there libraries loaded that are not needed? Are all libraries loaded at the top of the code script? Think about file names and variable names - are they clear and helpful? Could they be better? Do the variable names include units, when appropriate, like sbp_mmHg, ast_iu-mL, cr_mg-L, to make clear what is being measured without a codebook? Are the values (for categorical and ordinal scales) self-explanatory? (e.g “0_No” and “1_Yes” are superior to 0/1, or “1_Asian”, “2_Black”, or “1_placebo”, “2_amazingmab”, rather than having to backtrack to a codebook? Has data been checked for missing data, outliers, time trends, correlations? Are things that should be correlated actually correlated? (Data Exploration and Validation (DEV?)) Does the data contain any PHI that needs to be de-identified? Can it be scrubbed? Does all the code run in a new, clean session on a different computer? Use {tidylog} to track data cleaning/processing steps. Are there a lot more rows/columns disappearing than expected? Check and validate data cleaning steps - was the result what was intended? Check and validate merges - was the result what was intended? Check and validate recoding of variables - mutate steps, case_when steps, intervention arms, outcome definitions - as the result what was intended? Keep notes, make recommendations for improvement If things are unclear, suggest how to make them clear Are there repeated steps that could be made into functions? Are there functions repeated multiple times that could be done once with purrr::map? Are there scientific threats to the validity of the research? Is there possible bias? mis-measurement? Potential confounders not present in the dataset? is this the best dataset to answer this question? Any general recommendations or suggestions of tools, packages, or functions that could be used to make this (or future) code projects better? Sign off on this code chunk - add a signoff comment at the end # I, NAME, have reviewed this code for errors and clarity on DATE and APPROVE this code. 16.3 Sharing code on GitHub "],["comparing-two-measures-of-centrality.html", "Chapter 17 Comparing Two Measures of Centrality 17.1 Common Problem 17.2 One Sample T test 17.3 Insert flipbook for ttest here 17.4 Fine, but what about 2 groups? 17.5 3 Assumptions of Student’s t test 17.6 Getting results out of t.test 17.7 Reporting the results from t.test using inline code", " Chapter 17 Comparing Two Measures of Centrality A common question in medical research is whether one group had a better outcome than another group. These outcomes can be measured with dichotomous outcomes like death or hospitalization, but continuous outcomes like systolic blood pressure, endoscopic score, or ejection fraction are more commonly available, and provide more statistical power, and usually require a smaller sample size. There is a tendency in clinical research to focus on dichotomous outcomes, even to the point of converting continuous measures to dichotomous ones (aka “dichotomania”, see Frank Harrell comments here), for fear of detecting and acting upon a small change in a continuous outcome that is not clinically meaningful. While this can be a concern, especially in very large, over-powered studies, it can be addressed by aiming for a continuous difference that is at least as large as one that many clinicians agree (a priori) is clinically important (the MCID, or Minimum Clinically Important Difference). The most common comparison of two groups with a continuous outcome is to look at the means or medians, and determine whether the available evidence suggests that these are equal (the null hypothesis). This can be done for means with Student’s t-test. Let’s start by looking at the cytomegalovirus data set. This includes data on 64 patients who received bone marrow stem cell transplant, and looks at their time to activation of CMV (cytomegalovirus). In the code chunk below, we group the data by donor cmv status (donor.cmv), and look at the mean time to CMV activation (time.to.cmv variable). Run the code (using the green arrow at the top right of the code chunk below) to see the difference in time to CMV activation in months between groups. Try out some other grouping variables in the group_by statement, in place of donor.cmv. Consider variables like race, sex, and recipient.cmv. Edit the code and run it again with the green arrow at the top right. # insert libraries in each chunk as if independent library(tidyverse) library(medicaldata) cytomegalovirus %&gt;% group_by(sex) %&gt;% summarize(mean_time2cmv = mean(time.to.cmv)) -&gt; summ summ ## # A tibble: 2 × 2 ## sex mean_time2cmv ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 13.7 ## 2 1 12.7 That seems like a big difference for donor.cmv, between 13.7303333 months and 12.7441176 months. And it makes theoretical sense that having a CMV positive donor is more likely to be associated with early activation of CMV in the recipient. But is it a significant difference, one that would be very unlikely to happen by chance? That depends on things like the number of people in each group, and the standard deviation in each group. That is the kind of question you can answer with a t-test, or for particularly skewed data like hospital length of stay or medical charges, a Wilcoxon test. 17.1 Common Problem Comparing two groups Mean or median vs. expected Two arms of study - independent Pre and post / spouse and partner / left vs right arm – paired groups Are the means significantly different? Or the medians (if not normally distributed)? 17.1.1 How Skewed is Too Skewed? Formal test of normality = Shapiro-Wilk test Use base data set called ToothGrowth library(tidyverse) library(medicaldata) data &lt;- cytomegalovirus head(data) ## ID age sex race diagnosis ## 1 1 61 1 0 acute myeloid leukemia ## 2 2 62 1 1 non-Hodgkin lymphoma ## 3 3 63 0 1 non-Hodgkin lymphoma ## 4 4 33 0 1 Hodgkin lymphoma ## 5 5 54 0 1 acute lymphoblastic leukemia ## 6 6 55 1 1 myelofibrosis ## diagnosis.type time.to.transplant prior.radiation ## 1 1 5.16 0 ## 2 0 79.05 1 ## 3 0 35.58 0 ## 4 0 33.02 1 ## 5 0 11.40 0 ## 6 1 2.43 0 ## prior.chemo prior.transplant recipient.cmv donor.cmv ## 1 2 0 1 0 ## 2 3 0 0 0 ## 3 4 0 1 1 ## 4 4 0 1 0 ## 5 5 0 1 1 ## 6 0 0 1 1 ## donor.sex TNC.dose CD34.dose CD3.dose CD8.dose TBI.dose ## 1 0 18.31 2.29 3.21 0.95 200 ## 2 1 4.26 2.04 NA NA 200 ## 3 0 8.09 6.97 2.19 0.59 200 ## 4 1 21.02 6.09 4.87 2.32 200 ## 5 0 14.70 2.36 6.55 2.40 400 ## 6 1 4.29 6.91 2.53 0.86 200 ## C1/C2 aKIRs cmv time.to.cmv agvhd time.to.agvhd cgvhd ## 1 0 1 1 3.91 1 3.55 0 ## 2 1 5 0 65.12 0 65.12 0 ## 3 0 3 0 3.75 0 3.75 0 ## 4 0 2 0 48.49 1 28.55 1 ## 5 0 6 0 4.37 1 2.79 0 ## 6 0 2 1 4.53 1 3.88 0 ## time.to.cgvhd ## 1 6.28 ## 2 65.12 ## 3 3.75 ## 4 10.45 ## 5 4.37 ## 6 6.87 17.1.2 Visualize the Distribution of data variables in ggplot Use geom_histogram or geom_density (pick one or the other) look at the distribution of CD3.dose or time.to.cmv Bonus points: facet by sex or race or donor.cmv Your turn to try it library(tidyverse) library(medicaldata) data %&gt;% ggplot(mapping = aes(time.to.cmv)) + geom_density() + facet_wrap(~sex) + theme_linedraw() library(tidyverse) library(medicaldata) data %&gt;% ggplot(mapping = aes(time.to.cmv)) + geom_histogram() + facet_wrap(~race) 17.1.3 Visualize the Distribution of data$len in ggplot The OJ group is left skewed May be problematic for using means formally test with Shapiro-Wilk library(tidyverse) library(medicaldata) data$time.to.cmv %&gt;% shapiro.test() ## ## Shapiro-Wilk normality test ## ## data: . ## W = 0.68261, p-value = 0.0000000001762 17.1.4 Results of Shapiro-Wilk p-value = 0.1091 p not &lt; 0.05 Acceptably close to normal OK to compare means rather than medians can use t test rather than wilcoxon test if p is &lt; 0.05, use wilcoxon test also known as Mann-Whitney test a rank-based (non-parametric) test 17.1.5 Try it yourself use df &lt;- msleep library(tidyverse) library(medicaldata) df &lt;- msleep head(df$sleep_total) ## [1] 12.1 17.0 14.4 14.9 4.0 14.4 test the normality of total sleep hours in mammals 17.1.6 Mammal sleep hours library(tidyverse) library(medicaldata) shapiro.test(df$sleep_total) ## ## Shapiro-Wilk normality test ## ## data: df$sleep_total ## W = 0.97973, p-value = 0.2143 meets criteria - acceptable to consider normally distributed now consider - is the mean roughly 8 hours of sleep per day? 17.2 One Sample T test univariate test Ho: mean is 8 hours Ha: mean is not 8 hours can use t test because shapiro.test is NS 17.2.1 How to do One Sample T test library(tidyverse) library(medicaldata) t.test(df$sleep_total, alternative = &quot;two.sided&quot;, mu = 8) Try it out, see if you can interpret results 17.2.2 Interpreting the One Sample T test ## ## One Sample t-test ## ## data: df$sleep_total ## t = 4.9822, df = 82, p-value = 0.000003437 ## alternative hypothesis: true mean is not equal to 8 ## 95 percent confidence interval: ## 9.461972 11.405497 ## sample estimates: ## mean of x ## 10.43373 p is highly significant can reject the null, accept alternative sample mean 10.43, CI 9.46-11.41 17.2.3 What are the arguments of the t.test function? x = vector of continuous numerical data y= NULL - optional 2nd vector of continuous numerical data alternative = c(“two.sided”, “less”, “greater”), mu = 0 paired = FALSE var.equal = FALSE conf.level = 0.95 documentation 17.3 Insert flipbook for ttest here Below is a flipbook. It illustrates a bit of how to do a t-test. click on it and you can use the arrow keys to proceed forward and back through the slides, as you add lines of code and more results occur. Let’s start with a flipbook slide show. When the title slide appears, you can step through each line of the code to see what it does. The right/left and/or up/down arrows will let you move forward and backward in the code. You can use the arrow keys to go through it one step at a time (forward or backward, depending on which arrow key you use), to see what each line of code actually does. Give it a try below. See if you can figure out what each line of code is doing. 17.3.1 Flipbook Time! This is t-testing in action. 17.4 Fine, but what about 2 groups? consider df$vore library(tidyverse) library(medicaldata) prostate &lt;- medicaldata::blood_storage tabyl(prostate$AA) ## prostate$AA n percent ## 0 261 0.8259494 ## 1 55 0.1740506 hypothesis - herbivores need more time to get food, sleep less than carnivores how to test this? normal, so can use t test for 2 groups 17.4.1 Setting up 2 group t test formula interface: outcome ~ groupvar library(tidyverse) library(medicaldata) df %&gt;% filter(vore %in% c(&quot;herbi&quot;, &quot;carni&quot;)) %&gt;% t.test(formula = sleep_total ~ vore, data = .) Try it yourself What do the results mean? 17.4.2 Results of the 2 group t test ## ## Welch Two Sample t-test ## ## data: sleep_total by vore ## t = 0.63232, df = 39.31, p-value = 0.5308 ## alternative hypothesis: true difference in means between group carni and group herbi is not equal to 0 ## 95 percent confidence interval: ## -1.911365 3.650509 ## sample estimates: ## mean in group carni mean in group herbi ## 10.378947 9.509375 17.4.3 Interpreting the 2 group t test Welch t-test (not Student) Welch does NOT assume equal variances in each group p value NS accept null hypothesis Ho: means of groups roughly equal Ha: means are different 95% CI crosses 0 Carnivores sleep a little more, but not a lot 17.4.4 2 group t test with wide data You want to compare column A with column B (data are not tidy) Do mammals spend more time awake than asleep? library(tidyverse) library(medicaldata) t.test(x = df$sleep_total, y = df$awake, data = msleep) 17.4.5 Results of 2 group t test with wide data library(tidyverse) library(medicaldata) t.test(x = df$sleep_total, y = df$awake, data = msleep) ## ## Welch Two Sample t-test ## ## data: df$sleep_total and df$awake ## t = -4.5353, df = 164, p-value = 0.00001106 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.498066 -1.769404 ## sample estimates: ## mean of x mean of y ## 10.43373 13.56747 17.5 3 Assumptions of Student’s t test Sample is normally distributed (test with Shapiro) Variances are homogeneous (homoskedasticity) (test with Levene) Observations are independent not paired like left vs. right colon not paired like spouse and partner not paired like measurements pre and post Rx 17.5.1 Testing Assumptions of Student’s t test Normality - test with Shapiro If not normal, Wilcoxon &gt; t test Equal Variances - test with Levene If not equal, Welch t &gt; Student’s t Observations are independent Think about data collection are some observations correlated with some others? If correlated, use paired t test 17.6 Getting results out of t.test Use the tidy function from the broom package Do carnivores have bigger brains than insectivores? library(tidyverse) library(medicaldata) library(broom) df %&gt;% filter(vore %in% c(&quot;carni&quot;, &quot;insecti&quot;)) %&gt;% t.test(formula = brainwt ~ vore, data = .) %&gt;% tidy() -&gt; result result 17.6.1 Getting results out of t.test ## # A tibble: 1 × 10 ## estimate estimate1 estimate2 statistic p.value parameter ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0577 0.0793 0.0216 1.20 0.253 12 ## # … with 4 more variables: conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, ## # method &lt;chr&gt;, alternative &lt;chr&gt; 17.7 Reporting the results from t.test using inline code use backticks before and after, start with r i.e. My result is [backtick]r code here[backtick]. The mean brain weight for carnivores was 0.0792556 The mean brain weight for herbivores was 0.02155 The difference was 0.0577056 The t statistic for this Two Sample t-test was 1.1995501 The p value was 0.2534631 The confidence interval was from -0.05 to 0.16 17.7.1 For Next Time Skewness and Kurtosis Review Normality When to use Wilcoxon Levene test for equal variances When to use Welch t vs. Student’s t Paired t and Wilcoxon tests "],["sample-size-calculations-with-pwr.html", "Chapter 18 Sample Size Calculations with {pwr} 18.1 Sample Size for a Continuous Endpoint (t-test) 18.2 One Sample t-test for Lowering Creatinine 18.3 Paired t-tests (before vs after, or truly paired) 18.4 2 Sample t tests with Unequal Study Arm Sizes 18.5 Testing Multiple Options and Plotting Results 18.6 Your Turn 18.7 Sample Sizes for Proportions 18.8 Sample size for two proportions, equal n 18.9 Sample size for two proportions, unequal arms 18.10 Your Turn 18.11 add chi square 18.12 add correlation test 18.13 add anova 18.14 add linear model 18.15 add note on guessing effect sizes - cohen small, medium, large 18.16 Explore More", " Chapter 18 Sample Size Calculations with {pwr} When designing clinical studies, it is often important to calculate a reasonable estimate of the needed sample size. This is critical for planning, as you may find out very quickly that a reasonable study budget and timeline will be futile. Grant funding agencies will be very interested in whether you have a good rationale for your proposed sample size and timeline, so that they can avoid wasting their money. Fortunately, the {pwr} package helps with many of the needed calculations. Let’s first install this package from CRAN, and load it with a library() function. Remember that you can copy each code chunk to the clipboard, then paste it into your RStudio console (or a script) to edit and run it. Just hover your mouse pointer over the top right corner of each code chunk until a copy icon appears, then click on it to copy the code. install.packages(&#39;pwr&#39;) library(pwr) The {pwr} package has a number of functions for calculating sample size and power. Functions in the {pwr} package Test Sample Size Function one-sample, two-sample, and paired t-tests pwr.t.test() two-sample t-tests (unequal sample sizes) pwr.t2n.test() two-sample proportion test (unequal sample sizes) pwr.2p2n.test() one-sample proportion test pwr.p.test() two-sample proportion test pwr.2p.test() two-sample proportion test (unequal sample sizes) pwr.2p2n.test() one-way balanced ANOVA pwr.anova.test() correlation test pwr.r.test() chi-squared test (goodness of fit and association) pwr.chisq.test() test for the general linear model pwr.f2.test() Now that you have {pwr} up and running, we can start with a simple example. 18.1 Sample Size for a Continuous Endpoint (t-test) Let’s propose a study of a new drug to reduce hemoglobin A1c in type 2 diabetes over a 1 year study period. You estimate that your recruited participants will have a mean baseline A1c of 9.0, which will be unchanged by your placebo, but reduced (on average) to 7.0 by the study drug. You need to calculate an effect size (aka Cohen’s d) in order to estimate your sample size. This effect size is equal to the difference between the means at the endpoint, divided by the pooled standard deviation. Many clinicians can estimate the means and the difference, but the pooled standard deviation is not very intutitive. Sometimes you have an estimate from pilot data (though these tend to have wide confidence intervals, as pilot studies are small). In other circumstances, you can estimate a standard deviation for Hgb A1c from values from a large data warehouse. When you don’t have either of these, it can be helpful to start by estimating the range of values. This is something that is intutitive, and experienced clinicians can do fairly easily. Just get a few clinicians in a room, and ask them for the highest and lowest values of HgbA1c that they have ever seen. You will quickly find a minimum and maximum that you can estimate as the range (in this case, let’s say 5.0 and 17.0 for min and max of Hgb A1c). This range divided by 4 is a reasonable rough estimate of the standard deviation. Remember that a normally distributed continuous value will have a 95% confidence interval that is plus or minus 1.96 standard deviations from the sample mean. Round this up to 2 for the full range, and you can see why we divide the range by 4 to get an estimate of the standard deviation. In our case, the difference is 2 and the range/4 (estimate of SD) is 3. So our effect size (Cohen’s d) is 0.66. Plug in 0.66 for d in the code chunk below, and run this code chunk to get an estimate of the n in each arm of a 2 armed study with a two sample t-test of the primary endpoint. pwr::pwr.t.test(n = NULL, sig.level = 0.05, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;, power = 0.80, d = __) We come up with 37.02 participants in each group to provide 80% power to detect a difference of 2 in HgbA1c, assuming a standard deviation of 3, using a two-sided alpha of 0.05. To conduct this study, assuming a 20% dropout rate in each arm, would require 37+8 subjects per arm, or 90 overall. At an enrollment rate of 10 per month, it will require 9 months to enroll all the participants, and 21 months (9 + 12 month intervention) to complete the data collection. It is a very common mistake to look at the result for n, and assume that this is your total sample size needed. The n provided by {pwr} is the number per arm. You need to multiply this n by the number of arms (or in a paired analysis, by the number of pairs) to get your total n. Another common mistake is to assume no dropout of participants. It is important to have a reasonable estimate (10-20% for short studies, 30-50% for long or demanding studies) and inflate your intended sample size by this amount. It is even better if you know from similar studies what the actual dropout rate was, and use this as an estimate (if there are similar previous studies). As a general rule, it is better to be conservative, and estimate a larger sample size, than to end up with p = 0.07. Once you define your test type (the options are “two.sample”, “one.sample”, and “paired”), and the alternative (“two.sided”, “greater”, or “less”), four variables remain in a sample size and power calculation. These are the remaining four arguments of the pwr.t.test() function. These are: n the significance level (sig.level) the power the effect size (Cohen’s d) If you know any three of these, you can calculate the fourth. In order to do this, you set the one of these four arguments that you want to calculate equal to NULL, and specify the other 3. Imagine that we only have enough funds to run this study on 50 participants. What would our power be to detect a difference of 2 in Hgb A1c? You can set the power to NULL, and the n to 25 (remember that n is per arm), and run the chunk below. pwr::pwr.t.test(n = __, # note that n is per arm sig.level = 0.05, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;, power = __, d = 0.66) We end up with 62.8% power, assuming no participant dropout (which is an extremely unlikely assumption). You can do the same thing, changing the NULL, to calculate an effect size or a significance level, if you have any need to. In most cases, you are calculating a sample size, then realizing that you might not have that much money/resources. Then many calculate the power you would obtain given the resources you actually have. Let’s show a few more examples. 18.2 One Sample t-test for Lowering Creatinine Eddie Enema, holistic healer, has proposed an unblinded pilot study of thrice-daily 2 liter enemas with “Eddie’s Dialysis Cleanser,” a proprietary mix of vitamins and minerals, which he believes will lower the serum creatinine of patients on the kidney transplant waiting list by more than 1.0 g/dL in 24 hours. The creatinine SD in this group of patients is 2. The null hypothesis is &lt; 1.0 g/dL. The alternative hypothesis is &gt;= 1.0 g/dL. Cohen’s d is 1.0 (the proposed change, or delta)/2 (the SD) = 0.5. How many participants would Eddie Enema have to recruit to have 80% power to test this one-sample, one-sided hypothesis, with an alpha of 0.05? Check each of the argument values and run the chunk below to find out. pwr::pwr.t.test(n = NULL, # note that n is per arm sig.level = 0.05, type = &quot;one.sample&quot;, alternative = &quot;greater&quot;, power = 0.8, d = 0.5) Fast Eddie would need to recruit slightly more than 26 participants (you always have to round up to get whole human participants) to have 80% power, assuming no dropout between the first and third enema, or before the blood draw 24 hours after baseline. Note that since this is an unblinded, one-sample study, The n in the results is multiplied by the number of arms (there is only 1 arm) to give you a sample size of 27. A note about alpha and beta alpha is described as the type I error, or the probability of declaring significant a difference that is not truly significant. We often use a two-sided alpha, which cuts the region of significance in half, and distributes it to both tails of the distribution, allowing for both significant positive and negative differences. Alpha is commonly set at 0.05, which works out to 0.025 on each tail of the distribution with a two-tailed alpha. beta is the power, or (1- the risk of type II error). Type II error is the probability of missing a significant result and declaring it nonsignificant after hypothesis testing. Power is often set at 80%, or 0.8, but can be 90%, 95%, or 99%, depending on how important it is not to miss a significant result, and how much money and time you have to spend (both of which tend to increase N and power). There is often an important tradeoff between type I and type II error. Things that decrease type II error (increase power) like spending more time and money for a larger N, will increase your risk of type I error. Conversely, reducing your risk of type I error will generally increase your risk of type II error. You may be in situations in which you have to decide which type of error is more important to avoid for your clinical situation to maximize benefit and minimize harms for patients. 18.3 Paired t-tests (before vs after, or truly paired) As you can see from the above example, you can use a before-after design to measure differences from baseline, and essentially convert a two-sample paired design (each participant’s baseline measurement is paired with their post-intervention measurement) to a single sample design based on the difference between the before and after values. The before-after (or baseline-postintervention) design is probably the most common paired design, but occasionally we have truly paired designs, like when we test an ointment for psoriasis on one arm, and use a placebo or sham ointment on the other arm. When this is possible, through bilateral symmetry (this also works for eyedrops in eyes, or dental treatments), it is much more efficient (in the recruiting sense) than recruiting separate groups for the treatment and control arms. To see the difference between two-sample and paired designs, run the code chunk below, for a two-sample study with a Cohen’s d of 0.8 and 80% power. Then change the type to “paired”, and see the effect on sample size. pwr::pwr.t.test(n = NULL, # note that n is per arm sig.level = 0.05, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;, power = 0.8, d = 0.8) Note that this changes the needed sample size from 52 subjects (26 per arm) to 15 subjects (as there is only one participant needed each paired application of 2 study treatments, and n in this case indicates the number of pairs), though it would be wise to randomize patients to having the treatment on the right vs. left arm (to maintain the blind). This is a large gain in recruiting efficiency. Use paired designs whenever you can. 18.4 2 Sample t tests with Unequal Study Arm Sizes Occasionally investigators want unbalanced arms, as they feel that patients are more likely to participate if they have a greater chance of receiving the study drug than the placebo. It is fairly common to use 2:1 or 3:1 ratios. Larger ratios, like 4:1 or 5:1, are thought to risk increasing the placebo response rate, as participants assume that they are on the active drug. This is somewhat less efficient in recruiting terms, but it may improve the recruiting rate enough to compensate for the loss in efficiency. This requires a slightly different function, the pwr.t2n.test() function. Let’s look at an example below. Instead of n, we have n1 and n2, and we have to specify one of these, and leave the other as NULL. Or we can try a variety of ratios of n1 and n2, leaving the power set to NULL, and test numbers to produce the desired power. We are proposing a study in which the expected reduction in systolic blood pressure is 10 mm Hg, with a standard deviation of 20 mm Hg. We choose an n1 of 40, and a power of 80%, then let the function determine n2. pwr::pwr.t2n.test(n1 = 40, n2 = NULL, sig.level = 0.05, alternative = &quot;two.sided&quot;, power = 0.8, d = 0.5) In this case, n2 works out to slightly over 153 in the drug arm, or nearly 4:1. Calculating effect size, or Cohen’s d. You can calculate the d value yourself. Or, you can make your life easier to let the {pwr} package do this for you. You can leave the calculation of the delta/SD to the program, by setting d = (20-10)/20, and the program will calculate the d of 0.5 for you. We can also round up the ratio to 4:1 (160:40) and determine the resulting power. pwr::pwr.t2n.test(n1 = c(40), n2 = c(160), sig.level = 0.05, alternative = &quot;two.sided&quot;, power = NULL, d = 0.5) This provides a power of 80.3%. 18.5 Testing Multiple Options and Plotting Results It can be helpful to compare multiple scenarios, varying the n or the estimated effect size, to examine trade-offs and potential scenarios when planning a trial. You can test multiple particular scenarios by listing the variables in a concatenated vector, as shown below for n1 and n2. pwr::pwr.t2n.test(n1 = c(40, 60, 80), n2 = c(80, 120, 160), sig.level = 0.05, alternative = &quot;two.sided&quot;, power = NULL, d = 0.5) This provides 3 distinct scenarios, with 3 pairs of n1/n2 values, and the calculated power for each scenario. You can also examine many scenarios, with the sequence function, seq(). For the sequence function, three arguments are needed: from, the number the sequence starts from to, the number the sequence ends at (inclusive) by, the number to increment by Note that the length of the sequences produced by seq() must match (or be a multiple of the other) if you are sequencing multiple arguments, so that there is a number for each scenario. If the lengths of the sequences are multiples of each other (8 and 4 in the example below), the shorter sequence (n2) will be silently “recycled” (used again in the same order) to produce a vector of matching length (8). pwr::pwr.t2n.test(n1 = seq(from = 40, to = 75, by = 5), n2 = seq(60, 120, 20), sig.level = 0.05, alternative = &quot;two.sided&quot;, power = NULL, d = 0.5) Sometimes it is helpful to look at multiple scenarios and plot the results. You can do this by leaving n = NULL, and plotting the results, as seen below. The null value will be varied across a reasonable range, and the results plotted, with an optimal value identified. The plot function will use ggplot2 if this package is loaded, or base R plotting if ggplot2 is not available. As you can see below, you can modify the ggplot2 plot of the results with standard ggplot2 functions. results &lt;- pwr::pwr.t2n.test(n1 = c(40), n2 = NULL, sig.level = 0.05, alternative = &quot;two.sided&quot;, power = 0.80, d = 0.5) plot(results) + ggplot2::theme_minimal(base_size = 14) + labs(title = &#39;Optimizing Sample Size for my 2-Sided t test&#39;, subtitle = &quot;Always Round up for Whole Participants, N = 194&quot;) Note that the results object is a list, and you can access individual pieces with the dollar sign operator, so that `results$n1` equals 40, and `results$n2` equals 153. You can examine the components of the results object in the Environment pane in RStudio. You can use these in inline R expressions in an Rmarkdown document to write up your results. Remember that each inline R expression is wrapped in backwards apostrophes, like `r code` (using the character to the left of the 1 key on the standard US keyboard), and starts with an r to let the computer know that the incoming code is written in R. This helps you write up a sentence like the below for a grant application: Using an estimated effect size of 0.5, with a two-sided alpha of 0.05, we calculated that for 40 participants in group 1, 153.0968718 participants would be needed in group 2 to produce a power of 0.8. When you knit an Rmarkdown file with these inline R expressions, each will be automatically converted to the result number and appear as standard text. 18.6 Your Turn Try calculating the sample size or power needed in the continuous outcome scenarios below. See if you can plot the results as directed by editing the code chunks. 18.6.1 Scenario 1: FEV1 in COPD You want to increase the FEV1 (forced expiratory volume in 1 second) of patients with COPD (chronic obstructive pulmonary disease) by 10% of predicted from baseline using weekly inhaled stem cells vs. placebo. Unfortunately, the standard deviation of FEV1 measurements is 20%. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with equal n in each of the two arms. Fill in the blanks in the code chunk below to calculate the sample size needed (n x number of arms). Remember that the effect size (Cohen’s d) = change in endpoint (delta)/SD of the endpoint. pwr::pwr.t.test(n = __, # note that n is per arm sig.level = __, type = &quot;__&quot;, alternative = &quot;__&quot;, power = __, d = __) You should get 128 participants (assuming no dropout) from 64 per arm. Cohen’s d is 10/20 = 0.5. It can be tricky to keep the type of “two.sample” and the alternative of “two.sided” straight. But you can do this! 18.6.2 Scenario 2: BNP in CHF You want to decrease the BNP (brain natriuretic protein) of patients with CHF (congestive heart failure) by 300 pg/mL from baseline with a new oral intropic agent vs. placebo. BNP levels go up during worsening of heart failure, and a variety of effective treatments lower BNP, which can function as surrogate marker in clinical trials. The standard deviation of BNP measurements is estimated at 350 pg/mL. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with equal n in each of the two arms. Also consider an alternative scenario with a change in BNP of only 150 pg/mL. Remember that the effect size (Cohen’s d) = change in endpoint (delta)/SD of the endpoint. Fill in the blanks in the code chunk below (2 scenarios) to calculate the sample size needed (n x number of arms) for both alternatives. pwr::pwr.t.test(n = __, # note that n is per arm sig.level = __, type = &quot;two.__&quot;, alternative = &quot;two.__&quot;, power = __, d = __/__) pwr::pwr.t.test(n = __, # note that n is per arm sig.level = __, type = &quot;two.__&quot;, alternative = &quot;two.__&quot;, power = __, d = __/__) You should get 46 participants (assuming no dropout) from 23 per arm x 2 arms, or 174 participants (87x2) with the alternative effect size. The effect size (Cohen’s d) is 300/350 = 0.86 in the original, and 150/350 (0.43) in the alternative effect. Note that you can let R calculate the Cohen’s d - just type in 300/350 and 150/350, and R will use these as values of d. 18.6.3 Scenario 3: Barthel Index in Stroke You want to increase the Barthel Activities of Daily Living Index of patients with stroke by 25 points from baseline with an intensive in-home PT and OPT intervention vs. usual care (which usually increases BADLI by only 5 points). You roughly estimate the standard deviation of Barthel index measurements as 38. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with equal n in each of the two arms. You want to consider multiple possible options for n, and plot these for a nice figure in your grant application. Fill in the blanks in the code chunk below to calculate and plot the sample size needed (n x number of arms). results &lt;- pwr::pwr.t.test(n = __, # note that n is per arm sig.level = __, type = &quot;two.__&quot;, alternative = &quot;two.sided&quot;, power = __, d = __ ) plot(results) You should get an optimal sample size of 116 participants (assuming no dropout) from 58 per arm x 2 arms, with a nice plot to show this in your grant proposal. The effect size (Cohen’s d) is (25-5)/38 = 0.526. 18.7 Sample Sizes for Proportions Let’s assume that patients discharged from your hospital after a myocardial infarction have historically received a prescription for aspirin 80% of the time. A nursing quality improvement project on the cardiac floor has tried to increase this rate to 95%. How many patients do you need to track after the QI intervention to determine if the proportion has truly increased? the null hypothesis is that the proportion is 0.8 the alternative hypothesis is that the proportion is 0.95. For this, we need the pwr.p.test() function for one proportion. We will also use a built-in function of {pwr}, the ES.h() function, to help us calculate the effect size. This function takes our two hypothesized proportions and calculates an effect size with an arcsine transformation. pwr.p.test(h = ES.h(p1 = 0.95, p2 = 0.80), n = NULL, sig.level = 0.05, power = 0.80, alternative = &quot;greater&quot;) ## ## proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.4762684 ## n = 27.25616 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater We need to evaluate at least the next 28 patients discharged with MIs to have 80% power to test this one-sided hypothesis. A note about test sided-ness and publication. Frequently in common use, you may only be focused on an increase or decrease in a proportion or a continuous outcome, and a one-sided test seems reasonable. This is fine for internal use or local quality improvement work. However, for FDA approval of a drug, for grant applications, or for journal publications, the standard is to always use two-sided tests, being open to the possibility of both improvement or worsening of the outcome you are studying. This is important to know before you submit a grant application, a manuscript for publication, or a dossier for FDA approval of a drug or device. 18.8 Sample size for two proportions, equal n For this, we need the pwr.2p.test() function for two proportions. You want to calculate the sample size for a study of a cardiac plexus parasympathetic nerve stimulator for pulmonary hypertension. You expect the baseline one year mortality to be 15% in high-risk patients, and expect to reduce this to 5% with this intervention. You will compare a sham (turned off) stimulator to an active stimulator in a 2 arm study. Use a 2-sided alpha of 0.05 and a power of 80%. Copy and edit the code chunk below to determine the sample size (n, rounded up) per arm, and the overall sample size (2n) fo the study. pwr.p.test(h = ES.h(p1 = __, p2 = __), n = __, sig.level = __, power = __, alternative = &quot;__&quot;) We need to enroll at least 67 per arm, or 134 overall. Dichotomous endpoints are generally regarded as having greater clinical significance than continuous endpoints, but often require more power and sample size (and more money and time). Most investigators are short on money and time, and prefer continuous outcome endpoints. 18.9 Sample size for two proportions, unequal arms For this, we need the pwr.2p2n.test() function for two proportions with unequal sample sizes. Imagine you want to enroll class IV CHF patients in a device trial in which they will be randomized 3:1 to a device (vs sham) that restores their serum sodium to 140 mmol/L and their albumin to 40 mg/dL each night. You expect to reduce 1 year mortality from 80% to 65% with this device. You want to know what your power will be if you enroll 300 in the device arm and 100 in the sham arm. pwr.2p2n.test(h = ES.h(p1 = __, p2 = __), n1 = __, n2 = __, sig.level = __, power = __, alternative = &quot;two.sided&quot;) This (300:100) enrollment will have 84.6% power to detect a change from 15% to 5% mortality, with a two-sided alpha of 0.05. 18.10 Your Turn Try calculating the sample size or power needed in the proportional outcome scenarios below. See if you can plot the results as directed by editing the code chunks. 18.10.1 Scenario 1: Mortality on Renal Dialysis You want to decrease the mortality of patients on renal dialysis, which averages 20% per year in your local dialysis center. You will randomize patients to a bundle of statin, aspirin, beta blocker, and weekly erythropoietin vs. usual care, and hope to reduce annual mortality to 10%. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with equal n in each of the two arms. Fill in the blanks in the code chunk below to calculate the sample size needed (n x number of arms). pwr.p.test(h = ES.h(p1 = __, p2 = __), n = __, sig.level = .05, power = __, alternative = &quot;two.__&quot;) You always round up first (to whole participants per arm), then multiply by the number of arms. You will need a minimum of 98 per arm, for a total of 196 participants needed to complete the trial. 18.10.2 Scenario 2: Intestinal anastomosis in Crohn’s disease You want to decrease 1-year endoscopic recurrence rate in Crohn’s disease from 90% to 70%. A local surgeon claims that his new “slipknot anastomosis” technique will accomplish this, by reducing colonic backwash and thereby, reducing endoscopic recurrence. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with equal n in each of the two arms. Also consider an alternative, more conservative scenario with a endoscopic recurrence rate of 80% with the new method. Fill in the blanks in the code chunk below to calculate the sample size needed (n x number of arms) for both alternatives. pwr.p.test(h = c(ES.h(p1 = 0.9, p2 = __)), n = NULL, sig.level = .05, power = __, alternative = &quot;two.sided&quot;) pwr.p.test(h = c(ES.h(p1 = __, p2 = __)), n = __, sig.level = __, power = .80, alternative = __) With the originally claimed recurrence proportion of 70%, you will need 30 participants per arm, or 60 for the whole study. The more conservative estimate will require 98 subjects per arm, or 196 for the whole study. 18.10.3 Scenario 3: Metformin in Donuts Your local endocrinologist has identified consumption of glazed donuts as a major risk factor for development of type 2 diabetes in your region. She proposes to randomize participants to glazed donuts spiked with metformin vs usual donuts, expecting to reduce the 1 year proportion of prediabetics with a HgbA1c &gt; 7.0 from 25% to 10%. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with 2 times as many participants in the metformin donut arm. You want to consider multiple possible sample sizes (n = 25, 50, 75) for the control glazed donuts, with 2n (double the sample size in each scenario) for the metformin donuts group. Fill in the blanks in the code chunk below to calculate the resulting power for each of the three sample size scenarios. pwr.2p2n.test(h = ES.h(p1 = __, p2 = __), n1 = seq(from = __, to = __, by = 25), n2 = seq(from = __, to = __, by = 50), sig.level = __, power = NULL, alternative = &quot;two.sided&quot;) You should get a power of 37.7% for the smallest n, 64.5% for n1=50/n2=100, and 81.4% for the largest n scenario. 18.11 add chi square 18.12 add correlation test 18.13 add anova 18.14 add linear model 18.15 add note on guessing effect sizes - cohen small, medium, large 18.16 Explore More You can explore other examples here in the official {pwr} vignette. Power calculations for more complex endpoints and study designs can be found in R packages listed in the Clinical Trials CRAN Task View here. Consider the packages {samplesize}, {TrialSize}, {clusterpower}, {CRTsize}, {cosa}, {PowerTOST}, {PowerUpR}, and which may be relevant for your particular analysis. Two other helpful references are books: Cohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences (2nd ed.). LEA. Ryan, T.P. (2013) Sample Size Determination and Power. Wiley. "],["randomization-for-clinical-trials-with-r.html", "Chapter 19 Randomization for Clinical Trials with R 19.1 Printing these on Cards 19.2 Now, try this yourself 19.3 Now Freestyle", " Chapter 19 Randomization for Clinical Trials with R There are a number of packages for doing key functions for clinical trials in R. You can find many of these on the CRAN Task View for Clinical Trials, at https://cran.r-project.org/web/views/ClinicalTrials.html. This is a curated list of packages that anyone might find useful in designing, monitoring, or analyzing clinical trials, and is often a good place to start in looking for packages that might be relevant for clinical trials. If you use Ctrl-F to search the web page for “rand”, several packages address randomization, including blockrand randomizeR pwr experiment clusterPower CRTSize cosa PowerupR Several of these are specifically for more complex designs, including cluster and multilevel randomization (clusterPower, cosa, CRTSize). For today, we will focus on the straightforward randomization packages including {blockrand} and {randomizer}. The {blockrand} package creates randomizations for clinical trials with can include stratified enrollment and permuted block randomization, and can produce a PDF file of randomization cards. Let’s start with an example in {blockrand}. Details on the package can be found at https://cran.r-project.org/web/packages/blockrand/blockrand.pdf or by running help(blockrand) in your Console. You want to randomize 180 inpatients with severe ulcerative colitis to one of 3 arms: corticosteroids alone (control), corticosteroids + tofacitinib, or corticosteroids + upadacitinib. You want to stratify the participants by (1) prior biologic failure and (2) Albumin level above or below 3.0. To be prepared for dropouts and imbalanced enrollment, you want to have a randomization list with at least 60 assignments available for each arm and stratum. To avoid a recognizable pattern in the randomization, you want to have a permuted block design with blocks of sizes 3, 6, and 9. Below, you will see how to do this for the biologic failure - low albumin stratum. bfla &lt;- blockrand(n = 60, num.levels = 3, # three treatments levels = c(&quot;CS&quot;, &quot;CS/Tofa&quot;, &quot;CS/Upa&quot;), # arm names stratum = &quot;Bfail.LowAlb&quot;, # stratum name id.prefix = &quot;BfLA&quot;, # stratum abbrev block.sizes = c(1,2,3), # times arms = 3,6,9 block.prefix = &quot;BfLA&quot;) # stratum abbrev bfla ## id stratum block.id block.size treatment ## 1 BfLA01 Bfail.LowAlb BfLA01 3 CS ## 2 BfLA02 Bfail.LowAlb BfLA01 3 CS/Upa ## 3 BfLA03 Bfail.LowAlb BfLA01 3 CS/Tofa ## 4 BfLA04 Bfail.LowAlb BfLA02 3 CS/Upa ## 5 BfLA05 Bfail.LowAlb BfLA02 3 CS ## 6 BfLA06 Bfail.LowAlb BfLA02 3 CS/Tofa ## 7 BfLA07 Bfail.LowAlb BfLA03 9 CS/Tofa ## 8 BfLA08 Bfail.LowAlb BfLA03 9 CS/Tofa ## 9 BfLA09 Bfail.LowAlb BfLA03 9 CS ## 10 BfLA10 Bfail.LowAlb BfLA03 9 CS ## 11 BfLA11 Bfail.LowAlb BfLA03 9 CS/Upa ## 12 BfLA12 Bfail.LowAlb BfLA03 9 CS/Tofa ## 13 BfLA13 Bfail.LowAlb BfLA03 9 CS ## 14 BfLA14 Bfail.LowAlb BfLA03 9 CS/Upa ## 15 BfLA15 Bfail.LowAlb BfLA03 9 CS/Upa ## 16 BfLA16 Bfail.LowAlb BfLA04 6 CS/Upa ## 17 BfLA17 Bfail.LowAlb BfLA04 6 CS ## 18 BfLA18 Bfail.LowAlb BfLA04 6 CS/Upa ## 19 BfLA19 Bfail.LowAlb BfLA04 6 CS ## 20 BfLA20 Bfail.LowAlb BfLA04 6 CS/Tofa ## 21 BfLA21 Bfail.LowAlb BfLA04 6 CS/Tofa ## 22 BfLA22 Bfail.LowAlb BfLA05 6 CS/Upa ## 23 BfLA23 Bfail.LowAlb BfLA05 6 CS/Tofa ## 24 BfLA24 Bfail.LowAlb BfLA05 6 CS ## 25 BfLA25 Bfail.LowAlb BfLA05 6 CS/Upa ## 26 BfLA26 Bfail.LowAlb BfLA05 6 CS ## 27 BfLA27 Bfail.LowAlb BfLA05 6 CS/Tofa ## 28 BfLA28 Bfail.LowAlb BfLA06 6 CS ## 29 BfLA29 Bfail.LowAlb BfLA06 6 CS/Tofa ## 30 BfLA30 Bfail.LowAlb BfLA06 6 CS/Upa ## 31 BfLA31 Bfail.LowAlb BfLA06 6 CS/Tofa ## 32 BfLA32 Bfail.LowAlb BfLA06 6 CS ## 33 BfLA33 Bfail.LowAlb BfLA06 6 CS/Upa ## 34 BfLA34 Bfail.LowAlb BfLA07 6 CS/Tofa ## 35 BfLA35 Bfail.LowAlb BfLA07 6 CS/Upa ## 36 BfLA36 Bfail.LowAlb BfLA07 6 CS ## 37 BfLA37 Bfail.LowAlb BfLA07 6 CS/Tofa ## 38 BfLA38 Bfail.LowAlb BfLA07 6 CS/Upa ## 39 BfLA39 Bfail.LowAlb BfLA07 6 CS ## 40 BfLA40 Bfail.LowAlb BfLA08 9 CS ## 41 BfLA41 Bfail.LowAlb BfLA08 9 CS ## 42 BfLA42 Bfail.LowAlb BfLA08 9 CS ## 43 BfLA43 Bfail.LowAlb BfLA08 9 CS/Upa ## 44 BfLA44 Bfail.LowAlb BfLA08 9 CS/Tofa ## 45 BfLA45 Bfail.LowAlb BfLA08 9 CS/Tofa ## 46 BfLA46 Bfail.LowAlb BfLA08 9 CS/Tofa ## 47 BfLA47 Bfail.LowAlb BfLA08 9 CS/Upa ## 48 BfLA48 Bfail.LowAlb BfLA08 9 CS/Upa ## 49 BfLA49 Bfail.LowAlb BfLA09 9 CS/Tofa ## 50 BfLA50 Bfail.LowAlb BfLA09 9 CS/Tofa ## 51 BfLA51 Bfail.LowAlb BfLA09 9 CS ## 52 BfLA52 Bfail.LowAlb BfLA09 9 CS/Tofa ## 53 BfLA53 Bfail.LowAlb BfLA09 9 CS ## 54 BfLA54 Bfail.LowAlb BfLA09 9 CS/Upa ## 55 BfLA55 Bfail.LowAlb BfLA09 9 CS/Upa ## 56 BfLA56 Bfail.LowAlb BfLA09 9 CS/Upa ## 57 BfLA57 Bfail.LowAlb BfLA09 9 CS ## 58 BfLA58 Bfail.LowAlb BfLA10 9 CS ## 59 BfLA59 Bfail.LowAlb BfLA10 9 CS/Upa ## 60 BfLA60 Bfail.LowAlb BfLA10 9 CS/Tofa ## 61 BfLA61 Bfail.LowAlb BfLA10 9 CS/Upa ## 62 BfLA62 Bfail.LowAlb BfLA10 9 CS/Tofa ## 63 BfLA63 Bfail.LowAlb BfLA10 9 CS ## 64 BfLA64 Bfail.LowAlb BfLA10 9 CS/Upa ## 65 BfLA65 Bfail.LowAlb BfLA10 9 CS ## 66 BfLA66 Bfail.LowAlb BfLA10 9 CS/Tofa You can see the id for each participant, their stratum, the block.id for their permuted block, the block.size, and their assigned treatment. You can imagine this as a randomization list, or as assignments that you could print out on cards and seal in security envelopes for the time of randomization. Of course, this is only one of our four strata. We should do the same for the 3 other strata. bfha &lt;- blockrand(n = 60, num.levels = 3, # three treatments levels = c(&quot;CS&quot;, &quot;CS/Tofa&quot;, &quot;CS/Upa&quot;), # arm names stratum = &quot;Bfail.HiAlb&quot;, # stratum name id.prefix = &quot;BfHA&quot;, # stratum abbrev block.sizes = c(1,2,3), # times arms = 3,6,9 block.prefix = &quot;BfHA&quot;) # stratum abbrev bfha ## id stratum block.id block.size treatment ## 1 BfHA01 Bfail.HiAlb BfHA01 6 CS/Upa ## 2 BfHA02 Bfail.HiAlb BfHA01 6 CS/Tofa ## 3 BfHA03 Bfail.HiAlb BfHA01 6 CS ## 4 BfHA04 Bfail.HiAlb BfHA01 6 CS ## 5 BfHA05 Bfail.HiAlb BfHA01 6 CS/Upa ## 6 BfHA06 Bfail.HiAlb BfHA01 6 CS/Tofa ## 7 BfHA07 Bfail.HiAlb BfHA02 3 CS/Tofa ## 8 BfHA08 Bfail.HiAlb BfHA02 3 CS ## 9 BfHA09 Bfail.HiAlb BfHA02 3 CS/Upa ## 10 BfHA10 Bfail.HiAlb BfHA03 9 CS ## 11 BfHA11 Bfail.HiAlb BfHA03 9 CS/Upa ## 12 BfHA12 Bfail.HiAlb BfHA03 9 CS/Upa ## 13 BfHA13 Bfail.HiAlb BfHA03 9 CS/Tofa ## 14 BfHA14 Bfail.HiAlb BfHA03 9 CS/Upa ## 15 BfHA15 Bfail.HiAlb BfHA03 9 CS/Tofa ## 16 BfHA16 Bfail.HiAlb BfHA03 9 CS/Tofa ## 17 BfHA17 Bfail.HiAlb BfHA03 9 CS ## 18 BfHA18 Bfail.HiAlb BfHA03 9 CS ## 19 BfHA19 Bfail.HiAlb BfHA04 6 CS/Tofa ## 20 BfHA20 Bfail.HiAlb BfHA04 6 CS/Upa ## 21 BfHA21 Bfail.HiAlb BfHA04 6 CS ## 22 BfHA22 Bfail.HiAlb BfHA04 6 CS/Upa ## 23 BfHA23 Bfail.HiAlb BfHA04 6 CS/Tofa ## 24 BfHA24 Bfail.HiAlb BfHA04 6 CS ## 25 BfHA25 Bfail.HiAlb BfHA05 6 CS/Tofa ## 26 BfHA26 Bfail.HiAlb BfHA05 6 CS ## 27 BfHA27 Bfail.HiAlb BfHA05 6 CS/Upa ## 28 BfHA28 Bfail.HiAlb BfHA05 6 CS/Tofa ## 29 BfHA29 Bfail.HiAlb BfHA05 6 CS ## 30 BfHA30 Bfail.HiAlb BfHA05 6 CS/Upa ## 31 BfHA31 Bfail.HiAlb BfHA06 9 CS/Upa ## 32 BfHA32 Bfail.HiAlb BfHA06 9 CS/Tofa ## 33 BfHA33 Bfail.HiAlb BfHA06 9 CS ## 34 BfHA34 Bfail.HiAlb BfHA06 9 CS/Tofa ## 35 BfHA35 Bfail.HiAlb BfHA06 9 CS/Tofa ## 36 BfHA36 Bfail.HiAlb BfHA06 9 CS ## 37 BfHA37 Bfail.HiAlb BfHA06 9 CS ## 38 BfHA38 Bfail.HiAlb BfHA06 9 CS/Upa ## 39 BfHA39 Bfail.HiAlb BfHA06 9 CS/Upa ## 40 BfHA40 Bfail.HiAlb BfHA07 9 CS ## 41 BfHA41 Bfail.HiAlb BfHA07 9 CS ## 42 BfHA42 Bfail.HiAlb BfHA07 9 CS/Upa ## 43 BfHA43 Bfail.HiAlb BfHA07 9 CS ## 44 BfHA44 Bfail.HiAlb BfHA07 9 CS/Upa ## 45 BfHA45 Bfail.HiAlb BfHA07 9 CS/Tofa ## 46 BfHA46 Bfail.HiAlb BfHA07 9 CS/Tofa ## 47 BfHA47 Bfail.HiAlb BfHA07 9 CS/Tofa ## 48 BfHA48 Bfail.HiAlb BfHA07 9 CS/Upa ## 49 BfHA49 Bfail.HiAlb BfHA08 3 CS/Upa ## 50 BfHA50 Bfail.HiAlb BfHA08 3 CS ## 51 BfHA51 Bfail.HiAlb BfHA08 3 CS/Tofa ## 52 BfHA52 Bfail.HiAlb BfHA09 6 CS/Tofa ## 53 BfHA53 Bfail.HiAlb BfHA09 6 CS ## 54 BfHA54 Bfail.HiAlb BfHA09 6 CS ## 55 BfHA55 Bfail.HiAlb BfHA09 6 CS/Tofa ## 56 BfHA56 Bfail.HiAlb BfHA09 6 CS/Upa ## 57 BfHA57 Bfail.HiAlb BfHA09 6 CS/Upa ## 58 BfHA58 Bfail.HiAlb BfHA10 6 CS/Tofa ## 59 BfHA59 Bfail.HiAlb BfHA10 6 CS/Upa ## 60 BfHA60 Bfail.HiAlb BfHA10 6 CS ## 61 BfHA61 Bfail.HiAlb BfHA10 6 CS ## 62 BfHA62 Bfail.HiAlb BfHA10 6 CS/Tofa ## 63 BfHA63 Bfail.HiAlb BfHA10 6 CS/Upa bnha &lt;- blockrand(n = 60, num.levels = 3, levels = c(&quot;CS&quot;, &quot;CS/Tofa&quot;, &quot;CS/Upa&quot;), stratum = &quot;Bnaive.HiAlb&quot;, id.prefix = &quot;BnHA&quot;, block.sizes = c(1,2,3, 4), block.prefix = &quot;BnHA&quot;) bnha ## id stratum block.id block.size treatment ## 1 BnHA01 Bnaive.HiAlb BnHA1 3 CS/Upa ## 2 BnHA02 Bnaive.HiAlb BnHA1 3 CS ## 3 BnHA03 Bnaive.HiAlb BnHA1 3 CS/Tofa ## 4 BnHA04 Bnaive.HiAlb BnHA2 3 CS/Upa ## 5 BnHA05 Bnaive.HiAlb BnHA2 3 CS ## 6 BnHA06 Bnaive.HiAlb BnHA2 3 CS/Tofa ## 7 BnHA07 Bnaive.HiAlb BnHA3 12 CS ## 8 BnHA08 Bnaive.HiAlb BnHA3 12 CS ## 9 BnHA09 Bnaive.HiAlb BnHA3 12 CS/Tofa ## 10 BnHA10 Bnaive.HiAlb BnHA3 12 CS/Upa ## 11 BnHA11 Bnaive.HiAlb BnHA3 12 CS/Tofa ## 12 BnHA12 Bnaive.HiAlb BnHA3 12 CS ## 13 BnHA13 Bnaive.HiAlb BnHA3 12 CS/Tofa ## 14 BnHA14 Bnaive.HiAlb BnHA3 12 CS/Upa ## 15 BnHA15 Bnaive.HiAlb BnHA3 12 CS/Tofa ## 16 BnHA16 Bnaive.HiAlb BnHA3 12 CS/Upa ## 17 BnHA17 Bnaive.HiAlb BnHA3 12 CS ## 18 BnHA18 Bnaive.HiAlb BnHA3 12 CS/Upa ## 19 BnHA19 Bnaive.HiAlb BnHA4 12 CS/Upa ## 20 BnHA20 Bnaive.HiAlb BnHA4 12 CS/Tofa ## 21 BnHA21 Bnaive.HiAlb BnHA4 12 CS/Tofa ## 22 BnHA22 Bnaive.HiAlb BnHA4 12 CS/Upa ## 23 BnHA23 Bnaive.HiAlb BnHA4 12 CS ## 24 BnHA24 Bnaive.HiAlb BnHA4 12 CS/Upa ## 25 BnHA25 Bnaive.HiAlb BnHA4 12 CS ## 26 BnHA26 Bnaive.HiAlb BnHA4 12 CS ## 27 BnHA27 Bnaive.HiAlb BnHA4 12 CS/Tofa ## 28 BnHA28 Bnaive.HiAlb BnHA4 12 CS ## 29 BnHA29 Bnaive.HiAlb BnHA4 12 CS/Upa ## 30 BnHA30 Bnaive.HiAlb BnHA4 12 CS/Tofa ## 31 BnHA31 Bnaive.HiAlb BnHA5 12 CS/Upa ## 32 BnHA32 Bnaive.HiAlb BnHA5 12 CS ## 33 BnHA33 Bnaive.HiAlb BnHA5 12 CS/Upa ## 34 BnHA34 Bnaive.HiAlb BnHA5 12 CS/Upa ## 35 BnHA35 Bnaive.HiAlb BnHA5 12 CS ## 36 BnHA36 Bnaive.HiAlb BnHA5 12 CS ## 37 BnHA37 Bnaive.HiAlb BnHA5 12 CS/Tofa ## 38 BnHA38 Bnaive.HiAlb BnHA5 12 CS ## 39 BnHA39 Bnaive.HiAlb BnHA5 12 CS/Upa ## 40 BnHA40 Bnaive.HiAlb BnHA5 12 CS/Tofa ## 41 BnHA41 Bnaive.HiAlb BnHA5 12 CS/Tofa ## 42 BnHA42 Bnaive.HiAlb BnHA5 12 CS/Tofa ## 43 BnHA43 Bnaive.HiAlb BnHA6 6 CS ## 44 BnHA44 Bnaive.HiAlb BnHA6 6 CS/Upa ## 45 BnHA45 Bnaive.HiAlb BnHA6 6 CS/Tofa ## 46 BnHA46 Bnaive.HiAlb BnHA6 6 CS ## 47 BnHA47 Bnaive.HiAlb BnHA6 6 CS/Tofa ## 48 BnHA48 Bnaive.HiAlb BnHA6 6 CS/Upa ## 49 BnHA49 Bnaive.HiAlb BnHA7 12 CS/Tofa ## 50 BnHA50 Bnaive.HiAlb BnHA7 12 CS/Tofa ## 51 BnHA51 Bnaive.HiAlb BnHA7 12 CS/Tofa ## 52 BnHA52 Bnaive.HiAlb BnHA7 12 CS ## 53 BnHA53 Bnaive.HiAlb BnHA7 12 CS/Upa ## 54 BnHA54 Bnaive.HiAlb BnHA7 12 CS/Tofa ## 55 BnHA55 Bnaive.HiAlb BnHA7 12 CS/Upa ## 56 BnHA56 Bnaive.HiAlb BnHA7 12 CS/Upa ## 57 BnHA57 Bnaive.HiAlb BnHA7 12 CS ## 58 BnHA58 Bnaive.HiAlb BnHA7 12 CS ## 59 BnHA59 Bnaive.HiAlb BnHA7 12 CS ## 60 BnHA60 Bnaive.HiAlb BnHA7 12 CS/Upa bnla &lt;- blockrand(n = 60, num.levels = 3, levels = c(&quot;CS&quot;, &quot;CS/Tofa&quot;, &quot;CS/Upa&quot;), stratum = &quot;Bnaive.LoAlb&quot;, id.prefix = &quot;BnLA&quot;, block.sizes = c(1,2,3), block.prefix = &quot;BnLA&quot;) bnla ## id stratum block.id block.size treatment ## 1 BnLA01 Bnaive.LoAlb BnLA01 9 CS/Tofa ## 2 BnLA02 Bnaive.LoAlb BnLA01 9 CS/Tofa ## 3 BnLA03 Bnaive.LoAlb BnLA01 9 CS/Upa ## 4 BnLA04 Bnaive.LoAlb BnLA01 9 CS/Upa ## 5 BnLA05 Bnaive.LoAlb BnLA01 9 CS/Upa ## 6 BnLA06 Bnaive.LoAlb BnLA01 9 CS ## 7 BnLA07 Bnaive.LoAlb BnLA01 9 CS ## 8 BnLA08 Bnaive.LoAlb BnLA01 9 CS/Tofa ## 9 BnLA09 Bnaive.LoAlb BnLA01 9 CS ## 10 BnLA10 Bnaive.LoAlb BnLA02 9 CS/Upa ## 11 BnLA11 Bnaive.LoAlb BnLA02 9 CS/Tofa ## 12 BnLA12 Bnaive.LoAlb BnLA02 9 CS ## 13 BnLA13 Bnaive.LoAlb BnLA02 9 CS ## 14 BnLA14 Bnaive.LoAlb BnLA02 9 CS ## 15 BnLA15 Bnaive.LoAlb BnLA02 9 CS/Tofa ## 16 BnLA16 Bnaive.LoAlb BnLA02 9 CS/Tofa ## 17 BnLA17 Bnaive.LoAlb BnLA02 9 CS/Upa ## 18 BnLA18 Bnaive.LoAlb BnLA02 9 CS/Upa ## 19 BnLA19 Bnaive.LoAlb BnLA03 3 CS/Upa ## 20 BnLA20 Bnaive.LoAlb BnLA03 3 CS/Tofa ## 21 BnLA21 Bnaive.LoAlb BnLA03 3 CS ## 22 BnLA22 Bnaive.LoAlb BnLA04 3 CS ## 23 BnLA23 Bnaive.LoAlb BnLA04 3 CS/Upa ## 24 BnLA24 Bnaive.LoAlb BnLA04 3 CS/Tofa ## 25 BnLA25 Bnaive.LoAlb BnLA05 3 CS/Upa ## 26 BnLA26 Bnaive.LoAlb BnLA05 3 CS ## 27 BnLA27 Bnaive.LoAlb BnLA05 3 CS/Tofa ## 28 BnLA28 Bnaive.LoAlb BnLA06 3 CS/Upa ## 29 BnLA29 Bnaive.LoAlb BnLA06 3 CS/Tofa ## 30 BnLA30 Bnaive.LoAlb BnLA06 3 CS ## 31 BnLA31 Bnaive.LoAlb BnLA07 6 CS/Tofa ## 32 BnLA32 Bnaive.LoAlb BnLA07 6 CS ## 33 BnLA33 Bnaive.LoAlb BnLA07 6 CS/Upa ## 34 BnLA34 Bnaive.LoAlb BnLA07 6 CS/Tofa ## 35 BnLA35 Bnaive.LoAlb BnLA07 6 CS ## 36 BnLA36 Bnaive.LoAlb BnLA07 6 CS/Upa ## 37 BnLA37 Bnaive.LoAlb BnLA08 6 CS/Tofa ## 38 BnLA38 Bnaive.LoAlb BnLA08 6 CS ## 39 BnLA39 Bnaive.LoAlb BnLA08 6 CS ## 40 BnLA40 Bnaive.LoAlb BnLA08 6 CS/Upa ## 41 BnLA41 Bnaive.LoAlb BnLA08 6 CS/Upa ## 42 BnLA42 Bnaive.LoAlb BnLA08 6 CS/Tofa ## 43 BnLA43 Bnaive.LoAlb BnLA09 9 CS/Tofa ## 44 BnLA44 Bnaive.LoAlb BnLA09 9 CS ## 45 BnLA45 Bnaive.LoAlb BnLA09 9 CS ## 46 BnLA46 Bnaive.LoAlb BnLA09 9 CS/Tofa ## 47 BnLA47 Bnaive.LoAlb BnLA09 9 CS/Upa ## 48 BnLA48 Bnaive.LoAlb BnLA09 9 CS/Tofa ## 49 BnLA49 Bnaive.LoAlb BnLA09 9 CS/Upa ## 50 BnLA50 Bnaive.LoAlb BnLA09 9 CS ## 51 BnLA51 Bnaive.LoAlb BnLA09 9 CS/Upa ## 52 BnLA52 Bnaive.LoAlb BnLA10 3 CS/Tofa ## 53 BnLA53 Bnaive.LoAlb BnLA10 3 CS/Upa ## 54 BnLA54 Bnaive.LoAlb BnLA10 3 CS ## 55 BnLA55 Bnaive.LoAlb BnLA11 6 CS/Tofa ## 56 BnLA56 Bnaive.LoAlb BnLA11 6 CS ## 57 BnLA57 Bnaive.LoAlb BnLA11 6 CS/Upa ## 58 BnLA58 Bnaive.LoAlb BnLA11 6 CS/Upa ## 59 BnLA59 Bnaive.LoAlb BnLA11 6 CS ## 60 BnLA60 Bnaive.LoAlb BnLA11 6 CS/Tofa 19.1 Printing these on Cards Ideally, you will print out each randomization on a card, and seal it in a security envelope, with the outside of the envelope labeled with the id. You can do this with the plotblockrand() function. This function creates a pdf file of randomization cards for printing. These are designed so that the middle text will show in a standard letter sized envelope with a window, and the top text (the assignment) can be folded over to increase security (against anyone trying to peek through the security envelope to guess the assignment). uc_study &lt;- bind_rows(bfha, bfla, bnha, bnla) # bind together the four strata into one dataframe blockrand::plotblockrand(uc_study, # input dataframe file = &quot;uc_study.pdf&quot;, # output pdf # top hidden text with assignment top=list(text=c(&#39;My Study&#39;,&#39;Patient: %ID%&#39;,&#39;Treatment: %TREAT%&#39;), col=c(&#39;black&#39;,&#39;black&#39;,&#39;red&#39;),font=c(1,1,4)), # middle text to show through window of # 10 envelope middle=list(text=c(&quot;My Study&quot;,&quot;Stratum: %STRAT%&quot;,&quot;Patient: %ID%&quot;), col=c(&#39;black&#39;,&#39;blue&#39;,&#39;orange&#39;),font=c(1,2,3)), # bottom text- any instructions to study coordinator bottom=&quot;Call 123-4567 to report patient entry&quot;, cut.marks=TRUE) # add cut marks - 4 per page Open up the file uc_study.pdf in your Files tab to see the output pdf file, with assorted fonts and colors. Just for fun, change (and then re-run) the text “My Study” to something more interesting change “Patient” to “Participant” change “Treatment” to “Arm” or “Assignment” change some of the colors to standard R colors change some of the fonts (within 1-4) Sometimes with equal blocks, and clear treatment effects or side effects, nurses or study coordinators can guess the randomization pattern. If you want to get fancy, and make it even harder to guess treatment assignments, you can add one of the unequal blocks options, to make it hard to find patterns in treatment or in side effects. Set uneq.beg = TRUE for an unequal block in the beginning, or uneq.mid = TRUE for an unequal block in the middle. 19.2 Now, try this yourself You want to randomize 80 outpatients with Crohn’s disease to one of 8 arms, as part of a 2^3 factorial design to increase patient activation. These arms involve using (A,B, C) or not using (a,b,c) 3 intervention components. The 8 arms then become: abc abC aBc aBC Abc AbC ABc ABC Then we want to stratify the participants by baseline PAM score (a measure of patient activation) with levels of low, medium, and high PAM. To be prepared for dropouts and imbalanced enrollment, you want to have a randomization list with at least 32 assignments available for each arm and stratum. To avoid a recognizable pattern in the randomization, you want to have a permuted block design with blocks of sizes 8 and 16. You can hover over top right corner of the code chunk below, and a copy icon will appear - click this to copy the code to your clipboard. You can then paste it into your local version of RStudio, edit it, and run it. In the code block below, fill in the blanks to complete the code to make a dataframe for the low_pam stratum. low_pam &lt;- blockrand(n = __, num.levels = __, #eight treatments levels = c(&quot;abc&quot;, &quot;abC&quot;, &quot;aBc&quot;, &quot;aBC&quot;, &quot;Abc&quot;, &quot;AbC&quot;, &quot;ABc&quot;, &quot;ABC&quot;), # arm names stratum = &quot;__&quot;, # stratum name id.prefix = &quot;lp&quot;, # stratum abbrev block.sizes = c(1,2,3), # times arms block.prefix = &quot;LP&quot;) # stratum abbrev low_pam Now that you have one stratum sorted, edit the code block below to create the med_pam and high_pam strata. med_pam &lt;- blockrand(n = __, num.levels = __, #eight treatments levels = c(&quot;abc&quot;, &quot;abC&quot;, &quot;aBc&quot;, &quot;aBC&quot;, &quot;Abc&quot;, &quot;AbC&quot;, &quot;ABc&quot;, &quot;ABC&quot;), # arm names stratum = &quot;__&quot;, # stratum name id.prefix = &quot;__&quot;, # stratum abbrev block.sizes = c(__), # times arms block.prefix = &quot;__&quot;) # stratum abbrev med_pam high_pam &lt;- blockrand(n = __, num.levels = __, #eight treatments levels = c(&quot;abc&quot;, &quot;abC&quot;, &quot;aBc&quot;, &quot;aBC&quot;, &quot;Abc&quot;, &quot;AbC&quot;, &quot;ABc&quot;, &quot;ABC&quot;), # arm names stratum = &quot;__&quot;, # stratum name id.prefix = &quot;__&quot;, # stratum abbrev block.sizes = c(__), # times arms block.prefix = &quot;__&quot;) # stratum abbrev high_pam Great! Now try to bind these 3 strata into one dataframe print these as cards to a pdf file Edit the code chunk below to produce the pdf file cd_study &lt;- bind_rows(__,__,__) # bind together the 3 strata into one dataframe blockrand::plotblockrand(__, # input dataframe file = &quot;cd_study.pdf&quot;, # output pdf # top hidden text with assignment top=list(text=c(&#39;CD Study&#39;,&#39;Patient: %ID%&#39;,&#39;Treatment: %__%&#39;), col=c(&#39;orange&#39;,&#39;blue&#39;,&#39;red&#39;),font=c(1,1,4)), # middle text to show through window of # 10 envelope middle=list(text=c(&quot;CD Study&quot;,&quot;Stratum: %STRAT%&quot;,&quot;Patient: %__%&quot;), col=c(&#39;black&#39;,&#39;red&#39;,&#39;cadetblue&#39;),font=c(1,2,3)), # bottom text- any instructions to study coordinator bottom=&quot;Call 123-4567 to report patient entry&quot;, cut.marks=TRUE) # add cut marks - 4 per page 19.3 Now Freestyle Your turn. Create randomization tables and a pdf file of cards for a study of 2 microbiome interventions to reduce the formation of colon adenomas. your 3 study arms will be - placebo, Streptococcus thermophilus, and S.thermo plus lactose (a preferred sugar for S.t, making this arm a synbiotic, while arm 2 is a probiotic) - aka 3 arms called: pbo, probiotic, synbiotic. Your stratifications will be by prior polyps being MSI_hi or MSI_lo (for microsatellite instability mutations) BMI above or below 35. BMI_hi, BMI_low block sizes of 4,8,12,16 160 per arm Edit the code block below for the first stratum mhbh &lt;- blockrand(n = __, # treatment arms num.levels = __, # of treatments levels = c(&quot;placebo&quot;, &quot;probiotic&quot;, &quot;synbiotic&quot;), # arm names stratum = &quot;__,__&quot;, # stratum name id.prefix = &quot;mhbh&quot;, # stratum abbrev block.sizes = c(__,__,__,__), # times arms block.prefix = &quot;__&quot;) # stratum abbrev mhbh Edit the code block below for the remaining strata mhbl &lt;- blockrand(n = __, # treatment arms num.levels = 3, # of treatments levels = c(&quot;placebo&quot;, &quot;probiotic&quot;, &quot;__&quot;), # arm names stratum = &quot;msi_hi.bmi_lo&quot;, # stratum name id.prefix = &quot;__&quot;, # stratum abbrev block.sizes = c(1,2,__,__), # times arms block.prefix = &quot;MHBL&quot;) # stratum abbrev mhbl mlbl &lt;- blockrand(n = 160, # treatment arms num.levels = __, # of treatments levels = c(&quot;placebo&quot;, &quot;__&quot;, &quot;synbiotic&quot;), # arm names stratum = &quot;__&quot;, # stratum name id.prefix = &quot;mlbl&quot;, # stratum abbrev block.sizes = c(__,__,3,4), # times arms block.prefix = &quot;MLBL&quot;) # stratum abbrev mlbl mlbh &lt;- blockrand(n = __, # treatment arms num.levels = 3, # of treatments levels = c(&quot;__&quot;, &quot;probiotic&quot;, &quot;synbiotic&quot;), # arm names stratum = &quot;msi_lo.bmi_hi&quot;, # stratum name id.prefix = &quot;__&quot;, # stratum abbrev block.sizes = c(1,2,3,4), # times arms block.prefix = &quot;MLBH&quot;) # stratum abbrev mlbh Edit the code block below to bind the strata together and print the cards adenoma_study &lt;- bind_rows(mlbl, mlbh, mhbh, mhbl) # bind together the strata into one dataframe blockrand::plotblockrand(__, # input dataframe file = &quot;adenoma_cards.pdf&quot;, # output pdf # top hidden text with assignment top=list(text=c(&#39;Adenoma Study&#39;,&#39;Patient: %__%&#39;,&#39;Treatment: %TREAT%&#39;), col=c(&#39;orange&#39;,&#39;blue&#39;,&#39;red&#39;),font=c(1,1,4)), # middle text to show through window of # 10 envelope middle=list(text=c(&quot;Adenoma Study&quot;,&quot;Stratum: %__%&quot;,&quot;Patient: %ID%&quot;), col=c(&#39;black&#39;,&#39;red&#39;,&#39;cadetblue&#39;),font=c(1,2,3)), # bottom text- any instructions to study coordinator bottom=&quot;Call 123-4567 to report patient entry. \\nInstruct participant to avoid antibiotics and stop aspirin&quot;, cut.marks=TRUE) # add cut marks - 4 per page "],["univariate-ggplots-to-visualize-distributions.html", "Chapter 20 Univariate ggplots to Visualize Distributions 20.1 Histograms 20.2 Density Plots 20.3 Comparing Distributions Across Categories 20.4 Boxplots 20.5 Violin Plots 20.6 Ridgeline Plots", " Chapter 20 Univariate ggplots to Visualize Distributions When you first encounter a new dataset, it is often helpful to start with Data Exploration and Validation (DEV) . Note that DEV is different from EDA (Exploratory Data Analysis), which involves rummaging through your data (often with plots) and generating new hypotheses, performing multiple tests, and essentially data mining. DEV has specific goals, which include identifying variables with problematic values (NA, outliers), unexpected distributions (sometimes bimodal), and less-than-helpful variable names or data types. In this chapter, we will look at investigating single continuous variables, looking for outliers, multi-modal distributions, and making comparisons across categories. 20.1 Histograms One of the most helpful ways to get started is to explore your continuous variables with the humble histogram or dotplot. These geoms in ggplot2 allow you to see a distribution of a single variable, and are a good way to get started with ggplot. Let’s start by looking at the distribution of the number of colon polyps found in participants in a clinical trial. You just need to map a variable (in this case, number12m) to the x aesthetic, and you are good to go. medicaldata::polyps %&gt;% ggplot() + aes(x = number12m) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). This gives us a very basic histogram, with some participants with zero polyps, but many with much larger numbers. We also learn that geom_histogram is a bit grumpy, and complains that we have not picked a binwidth (or a number of bins). Since the distribution goes out to 60+ polyps, let’s pick a binwidth of 5 (or about 13 bins). polyps %&gt;% ggplot() + aes(x = number12m) + geom_histogram(binwidth = 5) ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). Even with 13 bins, most bins contain only one or two participants. It would be more helpful to visualize each participant as a single dot, which you can do with a dotplot (geom_dotplot). medicaldata::polyps %&gt;% ggplot() + aes(x = number12m, fill = treatment) + geom_dotplot(binwidth = 1) + scale_y_continuous(NULL, breaks = NULL) + # Make this ratio = (tallest column * binwidth * 1.5) coord_fixed(ratio = 6) + labs(title = &quot;Distribution of Participants binned by number of Polyps&quot;, subtitle = &quot;Colored by Treatment&quot;) + theme(legend.position = &quot;top&quot;) ## Warning: Removed 2 rows containing non-finite values ## (stat_bindot). One of the nice things about geom_dotplot() is that it represents each participant as a dot, which is closer to reality than columns which look like they are portraying values, rather than counts. The y-axis is less helpful. It is showing the proportion of the total sample, which is sort of helpful, but this leaves a lot of empty space with no data most of the time. 20.1.1 Comparisons of Distributions with Histograms When you have a continuous variable that could vary across several categories of a categorical variable, you may want to compare the distributions across the categories. You can compare a small number of categories with histograms, usually 5 categories or less is manageable. There are several ways to make this kind of comparison. You can set the fill aesthetic to the categorical variable. In this case, we will use the treatment variable. This is OK, as you can see the sulindac-treated patients are shifted to the left, but it is not great, as the counts are stacked, and this can make comparisons hard. Note that color = outline of bars color, as opposed to fill. medicaldata::polyps %&gt;% ggplot() + aes(x = number12m, fill = treatment) + geom_histogram(bins=15, color = &quot;purple&quot;) ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). A mirror histogram can work well for 2 categories. You filter the data by value within the geom, and set the y value to ..density.. for one category value, and to negative -..density.. for the other category value, as seen below. The ..density.. variable is a value ggplot calculates in the background with a stat function. medicaldata::polyps %&gt;% ggplot(aes(x = number12m)) + geom_histogram(fill = &quot;red&quot;, aes(y = ..density..), data = . %&gt;% filter(treatment == &#39;placebo&#39;), bins = 15) + geom_label(aes(x = 55, y = 0.03, label = &quot;Placebo&quot;), color = &quot;red&quot;) + geom_histogram(fill = &quot;#404080&quot;, aes(y = -..density..), data = . %&gt;% filter(treatment == &#39;sulindac&#39;), bins = 15) + geom_label(aes(x = 20, y = -0.03, label = &quot;Sulindac&quot;), color = &quot;#404080&quot;) ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). You can do small multiples with facet_wrap medicaldata::polyps %&gt;% ggplot() + aes(x = number12m, fill = treatment) + geom_histogram(bins =15) + facet_grid(. ~ treatment) ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). Note for facet_wrap and facet_grid, the formula notation or the arguments is y \\~ x, so that by putting treatment after the tilde, the treatments are compared on the x axis, or side by side. If you put the treatment categorical value in the first (y) position, they would be shown as top and bottom. medicaldata::polyps %&gt;% ggplot() + aes(x = number12m, fill = treatment) + geom_histogram(bins =15) + facet_grid(treatment ~ .) ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). While the viewer can generally make the comparisons with 2 categories, it can get more complicated with increasing numbers of categories. 20.1.2 Histograms and Categories You can also look at distributions of different categories, by color: mockstudy %&gt;% ggplot() + aes(x = age, color = sex, fill = sex) + geom_histogram(alpha=0.3, position=&quot;identity&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. Or with vertical facets: mockstudy %&gt;% ggplot() + aes(x = fu.time, color = sex) + geom_histogram(fill=&quot;white&quot;, alpha=0.5, position=&quot;identity&quot;)+ facet_grid(sex ~ .) ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. or horizontal facets: mockstudy %&gt;% ggplot() + aes(x = age, color = sex) + geom_histogram(fill=&quot;white&quot;, alpha=0.5, position=&quot;identity&quot;)+ facet_grid(. ~ sex) ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. 20.2 Density Plots Density Plots are essentially smoothed versions of histograms. Density plots are helpful to show the distribution of a numeric variable. The defaults for smoothing are quite reasonable. The only required aesthetic (like histogram) is an x variable. Optional aesthetics include alpha - setting the transparency, from 0 (invisible) to 1 (the default). Used when there are a lot of overlapping points, usually in the range from 0.05-0.5. group - for grouping into separate curves by a categorical variable. color - color of the distribution line, like color = “orchid”. When this is set to a categorical variable, geom_density will group by this variable and assign different colors to the groups. If you want to control the color assignments, you can use a palette with scale_color_brewer() or scale_color_manual (with the manual option you can set your own values or palette). Some options can be found here. fill color. You can use a single color, or set this to a categorical variable and use scale_fill_brewer or scale_fill_manual. You can also use scale_fill_viridis. size (line thickness) - the default is 1. linetype (6 types 1-6). You can also use the names of these 6 standard line types in quotes which are “solid”, “dashed”, “dotted”, “dotdash”, “longdash”, or “twodash”. You can even define a cutsom line type, providing the lengths of (up to 8) consecutive on-off segments. The string “3313” specifies a line with 3 units on, 3 units off, 1 unit on, 3 units off (dash-dot). The standard dotted and dashed line types (types 2-6) correspond to “44”, “13”. “1343”, “73” and “2262”. mockstudy %&gt;% ggplot() + aes(x = age) + geom_density() + geom_density(fill=&quot;orchid&quot;, size = 2, position=&quot;identity&quot;, alpha = 0.4, linetype = &quot;dashed&quot;)+ facet_grid(. ~ sex) + hrbrthemes::theme_ipsum() 20.2.1 Comparisons with Density plots You can compare a small number of categories with density plots, usually 5 or less. https://www.r-graph-gallery.com/density-plot You can set color and fill to the same variable. But you may have a lot of overlap. It may help to make these semi-transparent. It can be problematic if there is a lot of overlap and multiple categories, as they can hide each other A mirror density chart can work well for 2 categories. You can do small multiples with facet_wrap You can even do a stacked density chart. This is not great, as after the first layer, the baseline changes, and it ban be hard to compare. Can work for large differences between categories, but not good for small differences. 20.3 Comparing Distributions Across Categories Several geoms are designed for comparisons of distributions across multiple categories, and are more useful for this than histograms or density plots, particularly when the number of categories is large. These include Boxplots Ridgeline plots Violin plots All of these can be combined with plotting of individual data points to give both a summary of the distribution and visualization of the actual data. This can be important, especially when the number data points varies across categories, or of the number of data points are quite low in one part of the distribution. 20.4 Boxplots Boxplots are quick summaries of a distribution. They quickly give you a median line, and 25th and 75th percentiles (the ends of the box). The upper whisker is found at the last observation less than or equal to the 75th percentile plus 1.5* the IQR (interquartile range, or 75th-25th percentile). The lower whisker is found at the last observation less than or equal to the 25th percentile minus 1.5* the IQR. Points beyond the whiskers are considered “outliers”. Boxplots can be useful to compare distributions of a continuous variable across several categories. However, boxplots have several weakesses. Boxplots assume that your data are unimodal, and hide the distribution of your data points, which can be problematic if they are truly bimodal or trimodal (or even more local modes! (as in a Likert scale). Boxplots can also hide the number of observations, which may be quite different between categories. When this is an issue, it can be helpful to add data points, with geoms like jitter, beeswarm, or sina, or in ridgeline/raincloud plots. If there are a lot of overlapping points, you may need to set the geom_point alpha argument to a low value (0.05-0.5), rather than the default transparency of one. It can help the viewer of box plots to arrange your categories by order of their median, to make the interpretation easier. If your categories have a natural order (like months of the year), you will generally be better off keeping this natural order than ordering by median. 20.5 Violin Plots https://www.r-graph-gallery.com/violin_and_boxplot_ggplot2.html Violin Plots are essentially distribution plots that are symmetrical around their baseline. They are better than boxplots in that they don’t hide the distribution of your data points, and they don’t assume that the data are unimodal. You can see multiple local clusters if they are present in your data. A bimodal distribution will give you a shape that is reminiscent of a violin body. Violin plots can be helpful for comparing the distribution of a continuous variable across many categories. Violin plots can hide the number of observations, which may be quite different between categories. When this is an issue, it can be helpful to add points, with geoms like jitter, beeswarm, or sina, or in ridgeline/raincloud plots. If there are a lot of overlapping points, you may need to set the geom_point alpha argument to a low value (0.05-0.5), rather than the default transparency of one. Violin plots are helpful when you are comparing many groups. 20.6 Ridgeline Plots https://www.r-graph-gallery.com/ridgeline-plot Ridgeline plots are a nice way to show the distribution of a continuous variable, and compare distributions across several categories. Ridgeline plots are great for showing off striking differences between categories. Because they can have some overlap, they are not best for small or subtle differences. Note that you can add the N in each category to the category label when the number in each category varies. cmv %&gt;% ggplot(aes(x = time.to.transplant, y = diagnosis)) + geom_density_ridges(scale = 0.9) ## Picking joint bandwidth of 8.96 ## Warning: Removed 1 rows containing non-finite values ## (stat_density_ridges). 20.6.1 Including Plots You can also embed plots, for example: cmv %&gt;% ggplot(aes(x = time.to.agvhd, y = diagnosis, fill= stat(x))) + geom_density_ridges_gradient(scale = 0.9) + scale_fill_viridis_c(name = &quot;Days&quot;, option = &quot;A&quot;) + labs(title = &quot;Time to AGvHD in Days&quot;) ## Picking joint bandwidth of 8.13 20.6.2 Including Points You can also add data points, for example: cmv %&gt;% ggplot(aes(x = time.to.cmv, y = diagnosis, fill= stat(x))) + geom_density_ridges_gradient(scale = 0.5, jittered_points = TRUE, position = &quot;raincloud&quot;, alpha = 0.5) + scale_fill_viridis_c(name = &quot;Days&quot;, option = &quot;B&quot;) + labs(title = &quot;Time to CMV in Days&quot;) + theme_ridges() ## Picking joint bandwidth of 8.96 20.6.3 Including Points You can also add data points, for example: cmv %&gt;% ggplot(aes(x = CD8.dose, y = diagnosis, fill= stat(x))) + geom_density_ridges_gradient(scale = 0.9) + scale_fill_viridis_c(name = &quot;Days&quot;, option = &quot;C&quot;) + labs(title = &quot;CD8 Dose&quot;) + theme_ridges() ## Picking joint bandwidth of 0.249 20.6.4 Including Points You can also add data points, for example: cmv %&gt;% ggplot(aes(x = CD3.dose, y = diagnosis, fill= stat(x))) + geom_density_ridges_gradient(scale = 0.7, jittered_points = TRUE, position = &quot;raincloud&quot;) + scale_fill_viridis_c(name = &quot;Days&quot;, option = &quot;D&quot;) + labs(title = &quot;CD3 Dose&quot;) + theme_ridges() ## Picking joint bandwidth of 0.761 20.6.5 Including Points You can also add data points, for example: cmv %&gt;% ggplot(aes(x = TNC.dose, y = diagnosis, fill= stat(x))) + geom_density_ridges_gradient(scale = 2, jittered_points = TRUE, position = &quot;points_sina&quot;, alpha = 0.7) + scale_fill_viridis_c(name = &quot;Days&quot;, option = &quot;E&quot;) + labs(title = &quot;TNC Dose&quot;) + theme_ridges() ## Picking joint bandwidth of 2.19 "],["bivariate-ggplot2-scatterplots-to-visualize-relationships-between-variables.html", "Chapter 21 Bivariate ggplot2 Scatterplots to Visualize Relationships Between Variables 21.1 Packages used in this Chapter 21.2 Data Exploration and Validation (DEV) 21.3 Scatterplots 21.4 Mapping More Variables 21.5 Inheritance and Layering in ggplot2 21.6 Aesthetic mapping Micro-Quiz! 21.7 Controlling Point Shape, Size, and Color Manually", " Chapter 21 Bivariate ggplot2 Scatterplots to Visualize Relationships Between Variables 21.1 Packages used in this Chapter Use library(packagename) to load these packages, and install.packages(\"packagename\") to install these if they are not already installed. {tidyverse} {medicaldata} {ggpubr} 21.2 Data Exploration and Validation (DEV) When you first encounter a new dataset, it is often helpful to start with Data Exploration and Validation (DEV) . Note that DEV is different from EDA (Exploratory Data Analysis), which involves rummaging through your data (often with plots) and generating new hypotheses, performing multiple tests, and essentially data mining. DEV has specific goals, which include identifying variables with problematic values (NA, outliers), unexpected distributions (sometimes bimodal), and less-than-helpful variable names or data types. In this chapter, we will look at investigating pairs of continuous variables, looking for relationships and correlations. We will also add some new skills to help you customize your scatter plots, and to learn to think conceptually about building up ggplots in layers. 21.3 Scatterplots One of the most helpful ways to get started in exploring relationships between continuous variables is to explore pairs of continuous variables with a scatterplot. These are accomplished with geom_point() in ggplot2. A scatterplot allows you to see the relationship between two continuous variables, and is a good way to get started with ggplot. Let’s start by looking at the realtionship between the number of colon polyps found in participants in a clinical trial and their age. We start by building up layers data layer - tell R which dataset to use ggplot command - start a ggplot aesthetic mapping layer (ask R to map variables) geom layer (how to draw the datapoints) In this example, we will map the participant age to x, and the baseline number of polyps to y, and use geom_point as the geom. medicaldata::polyps %&gt;% ggplot() + aes(x = age, y = baseline) + geom_point() Copy this code to your RStudio console, and run it to get this plot. This gives us a very basic scatterplot, with a y-axis range between 7 and 318 polyps (that would have been a looooong colonoscopy if you tried to remove all 318 polyps). Not a dramatic correlation, though you could imagine a downward slope with age if you fitted a linear regression line. Though this is likely pulled way up at age 18 by the participant with many polyps. Now try changing the code to map y to the number of polyps at 3 months (number3m). The code example below adds a linear regression line with method = \"lm\". lm stands for linear model. Add a smoothing line to your code in the Console. Try method = \"loess\" for a different look, rather than forcing a linear relationship. polyps %&gt;% ggplot() + aes(x = age, y = baseline) + geom_point() + geom_smooth(method = &#39;loess&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Does it matter whether you map x or y first, in the aes() function? Try swapping these to find out. 21.3.1 Micro-quiz! (correct answers will be green!) X must always be mapped before Y in ggplot2 aes() functions TRUEFALSE The medicaldata::polyps line of code represents the geomdataaesthetic layer of the ggplot. The aes() function is the aesthetic layer, which pipesgluesmaps data variables to visual properties (aesthetics) of the plot. 21.4 Mapping More Variables You can add more information to a plot by mapping additional variables, which can help you make comparisons. You can map the treatment variable (sulindac or placebo) to the color aesthetic for the points. You just add color = varname to map a new variable named (varname) to the color. Try this by modifying the code below. Remember to put a comma between mappings, and to put your aesthetic mapping inside the aes() function. Then run the code chunk to generate a plot. polyps %&gt;% ggplot() + aes(x = age, y = number3m) + geom_point(color = &#39;blue&#39;) + geom_smooth(method = &#39;lm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; If this did not work, put , color = treatment after y = number3m inside the aes() parentheses. This should produce a plot with 2 colors of points, for placebo and sulindac, and 2 smoothed lines, one for each treatment. While the confidence intervals overlap, this does suggest that participants receiving sulindac might have fewer polyps. Try this for the 12 month polyp count, using y = number12m. This should look even more impressive. You can map the treatment variable to other aesthetics (visual properties) of the plot. Try this out with a few options: mapping treatment to shape (and color!) mapping number3m to size, and/or alpha NOTE that alpha is a term for the degree of opacity of a visual object, with zero = transparent. NOTE that a variable can be mapped to more than one visual property. BUT if you try to map more than one variable to a single visual property within the aes() function, the subsequent mappings will be ignored. The 1st mapping (within one aes) wins. polyps %&gt;% ggplot() + aes(x = age, y = number3m) + geom_point() If you try to map a variable to a visual property outside of the aes() function, it will be treated as a constant, not a variable, which can lead to strange results. Try moving color = treatment to a spot inside of the geom_point() function parentheses. Or even better, try color = 'blue' to within geom_point(). What happens? 21.5 Inheritance and Layering in ggplot2 Adding layers to a ggplot allows you to increase the complexity and customize a plot, building it up, layer by layer. This layered structure allows you to map a visual property (aesthetic) once, in the ggplot layer, or in the aes() layer. All subsequent layers inherit these visual properties, unless you specify differently. So your geom_point() layer inherits the mappings from the aesthetic layer. But if you want to, you can over-write the inherited properties, by stating a different aes() mapping. See the example below - the color mapping in geom_point over-writes the color mapping in the aesthetic layer, because it is in a later layer. In this case, the last layer wins. polyps %&gt;% ggplot() + aes(x = age, y = number3m, color = treatment) + geom_point(aes(color = age)) You can choose to do this deliberately. In the case below, this is done to make a single regression line that is purple, rather than the default black color. Because the color in geom_smooth() is set to a constant, you only get one (overall) regression line. Also notice that the color of the points is a bit dimmer than the previous plot. This is because the geom_smooth gray confidence interval from the linear regression is covering most of the points. Test this out by moving the geom_point() layer to the line after geom_smooth(). This should bring these points to the front. Whatever layer is last in the code is plotted on top. You can also ‘lighten up’ the confidence interval by setting the alpha for geom_smooth() to 0.1 (on a 0 to 1 opacity scale) polyps %&gt;% ggplot() + aes(x = age, y = number3m, color = treatment) + geom_point() + geom_smooth(method = &#39;lm&#39;, color = &#39;purple&#39;) 21.6 Aesthetic mapping Micro-Quiz! (correct answers will be green!) Which one of the following is NOT a visual property that you can map a variable to in a scatterplot fillcolorshapealphadatasize All mapping of variables to visual properties must be within the parentheses of the ggplotgeom_pointcoloraesscale function. You should assign visual properties to constant values, like color = \"blue\" or size = 2 within ggplot() or the geom_point() function, but outside of an aes() function TRUEFALSE. 21.7 Controlling Point Shape, Size, and Color Manually There are 25 point shapes available in R, which can be seen here, with the help of the show_point_shapes() function from {ggpubr}: ggpubr::show_point_shapes() Notice that the shapes 21-25 have an outline color, and can be filled with a different color, while shapes 1-20 only have a single color option. By default, ggplot2 selects shapes and colors for you automatically (in order, 1-25, for shapes, and in the order of the default color palette for colors), so that you don’t have to worry about specifying these. But if you want to take the wheel, you can have more control of the shape, size, and color of your scatterplot points with scale functions. Let’s try this with a scatterplot using the cmv dataset. We will look at the relationship between the CD34 cell dose at the time of bone marrow transplant, and the time to development of CMV. For our base plot, we will map donor cmv status to color. Since the donor.cmv variable is stored as a double it is continuous, but we actually want this as a dichtomous 0/1 variable, so we will recode this ‘on the fly’ in the aesthetic mapping as factor(donor.cmv) instead of just donor.cmv. You can remove factor() from the code below to how this helps. cmv %&gt;% ggplot() + aes(y = CD34.dose, x = time.to.cmv, color = factor(donor.cmv)) + geom_point() + geom_smooth(method = &#39;lm&#39;) These data show an interaction between cmv.donor status and the CD34.dose. In folks who receive a CMV+ donor BMT, higher doses of CD34 cells are associated with a delay in the onset of CMV, but this is not the case with CMV negative BMT donors. 21.7.1 Manual Shapes To control the point shapes, let’s replace the geom_smooth line with scale_shape_manual() and set the values to 22 and 25. We also have to set an aesthetic mapping for shape. Let’s map this to the donor.sex variable. cmv %&gt;% ggplot() + aes(y = CD34.dose, x = time.to.cmv, color = factor(donor.cmv), shape = factor(donor.sex)) + geom_point() + scale_shape_manual(values = c(22, 25)) Again, because donor.sex is stored as a numeric variable, we have to make it into a dichotomous factor on the fly in order to map this variable to shapes. Note that when you use the fillable shapes (21-25), you can also map a fill aesthetic to different color properties, separate from the color outline of the shape. You can also see that encoding donor.sex as 0 or 1 is not terribly helpful. It is often helpful to store data in #_label format, like 0_male, 1_female, to keep the encoding clear. You can extract the numeric portion with parse_number() if you want to do math on the variable later. If you have a lot of categories (like the 13 diagnosis categories in the cmv dataset), it can be easier to let ggplot handle the shapes, rather than having to specify 13 shapes. 21.7.2 Manual Sizes Let’s manually control the sizes of points - use the same approach, with scale_size_manual(). We want to emphasize the points with donor.cmv = 1, to see how this relates to the outcome variable, cmv, which is mapped to color. Run the scatter plot below as an example, then edit the code to map size to factor(donor.cmv). Then add a line of code for scale_size_manual(), and set the values to 1 and 4. cmv %&gt;% ggplot() + aes(y = age, x = time.to.transplant, size = CD8.dose, color = factor(cmv)) + geom_point() 21.7.3 Manual Color Let’s manually control the colors - use the same approach, with scale_color_manual(). Run the scatter plot below as an example, then edit the code to map color to factor(donor.cmv). Then add a line of code for scale_color_manual(), and set the values to “green” and “red”. cmv %&gt;% ggplot() + aes(y = age, x = time.to.transplant, color = diagnosis) + geom_point() "],["extensions-to-ggplot.html", "Chapter 22 Extensions to ggplot 22.1 Goals for this Chapter 22.2 Packages Needed for this chapter 22.3 A Flipbook of Where We Are Going With ggplot Extensions 22.4 A Waffle Plot 22.5 An Alluvial Plot 22.6 Lollipop Plots 22.7 Dumbbell Plots 22.8 Test what you have learned 22.9 Dumbbell Plots with ggalt For Visualizing Change 22.10 Direct Labeling of Plots 22.11 Test what you have learned", " Chapter 22 Extensions to ggplot The {ggplot2} package is made to be extensible - so that other people can write packages that add new (often niche) geoms for specific purposes. This chapter is a short tour of some of the neat extensions people have written, and when and where they can be useful. Please check out the links to the individual packages to learn more, as we will frequently just scratch the surface of what is available. 22.1 Goals for this Chapter learn how and why to use waffle plots learn how and when to use alluvial plots learn how and when to use lollipop plots -learn how and when to use dumbbell plots 22.2 Packages Needed for this chapter You will need {tidyverse}, {medicaldata}, {waffle}, {ggalluvial}, {ggalt}, {ggrepel}, {ggforce}, {ggalt}, {ggtext}. {ggsignif}, {ggbump}, {survminer}, {ggcorrplot}, {plotROC}, {directlabels}, {geomtextpath}, {ggheatmap}, {ggQC}, {ggupset}, {plotly}, and {gganimate}. # install.packages(&#39;tidyverse&#39;) # install.packages(&#39;medicaldata&#39;) library(tidyverse) library(medicaldata) library(ggrepel) library(ggforce) library(ggalt) library(ggtext) library(ggsignif) library(ggbump) library(survminer) library(ggmosaic) library(ggcorrplot) library(plotROC) library(directlabels) library(geomtextpath) library(ggheatmap) library(ggQC) library(ggupset) library(ComplexUpset) library(plotly) library(gganimate) 22.3 A Flipbook of Where We Are Going With ggplot Extensions See the flipbook below, which contains some examples of what you can do with ggplot extensions. You can use the the icons in the bottom bar to expand to full screen or share this flipbook. If you are in full screen mode, you can use the Home button to go the the first slide and the End button to go to the last slide, and the Escape key to get out of full screen mode. 22.3.1 MAKE FLIPBOOK 22.4 A Waffle Plot Why is this better than a bar plot or a dotplot? In order to represent counts, or individual participants in a trial, a bar plot is a bit deceiving. It appears to be a continuous variable. But each participant in a clinical trial is a discrete individual. A bar plot can be helpful for very large numbers, but for manageable numbers it is a bit of a misrepresentation. A dotplot, with geom_dotplot, would seem like a good option, but it only displays proportions, not counts. In order to show outcomes for distinct individual participants, a waffle plot comes in handy. These have become quite popular in data journalism to represent counts. Let’s start with a waffle plot of indo_rct &lt;- medicaldata::indo_rct scaler &lt;- 1 indo_data &lt;- indo_rct %&gt;% group_by(outcome, rx) %&gt;% count() %&gt;% mutate(n = n/scaler) indo_data %&gt;% #mutate(site = case_when(site == &quot;1_UM&quot; ~ &quot;Michigan&quot;, site == &quot;2_IU&quot; ~ &quot;Indiana&quot;, site == &quot;3_UK&quot; ~ &quot;Kentucky&quot;, site == &quot;4_Case&quot; ~ &quot;Case&quot;)) %&gt;% mutate(rx = str_sub(rx, 3L, 10L)) %&gt;% ggplot(aes(fill = outcome, values = n)) + geom_waffle(color = &quot;white&quot;, size=0.25, n_rows = 10, flip = TRUE, radius = unit(0.7, units = &quot;mm&quot;)) + facet_wrap(~rx, nrow = 1, strip.position = &quot;bottom&quot;) + scale_x_discrete() + scale_y_continuous(breaks = seq(10, 40, 10), labels = function(x) format(x * 10*scaler, scientific = F), expand = c(0,0)) + scale_fill_manual(values = c(&quot;#1a85ff&quot;, &quot;#d41159&quot; )) + coord_equal() + labs( title = &quot;Post-ERCP Outcomes by Treatment: &lt;br&gt;&lt;span style = &#39;color:#d41159;&#39;&gt;Pancreatitis&lt;/span&gt;, or &lt;span style = &#39;color:#1a85ff;&#39;&gt;No Event&lt;/span&gt;, &lt;br&gt;in the Rectal Indomethacin Trial&quot;, subtitle = sprintf(&quot;One square = %s Outcome, Each Row = 10 Outcomes&quot;, scaler), x = &quot;&quot;, y = &quot;&quot;, caption = &quot;Data: NEJM 2012; 366:1414-1422, Elmunzer&quot; ) + theme_minimal() + #theme_ipsum_rc()+ theme(panel.grid = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.ticks.y = element_line(), plot.title = element_markdown(), legend.position = &quot;none&quot;) + guides(fill = guide_legend(reverse = TRUE)) The waffle plot is an interesting hack of ggplot. The geom_waffle() is actually a faceted plot, with one row of facets (examine the code above). You can change the scaler constant to make each square count for N cases. You can learn more about the many capabilities of the {waffle} package - including pictograms and an alternative to pie charts - here. Note that we have used colors in the title in place of a legend, by coloring the outcomes with the corresponding colors, using the {ggtext} extension package, which allows you to add color, backgrounds, images, bold face, or italic face to text in ggplots, using markdown/HTML tags. You can learn more about the many capabilities of the {ggtext} package and what it can do here. 22.5 An Alluvial Plot An alluvial plot depicts flow, like a river, which can split off branches and re-unite streams. This king of plot can be helpful to show patient flow from one state to the next. datafr &lt;- tibble::tribble( ~gender, ~triage, ~next_day, ~outcome, ~count, &quot;Male&quot;, &quot;ER&quot;, &quot;Hospitalized&quot;, &quot;Survived&quot;, 211, &quot;Male&quot;, &quot;ER&quot;, &quot;Hospitalized&quot;, &quot;Survived&quot;, 43, &quot;Male&quot;, &quot;ER&quot;, &quot;Discharged&quot;, &quot;Died&quot;, 280, &quot;Male&quot;, &quot;ER&quot;, &quot;Discharged&quot;, &quot;Died&quot;, 15, &quot;Male&quot;, &quot;Observation&quot;, &quot;Hospitalized&quot;, &quot;Survived&quot;, 67, &quot;Male&quot;, &quot;Observation&quot;, &quot;Hospitalized&quot;, &quot;Died&quot;, 11, &quot;Male&quot;, &quot;Observation&quot;, &quot;Discharged&quot;, &quot;Survived&quot;, 415, &quot;Male&quot;, &quot;Observation&quot;, &quot;Discharged&quot;, &quot;Died&quot;, 5, &quot;Female&quot;, &quot;ER&quot;, &quot;Hospitalized&quot;, &quot;Survived&quot;, 219, &quot;Female&quot;, &quot;ER&quot;, &quot;Hospitalized&quot;, &quot;Survived&quot;, 33, &quot;Female&quot;, &quot;ER&quot;, &quot;Discharged&quot;, &quot;Died&quot;, 250, &quot;Female&quot;, &quot;ER&quot;, &quot;Discharged&quot;, &quot;Died&quot;, 45, &quot;Female&quot;, &quot;Observation&quot;, &quot;Hospitalized&quot;, &quot;Survived&quot;, 88, &quot;Female&quot;, &quot;Observation&quot;, &quot;Hospitalized&quot;, &quot;Died&quot;, 27, &quot;Female&quot;, &quot;Observation&quot;, &quot;Discharged&quot;, &quot;Survived&quot;, 402, &quot;Female&quot;, &quot;Observation&quot;, &quot;Discharged&quot;, &quot;Died&quot;, 14) %&gt;% mutate(gender = as_factor(gender), triage = as_factor(triage), next_day = as_factor(next_day), outcome = as_factor(outcome)) ggplot(datafr, aes(y = count, axis1 = gender, axis2 = triage, axis3 = next_day)) + geom_alluvium(aes(fill = outcome), width = 1/12) + geom_stratum(width =1/12, fill = &quot;black&quot;, color = &quot;grey&quot;) + geom_label(stat = &quot;stratum&quot;, aes(label = after_stat(stratum))) + scale_x_discrete(limits = c(&quot;gender&quot;, &quot;triage&quot;, &quot;next_day&quot;), expand = c(.10, .10)) + ggtitle(&quot;Patients Presenting with Chest Pain&quot;) + scale_fill_manual(values = c(&quot;#1a85ff&quot;, &quot;#d41159&quot; )) Now try this yourself. Copy the code above (click on the copy icon in the top right of the code chunk), paste it into your RStudio IDE, and edit to: change the x axis so that it starts at age 15, and ends at 90. Click on the Solution button to toggle showing or hiding the solution. Solution indo_rct %&gt;% ggplot() + aes(x = age, y = risk, color = outcome) + geom_jitter() + scale_y_continuous(limits = c(0,6)) + scale_x_continuous(limits = c(15,90)) ## Warning: Removed 1 rows containing missing values ## (geom_point). 22.6 Lollipop Plots While bar charts are quite popular for comparing continuous variables across categories, they have limitations. Humans are good at comparing length, but the bars add width, which is a distractions. Bar charts are also often used for counts, and it is not always clear whether a continuous or a discrete count variable is being plotted (a waffle chart can clear up discrete counts). For a continuous variable, you have a single point estimate (the end of the bar), and it is better to emphasize this estimate, without giving up the benefit of comparing lengths (which humans are good at). A lollipop plot emphasizes the continuous value, while de-emphasizing the width of a bar. Let’s look at an example below. medicaldata::covid_testing %&gt;% mutate(positive = case_when(ct_result &lt; 45 ~ 1, ct_result &gt;= 45 ~ 0)) %&gt;% group_by(demo_group) %&gt;% count(positive) %&gt;% filter(!is.na(positive)) %&gt;% mutate(freq = n/sum(n)) %&gt;% filter(positive==1) %&gt;% ggplot() + aes(x = fct_reorder(demo_group, freq), y = freq) + geom_lollipop(point.size = 5, point.colour = &quot;red&quot;) + scale_y_continuous(labels = scales::percent_format(scale = 100)) + labs(y = &quot;Percent Positive&quot;, x = &quot;Demographic Category&quot;) Now try this yourself. Copy the code above (click on the copy icon in the top right of the code chunk), paste it into your RStudio IDE, and edit to: Change the x axis so that it starts at age 0, and ends at 85. Make the x-axis expansion multiplier zero (not the default of 0.05). Click on the Solution button to toggle showing or hiding the solution. Solution indo_rct %&gt;% ggplot() + aes(x = age, y = risk, color = outcome) + geom_jitter() + scale_y_continuous(limits = c(0,6)) + scale_x_continuous(limits = c(0,85), expand = expansion(mult = 0)) ## Warning: Removed 1 rows containing missing values ## (geom_point). 22.7 Dumbbell Plots You can see that ggplot picks sensible breaks, but the defaults might not always work for you. Let’s change the risk scale to breaks of 0.5, using the breaks argument. Note that using the limits argument also lets you establish the limits of the y-axis. indo_rct %&gt;% ggplot() + aes(x = age, y = risk, color = outcome) + geom_point() + scale_y_continuous(limits = c(0,6), breaks = seq(0, 6, by = 0.5)) Now try this yourself. Copy the code above (click on the copy icon in the top right of the code chunk), paste it into your RStudio IDE, and edit to: Change the x axis so that it starts at age 0, and ends at 95, with breaks at every decade from 10-90 (but not zero). Click on the Solution button to toggle showing or hiding the solution. Solution indo_rct %&gt;% ggplot() + aes(x = age, y = risk, color = outcome) + geom_jitter() + scale_y_continuous(limits = c(0,6)) + scale_x_continuous(limits = c(0,95), expand = expansion(mult = 0), breaks = seq(10, 90, by = 10)) Notice that the y axis has the default 5% multiplier, but the x axis does not, so it has limits exactly at 0 and 95. 22.8 Test what you have learned (correct answers will be green!) You can set the start and end points of an axis with the limits argument TRUEFALSE You can set the ticks on an axis with the breakstickslines argument in a scales function. To expand the margin of a plot on one side by a specific amount, you use the multsqrtadd argument in the expand argument within a scales function. 22.9 Dumbbell Plots with ggalt For Visualizing Change The Dumbbell Plot is a visualization that shows change between two points (usually 2 time points) in our data. It gets the name because of its dumbbell shape. It’s a great way to show changes in data between two points (think start and finish). Note that a bit of data wrangling needs to be done to produce the correct data format for geom_dumbbell(). You may need to pivot_wider() to get 2 columns of data on distinct dates (in this case, month 1 vs month 4). See the data wrangling below to get mean age for these 2 months. medicaldata::covid_testing %&gt;% filter(!str_detect(patient_class, &quot;surgery&quot;)) %&gt;% mutate(pan_month = ceiling((pan_day)/30)) %&gt;% filter(pan_month %in% c(1,4)) %&gt;% pivot_wider(names_from = pan_month, values_from = age, id_cols = patient_class, values_fn = function(x) mean(x, na.rm = TRUE), names_prefix = &quot;month_&quot;) -&gt; dumb_covid_data dumb_covid_data ## # A tibble: 6 × 3 ## patient_class month_1 month_4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 inpatient 3.42 8.68 ## 2 not applicable 9.11 24.6 ## 3 emergency 2.25 13.5 ## 4 recurring outpatient 2.21 6.93 ## 5 observation 2.55 12.2 ## 6 outpatient 6.10 19.3 dumb_covid_data %&gt;% ggplot(aes(x = month_1, xend = month_4, y = patient_class, group = patient_class)) + geom_dumbbell(size = 2, # size of line size_x =4, size_xend = 4, # dot size colour = &quot;lightblue2&quot;, #line color colour_x = &quot;dodgerblue&quot;, # 1st dot color colour_xend = &quot;blue&quot;) + # end dot color labs(x = &quot;Mean Patient Age&quot;, y = &quot;Patient Class&quot;, title = &quot;Increases in Mean Patient Age at a Pediatric Hospital \\nfrom Month 1 to Month 4 of the Pandemic&quot;, subtitle = &quot;Keeping the Young Ones Safe at Home&quot;) + xlim(1,25) + # set limits on x axis theme_linedraw() + theme(plot.title.position = &quot;plot&quot;) # align title and subtitle to left edge, rather than to plot area+ Let’s expand the x axis to the righ to make room for a legend in the plot on the right, using the expand argument. We can change the axis name and position as well. indo_rct %&gt;% ggplot() + aes(x = rx, y = risk, color = outcome) + geom_jitter() + theme(legend.position = c(0.85, 0.5)) + scale_y_continuous(limits = c(0.5,6), breaks = seq(0.5, 6, by = 0.5)) + scale_x_discrete(expand = expansion(add =c(0.6,1.5)), name = &quot;Treatment&quot;, position = &quot;top&quot;) Now try this yourself. Copy the code above (click on the copy icon in the top right of the code chunk), paste it into your RStudio IDE, and edit to: Change the x axis so that you add 1.5 to the left side (add 1.5, 0.6), move the legend to the left (0.15, 0.5) change the title to “Suppository” move the title position to the bottom Click on the Solution button to toggle showing or hiding the solution. Solution indo_rct %&gt;% ggplot() + aes(x = rx, y = risk, color = outcome) + geom_jitter() + theme(legend.position = c(0.15, 0.5)) + scale_y_continuous(limits = c(0.5,6), breaks = seq(0.5, 6, by = 0.5)) + scale_x_discrete(expand = expansion(add =c(1.5, 0.6)), name = &quot;Suppository&quot;, position = &quot;bottom&quot;) The legend position is based on the proportion of the x axis (0-1) and the y axis (0-1), so that legend.position (0,0) is the bottom left, and legend.position (1,1) is the top right. 22.10 Direct Labeling of Plots Whilea legend can help a viewer interpret a plot, it requires a bit of cognitive work to look back and forth between the plot and the legend to sort things out. If you have a manageable number of categories, it can be more effective to add category labels directly to your plot. This can be done manually, with geom_label, or with a bit of help from {directlabel} or {geomtextpath}. 22.10.1 GeomTextPath This geom, from the {geomtextpath} package, allows you to put labels on a curved path, and can be helpful for labeling distributions. In this density plot example, x will map to your continous variable/distribution, while color and label will map to your categorical variable. This is an example of age distributions by the type of sphincter of oddi dysfunction (the muscular valve at the end of the common discharge pathway of the bile ducts and pancreatic ducts into the small intestine) in the indo_rct dataset. The patients with more severe SOD tend to be younger than those without SOD, and type 2 SOD has a bimodal distribution. medicaldata::indo_rct %&gt;% ggplot(aes(x = age, colour = type)) + geom_textpath(aes(label = type), stat = &quot;density&quot;, size = 4, fontface = 2, hjust = 0.65, vjust = -0.1) + theme(legend.position = &quot;none&quot;) Now try this yourself. Copy the code above (click on the copy icon in the top right of the code chunk), paste it into your RStudio IDE, and edit to: Change the legend make the title “PEP” change the labels to “good” and “BAD” change the colors to “yellow” and “darkorchid” Click on the Solution button to toggle showing or hiding the solution. Solution medicaldata::indo_rct %&gt;% ggplot() + aes(x = rx, y = risk, color = outcome) + geom_jitter() + theme(legend.position = c(0.15, 0.5)) + scale_y_continuous(limits = c(0.5,6), breaks = seq(0.5, 6, by = 0.5)) + scale_x_discrete(expand = expansion(add =c(1.5, 0.6)), name = &quot;Suppository&quot;, position = &quot;bottom&quot;) + scale_color_manual(name = &quot;PEP&quot;, labels = c(&quot;good&quot;, &quot;BAD&quot;), values = c(&quot;yellow&quot;, &quot;darkorchid&quot;)) The legend position is based on the proportion of the x axis (0-1) and the y axis (0-1), so that legend.position (0,0) is the bottom left, and legend.position (1,1) is the top right. 22.11 Test what you have learned (multiple-choice, fill-in-the-blank, and TRUE/FALSE - correct answers will be green!) You can take complete control of colors with scale_color_ continuousdiscretemanual You can set the title of a color legend within the scale_color_discrete() function with the argument. You can set the names of each level of a discrete color legend within the scale_color_discrete() function with the argument. You can set each color of a discrete color legend within the scale_color_discrete() function with the values TRUEFALSE argument. 22.11.1 More Examples with Flipbooks Now try some challenging code exercises using scales in the learnr app below. Use your knowledge to try to do these without hints, but press the Hints button if needed. In each case, the 2nd hint is the solution. "],["customizing-plot-scales.html", "Chapter 23 Customizing Plot Scales 23.1 Goals for this Chapter 23.2 Packages Needed for this chapter 23.3 A Flipbook of Where We Are Going With Scales 23.4 A Basic Scatterplot 23.5 But what if you want the scale for risk to start at 0? 23.6 But this axis does not really start at Exactly 0 23.7 Control the Limits and the Breaks 23.8 Test what you have learned 23.9 Continuous vs. Discrete Plots and Scales 23.10 Using Scales to Customize a Legend 23.11 Test what you have learned", " Chapter 23 Customizing Plot Scales 23.1 Goals for this Chapter learn how to use scales to control x and y axes learn how to use scales to control shape, color, and alpha learn how to use scales to control the legend learn how to use scales for dates, percents, log, ordinals, dollars, and scientific notation 23.2 Packages Needed for this chapter You will need {tidyverse}, {medicaldata}, and {scales} # install.packages(&#39;tidyverse&#39;) # install.packages(&#39;medicaldata&#39;) # install.packages(&#39;scales&#39;) library(tidyverse) library(medicaldata) library(scales) 23.3 A Flipbook of Where We Are Going With Scales See the flipbook below, which contains some examples of what you can do with customized scales. These control both the axes and the legend(s). You can click on the flipbook arrows and move forward or backward in the slides with the left and right arrows to see what each line of code actually does. Go forward and backward until you understand the function of each line. You can use the the icons in the bottom bar to expand to full screen or share this flipbook. If you are in full screen mode, you can use the Home button to go the the first slide and the End button to go to the last slide, and the Escape key to get out of full screen mode. 23.4 A Basic Scatterplot Let’s start with a scatterplot of age vs risk of PEP in the indo_rct dataset indo_rct &lt;- medicaldata::indo_rct indo_rct %&gt;% ggplot() + aes(x = age, y = risk, color = outcome) + geom_jitter() The axes cover the whole range by default, whith a bit of space added at the edges. This occurs because the default for scale_(x|y)continuous* for continuous variables adds 5% at either end so that points are not right at the edge. Similarly, the scale_discrete function for discrete variables adds 0.6 of a category to the width to either side. 23.5 But what if you want the scale for risk to start at 0? You can do this, by taking control of the scales. In this case, the scale_y_continuous() function. indo_rct %&gt;% ggplot() + aes(x = age, y = risk, color = outcome) + geom_jitter() + scale_y_continuous(limits = c(0,6)) Now try this yourself. Copy the code above (click on the copy icon in the top right of the code chunk), paste it into your RStudio IDE, and edit to: change the x axis so that it starts at age 15, and ends at 90. Click on the Solution button to toggle showing or hiding the solution. Solution indo_rct %&gt;% ggplot() + aes(x = age, y = risk, color = outcome) + geom_jitter() + scale_y_continuous(limits = c(0,6)) + scale_x_continuous(limits = c(15,90)) ## Warning: Removed 1 rows containing missing values ## (geom_point). 23.6 But this axis does not really start at Exactly 0 You can see that the x- and y-axes extend a bit past 0. This is because there is a default expansion of the scales (5% for continuous variables). You can control this default with the expand() function. Let’s see how this works to make the y-axis start at exactly zero. You can set the expansion term as a multiplier (mult) or an additive (add). indo_rct %&gt;% ggplot() + aes(x = age, y = risk, color = outcome) + geom_jitter() + scale_y_continuous(limits = c(0,6), expand = expansion(mult =0)) Now try this yourself. Copy the code above (click on the copy icon in the top right of the code chunk), paste it into your RStudio IDE, and edit to: Change the x axis so that it starts at age 0, and ends at 85. Make the x-axis expansion multiplier zero (not the default of 0.05). Click on the Solution button to toggle showing or hiding the solution. Solution indo_rct %&gt;% ggplot() + aes(x = age, y = risk, color = outcome) + geom_jitter() + scale_y_continuous(limits = c(0,6)) + scale_x_continuous(limits = c(0,85), expand = expansion(mult = 0)) ## Warning: Removed 1 rows containing missing values ## (geom_point). 23.7 Control the Limits and the Breaks You can see that ggplot picks sensible breaks, but the defaults might not always work for you. Let’s change the risk scale to breaks of 0.5, using the breaks argument. Note that using the limits argument also lets you establish the limits of the y-axis. indo_rct %&gt;% ggplot() + aes(x = age, y = risk, color = outcome) + geom_point() + scale_y_continuous(limits = c(0,6), breaks = seq(0, 6, by = 0.5)) Now try this yourself. Copy the code above (click on the copy icon in the top right of the code chunk), paste it into your RStudio IDE, and edit to: Change the x axis so that it starts at age 0, and ends at 95, with breaks at every decade from 10-90 (but not zero). Click on the Solution button to toggle showing or hiding the solution. Solution indo_rct %&gt;% ggplot() + aes(x = age, y = risk, color = outcome) + geom_jitter() + scale_y_continuous(limits = c(0,6)) + scale_x_continuous(limits = c(0,95), expand = expansion(mult = 0), breaks = seq(10, 90, by = 10)) Notice that the y axis has the default 5% multiplier, but the x axis does not, so it has limits exactly at 0 and 95. 23.8 Test what you have learned (correct answers will be green!) You can set the start and end points of an axis with the limits argument TRUEFALSE You can set the ticks on an axis with the breakstickslines argument in a scales function. To expand the margin of a plot on one side by a specific amount, you use the multsqrtadd argument in the expand argument within a scales function. 23.9 Continuous vs. Discrete Plots and Scales You can see below that ggplot picks sensible spacing and breaks for a discrete scale, but the defaults might not always work for you. indo_rct %&gt;% ggplot() + aes(x = rx, y = risk, color = outcome) + geom_jitter() + theme(legend.position = c(0.85, 0.5)) + scale_y_continuous(limits = c(0.5,6), breaks = seq(0.5, 6, by = 0.5)) Let’s expand the x axis to the righ to make room for a legend in the plot on the right, using the expand argument. We can change the axis name and position as well. indo_rct %&gt;% ggplot() + aes(x = rx, y = risk, color = outcome) + geom_jitter() + theme(legend.position = c(0.85, 0.5)) + scale_y_continuous(limits = c(0.5,6), breaks = seq(0.5, 6, by = 0.5)) + scale_x_discrete(expand = expansion(add =c(0.6,1.5)), name = &quot;Treatment&quot;, position = &quot;top&quot;) Now try this yourself. Copy the code above (click on the copy icon in the top right of the code chunk), paste it into your RStudio IDE, and edit to: Change the x axis so that you add 1.5 to the left side (add 1.5, 0.6), move the legend to the left (0.15, 0.5) change the title to “Suppository” move the title position to the bottom Click on the Solution button to toggle showing or hiding the solution. Solution indo_rct %&gt;% ggplot() + aes(x = rx, y = risk, color = outcome) + geom_jitter() + theme(legend.position = c(0.15, 0.5)) + scale_y_continuous(limits = c(0.5,6), breaks = seq(0.5, 6, by = 0.5)) + scale_x_discrete(expand = expansion(add =c(1.5, 0.6)), name = &quot;Suppository&quot;, position = &quot;bottom&quot;) The legend position is based on the proportion of the x axis (0-1) and the y axis (0-1), so that legend.position (0,0) is the bottom left, and legend.position (1,1) is the top right. 23.10 Using Scales to Customize a Legend Legends are also scales, for discrete or continuous scales. You can use scales_(color|size|shape|alpha)_nnn functions to customize them. Let’s see an example below. medicaldata::indo_rct %&gt;% ggplot() + aes(x = rx, y = risk, color = outcome) + geom_jitter() + theme(legend.position = c(0.85, 0.5)) + scale_y_continuous(limits = c(0.5,6), breaks = seq(0.5, 6, by = 0.5)) + scale_x_discrete(expand = expansion(add =c(0.6,1.5)), name = &quot;Treatment&quot;, position = &quot;top&quot;) + scale_color_manual(name = &quot;Post-ERCP\\nPancreatitis&quot;, labels = c(&quot;no&quot;, &quot;yes&quot;), values = c(&quot;dodgerblue&quot;, &quot;red&quot;)) Now try this yourself. Copy the code above (click on the copy icon in the top right of the code chunk), paste it into your RStudio IDE, and edit to: Change the legend make the title “PEP” change the labels to “good” and “BAD” change the colors to “yellow” and “darkorchid” Click on the Solution button to toggle showing or hiding the solution. Solution medicaldata::indo_rct %&gt;% ggplot() + aes(x = rx, y = risk, color = outcome) + geom_jitter() + theme(legend.position = c(0.15, 0.5)) + scale_y_continuous(limits = c(0.5,6), breaks = seq(0.5, 6, by = 0.5)) + scale_x_discrete(expand = expansion(add =c(1.5, 0.6)), name = &quot;Suppository&quot;, position = &quot;bottom&quot;) + scale_color_manual(name = &quot;PEP&quot;, labels = c(&quot;good&quot;, &quot;BAD&quot;), values = c(&quot;yellow&quot;, &quot;darkorchid&quot;)) The legend position is based on the proportion of the x axis (0-1) and the y axis (0-1), so that legend.position (0,0) is the bottom left, and legend.position (1,1) is the top right. 23.11 Test what you have learned (multiple-choice, fill-in-the-blank, and TRUE/FALSE - correct answers will be green!) You can take complete control of colors with scale_color_ continuousdiscretemanual You can set the title of a color legend within the scale_color_discrete() function with the argument. You can set the names of each level of a discrete color legend within the scale_color_discrete() function with the argument. You can set each color of a discrete color legend within the scale_color_discrete() function with the values TRUEFALSE argument. 23.11.1 More Examples with Flipbooks Now try some challenging code exercises using scales in the learnr app below. Use your knowledge to try to do these without hints, but press the Hints button if needed. In each case, the 2nd hint is the solution. "],["helping-out-with-ggplot.html", "Chapter 24 Helping out with ggplot 24.1 ggx::gghelp() 24.2 Getting more help with theming with ggThemeAssist 24.3 Website helpers for ggplot 24.4 Getting Even more help with esquisse", " Chapter 24 Helping out with ggplot The {ggplot} package is extremely powerful, and has many extension packages that augment that power for data visualization. But the number of options, particularly for theming, can get overwhelming. This is especially true if you are not using {ggplot} every day. Medical day (and night) jobs can get in the way of great data visualizations. But there are several helper packages to make your life easier, including {ggx}, {ggThemeAssist}, and {esquisse}. 24.1 ggx::gghelp() Let’s start with {ggx}. The gghelp() function in this simple package converts natural language queries (in quotes) into a ggplot command string. It can be helpful for styling axes, labels, font size, title, and legends. It is limited by its library of commands and range of styling, but can be helpful in a pinch. Try a few questions below, and make your own. ggx::gghelp(&quot;how do I remove the legend&quot;) ## theme(legend.position = &quot;none&quot;) ggx::gghelp(&quot;how do I increase the font size of the title&quot;) ## theme(axis.title.x=element_text(size=rel(2))) ggx::gghelp(&quot;change the x axis label to &#39;systolic blood pressure&#39;&quot;) ## xlab(&#39;systolic blood pressure&#39;) 24.2 Getting more help with theming with ggThemeAssist ggThemeAssist is an RStudio add-in that you install when you install the {ggThemeAssist} package. It is used after you have your basic plot in place, with the right geom, and the x and y variables in place. Let’s start you out with a simple plot from the {medicaldata} package. This plots the fentanyl requirements for anesthesia in the supraclavicular dataset comparing high and low BMI, age, and gender effects. Run this code block to see what the basic plot looks like. plot1 &lt;- medicaldata::supraclavicular %&gt;% filter(!is.na(bmi)) %&gt;% mutate(gender_cat = case_when(gender == 1 ~ &quot;Male&quot;, gender == 0 ~ &quot;Female&quot;)) %&gt;% mutate(bmi_cat = case_when(bmi &lt;30 ~ &quot;low&quot;, bmi &gt;= 30 ~ &quot;high&quot;)) %&gt;% ggplot(aes(x=age, y = fentanyl, col = gender_cat)) + geom_point() + facet_wrap(. ~ bmi_cat) plot1 + theme(axis.text.y = element_text(family = &quot;serif&quot;), panel.background = element_rect(fill = &quot;antiquewhite&quot;), plot.background = element_rect(fill = &quot;white&quot;)) + labs(title = &quot;Fentanyl Requirements&quot;, x = &quot;Age&quot;, y = &quot;Fentanyl in mcg&quot;, colour = &quot;Gender&quot;, subtitle = &quot;by BMI and Age&quot;) ## Warning: Removed 1 rows containing missing values ## (geom_point). Now select the code that generates the plot. You can also assign the plot (with an assignment arrow) to an imaginative name, like plot1. You can then select this object as well. Once you have the plot selected in your code, you can activate ggThemeAssist in one of two ways: Go to your Addins dropdown menu, and select ggThemeAssist type in the Console: ggthemeAssistGaget(plot1) Either approach will open up an interactive window, with 6 tabs and a bunch of options. These tabs include: Settings Panel and Background Axis Title and Label Legend Subtitle and Caption Open these up and experiment. You will see the results as you change options. When you are happy with the result, click on the Done button in the top right. This will add all of your thematic changes as valid R code to the existing plot object. 24.3 Website helpers for ggplot Several websites provide quick help for ggplot needs, and are worth bookmarking. The Aesthetics Helper, at https://ggplot2tor.com/aesthetics/, provides a quick guide to which aesthetics are required (green), and which other aesthetics are available (optional, in orange) to map variables in your dataset to components of a plot for each geom. The Guide to Scales, at https://ggplot2tor.com/scales, helps you find the proper names of scales in ggplot2, by selecting your variable type and (if needed) the aesthetic for the scale. Then by clicking on one of the available scales, you can get example code with appropriate syntax for multiple arguments. This can be very helpful in preventing frustration with not having the scales quite right. The R Graph Gallery, at https://www.r-graph-gallery.com, is a popular overview of available graph types. You can quickly scan a bunch of plots based on distributions, correlations, rankings, parts of a whole, change over time, maps, and flow and select one that looks interesting. When you find one you like, you can click on it and get the underlying ggplot code. Another popular web gallery with code is the Top 50 ggplot2 visualizations, at http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html Cedric Scherer has a lengthy free tutorial with lots of great examples at https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/ There is also a gallery of all the many extension packages people have built as add-ons to ggplot, often for specific and specialized plotting needs. Take a loot at https://exts.ggplot2.tidyverse.org/gallery/ Packages ggridges, ggalluvial, ggmosaic, ggdist, gghalves, and ComplexUpset have proven quite popular. Take a look at all the options for extending ggplot. 24.4 Getting Even more help with esquisse {esquisse} is an RStudio add-in that you install when you install the {esquisse} package. It is used to create plots from scratch in ggplot. A nice website on how to get started can be found at: https://cran.r-project.org/web/packages/esquisse/vignettes/get-started.html But the two ways to get started are to either: run esquisser(dataframe) in the Console, with the dataframe that you want to make a plot from, or In the Addins menu, select ggplot2_builder under ESQUISSE Either approach opens up a new window. If you did not specify a dataframe, it will ask you to select one, either from the Global Environment, or from a data package. For our purposes, select the {medicaldata} package, and the dataframe blood_storage. This will give you a window with many options, and a sandbox full of variables. To look at the data, click on the Show Data icon at top left You can drag variables into different aesthetic mappings. Start with PreopPSA for x TimeToRecurrence for y Age mapped to color FamHx or AA mapped to Size You can drag variables out of mappings and back to the variable sandbox if needed You can click on the Settings Gear at the top right to activate more aesthetics You can click on the geom icon at top left to change the plot type You can edit the Title and Labels with this tab You can edit Plot Options You can edit the plot appearance When you are happy with it, you can click on the Code tab, and copy it to your script. {esquisse} can get you started on good plots, and remind you of ggplot options you may have forgotten (or never knew) about. There are a number of fancy plots and extensions it can not do, and it will not clean your data for you, or reorder factors. But it can be really helpful if you are not plotting very often or if you are just getting started with ggplot. "],["functions-1.html", "Chapter 25 Functions 25.1 Don’t repeat yourself 25.2 Your Turn 25.3 Freestyle 25.4 Read More", " Chapter 25 Functions Nearly everything in R and the tidyverse is built on functions. Every command you write that ends with parentheses is a function. The parentheses contain arguments (unless you just use the defaults), which the function acts upon. If you type a function into the console and forget the parentheses, you may be surprised at what you get back when you press the Enter key. Try this with the sd function by running the chunk below sd At first this looks like computer gobbledygook, but if you look at it closely, the first line sets up the structure of the function and its arguments, and the second line defines the function as the square root of the variance of a vector that contains numbers. You are seeing the inner workings of the sd function. Other functions may be written in other languages (like C++) for more speed, but you can use R to write more R functions. When you try this for another function, like filter in the dplyr package, it is less informative. As you can see when you unpack the function below. dplyr::filter 25.1 Don’t repeat yourself Functions are especially helpful when you do something repeatedly, or do the same thing on several variables or several datasets. Let’s start with a simple plot of Ct (COVID viral load) vs age for inpatients. Run the code chunk below. covid %&gt;% filter(patient_class == &quot;inpatient&quot;) %&gt;% ggplot() + aes(x = age, y = ct_result) + geom_point() + theme_linedraw() + labs(y = &quot;viral load (in Ct)&quot;, title = &quot;Patient class: inpatient&quot;) This is great, but you would like a similar plot for ER patients, observation patients, outpatients, etc. How do you do this without multiple copy-paste and careful edits of the key variables (often with errors)? First, you need to avoid hard-coding the specific values that change from plot to plot. In our next version of this plot, we will set an envronment variable (patient_class_choice_env) to “inpatient”, then use that variable to: filter the data set the title (using the glue function) patient_class_choice_env = &quot;inpatient&quot; covid %&gt;% filter(.data$patient_class == .env$patient_class_choice_env) %&gt;% ggplot() + aes(x = age, y = ct_result) + geom_point() + theme_linedraw() + labs(y = &quot;viral load (in Ct)&quot;, title = glue(&quot;Patient class: {patient_class_choice_env}&quot;)) The use of variable pronouns is very important in the filter step. .data$patient_class means the variable “patient_class” in the currently in-use dataframe. .env$patient_class_choice_env means the variable “patient_class_choice_env” in the current environment (check the Environment pane). This can be especially helpful when these variables have the same names - the .data pronoun helps you refer specifically to variables in the dataframe, and the .env pronoun helps you refer specifically to variables in the environment. It is usually better to give these slightly different names. In this case, I have added the suffix, “choice_env” for the variable in the environment in which I have specified the value chosen. Let’s try another example. In the code chunk below, edit the patient_class_choice_env variable to the value “emergency”, and then run the code chunk. patient_class_choice_env = &quot;emergency&quot; covid %&gt;% filter(.data$patient_class == .env$patient_class_choice_env) %&gt;% ggplot() + aes(x = age, y = ct_result) + geom_point() + theme_linedraw() + labs(y = &quot;viral load (in Ct)&quot;, title = glue(&quot;Patient class: {patient_class_choice_env}&quot;)) Fairly slick, right? You can edit and run through each value for patient_class. But what if you want to automate this, and just produce all the plots for all the possible values? To do this, you have to turn your plot into a function. To make a function, you first need a chunk of code with no hard-coded values. Then, you wrap the code in curly braces, and preface it with “function(arguments)” Then you assign it to a function name. Examine, then run the example below. make_covid_plot &lt;- function(patient_class_choice_env) { covid %&gt;% filter(.data$patient_class == .env$patient_class_choice_env) %&gt;% ggplot() + aes(x = age, y = ct_result) + geom_point() + theme_linedraw() + labs(y = &quot;viral load (in Ct)&quot;, title = glue(&quot;Patient class: {patient_class_choice_env}&quot;)) } make_covid_plot(&quot;recurring outpatient&quot;) Note that this creates a function object named make_covid_plot in your Environment pane. And you still have to call the function with the argument “recurring outpatient” to get one plot. Generally, you should write functions when you find yourself copy-pasting the same code several times. Each function should be no more than 20-30 lines. If longer, break it up into a couple of functions. You can do this repeatedly, over several values for patient_class. Try this below by editing the code chunk to make plots for “emergency” and “not applicable” patient classes. make_covid_plot(&quot;outpatient&quot;) make_covid_plot(&quot;observation&quot;) This is better, but it is still not completely automated. Let’s automate calling the function over several values with the map function. map takes each element of the vector patient_class and uses it as input for make_covid_plot(). map returns a list of created plots. We can return these with bracket references to items 1,2, and 3 in the list. patient_class &lt;- c(&quot;emergency&quot;, &quot;inpatient&quot;, &quot;observation&quot;) plots &lt;- map(patient_class, make_covid_plot) plots[[1]] plots[[2]] plots[[3]] We can make this simpler with the walk function. The walk function is like map, but it does not return values, only side effects (like printing). In this case, it is applying the print function to each element of the plots list. walk(plots, print) We can automate this further, and not need to supply the vector of patient_class values. We can automate it by applying the make_covid_plot function to all available patient_class values. To demonstrate this, we need a more general function, with 2 arguments: one for the dataset, and one for for patient_class. We will test this on the “inpatient” class. The filtering operation will be moved outside of the function make_covid_plot_2 &lt;- function(data, patient_class) { data %&gt;% # note data argument # filtering now done outside of function ggplot() + aes(x = age, y = ct_result) + geom_point() + theme_linedraw() + labs(y = &quot;viral load (in Ct)&quot;, title = glue(&quot;Patient class: {patient_class}&quot;)) } data_inpatient &lt;- covid %&gt;% filter(patient_class == &quot;inpatient&quot;) make_covid_plot_2(data_inpatient, patient_class = &quot;inpatient&quot;) Now we can try this in a tidy pipeline. We will first subset the data by nesting it with the nest function. Take a look at what this does covid %&gt;% nest(data = -patient_class) ## # A tibble: 5 × 2 ## patient_class data ## &lt;chr&gt; &lt;list&gt; ## 1 observation &lt;tibble [28 × 16]&gt; ## 2 outpatient &lt;tibble [65 × 16]&gt; ## 3 emergency &lt;tibble [124 × 16]&gt; ## 4 recurring outpatient &lt;tibble [11 × 16]&gt; ## 5 inpatient &lt;tibble [111 × 16]&gt; We have essentially grouped the data in to 5 distinct tibbles (dataframes) in the list-column named data. These are dataframes in a column nested inside of a larger dataframe. Now we can make a plot from each of these smaller dataframes, using the mutate function and the map function, and our original make_covid_plot function. covid %&gt;% nest(data = -patient_class) %&gt;% mutate(plots = map(patient_class, make_covid_plot)) ## # A tibble: 5 × 3 ## patient_class data plots ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 observation &lt;tibble [28 × 16]&gt; &lt;gg&gt; ## 2 outpatient &lt;tibble [65 × 16]&gt; &lt;gg&gt; ## 3 emergency &lt;tibble [124 × 16]&gt; &lt;gg&gt; ## 4 recurring outpatient &lt;tibble [11 × 16]&gt; &lt;gg&gt; ## 5 inpatient &lt;tibble [111 × 16]&gt; &lt;gg&gt; Now we have a column that contains the plots. We can pull these out of the dataframe into a vector with the pull function (because walk works on vectors), then print them to the Plots tab with the walk function. covid %&gt;% nest(data = -patient_class) %&gt;% mutate(plots = map(patient_class, make_covid_plot)) %&gt;% pull(plots) %&gt;% walk(print) Note that we could also walk the ggsave function if we wanted to save these plots as tiffs or pdfs or png files. We can use our more generalizable function make_covid_plot_2 in this pipeline if we use map2, which is like map, but for functions with 2 arguments. The arguments of the map2 function include the two arguments (data, patient_class) for making plots, and then the function (make_covid_plot_2). covid %&gt;% nest(data = -patient_class) %&gt;% mutate(plots = map2(data, patient_class, make_covid_plot_2)) %&gt;% pull(plots) %&gt;% walk(print) This version of the pipeline will automatically process all patient classes in the dataset, whatever they are called, without us having to specify them. 25.2 Your Turn Let’s look at how to do this with the prostate dataset. We will start with a simple violin and jitter plot of time to recurrence by baseline Gleason score (bgs), in the subgroup where rbc_age_group = 1. rbc_age_group = 1 prostate %&gt;% filter(rbc_age_group == 1) %&gt;% ggplot() + aes(x = bgs, y = time_to_recurrence, fill = bgs) + geom_violin() + geom_jitter(width = 0.2) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = &quot;Baseline Gleason Score&quot;, y = &#39;Time to Recurrence&#39;, title = glue(&quot;Preop PSA by baseline Gleason Score for RBC Age: {rbc_age_group}&quot;)) We would like to make this plot for each level of rbc age (rbc_age_group). Let’s walk through the process of how to build this into a function, and how to map it across all the values of rbc_age_group. First, let’s avoid hard-coding the value for rbc_age_group. Use .data$var for a variable in a column, and .env$var for an environment variable. Edit both sides of logic test in the filter statement in the code chunk below to eliminate hard coding the filter variable and the filter value. Then, check that this works for other values by changing the rbc_age_group environmental variable value to 2 or 3. rbc_age_group = 1 prostate %&gt;% filter(rbc_age_group == 1) %&gt;% ggplot() + aes(x = bgs, y = time_to_recurrence, fill = bgs) + geom_violin() + geom_jitter(width = 0.2) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = &quot;Baseline Gleason Score&quot;, y = &#39;Preoperative PSA&#39;, title = glue(&quot;Preop PSA by baseline Gleason Score for RBC Age: {rbc_age_group}&quot;)) Now make this into a function, named make_plot. Edit the chunk below to turn it into a function by wrapping the code in curly braces on the preceding and following line prefacing the first curly brace with function(argument) - in this case, the argument will be rbc_age_group. assigning the function to the name make_plot Then run make_plot(1) rbc_age_group = 1 { prostate %&gt;% filter(.data$rbc_age_group == .env$rbc_age_group) %&gt;% ggplot() + aes(x = bgs, y = time_to_recurrence, fill = bgs) + geom_violin() + geom_jitter(width = 0.2) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = &quot;Baseline Gleason Score&quot;, y = &#39;Preoperative PSA&#39;, title = glue(&quot;Preop PSA by baseline Gleason Score for RBC Age: {rbc_age_group}&quot;)) } Now, let’s automate calling the function, with a vector of the values of rbc_age_group, and running the function over each of these with map(vector, function). Edit the vector and function in the code chunk below to use the map function to create a list of plots. Hint - the vector is provided, and your new function name is make_plot. When it is working, simplify the plots1-3 by replacing them with walk(plots, print) rbc_age_group = 1:3 plots &lt;- map(vector, func) plots[[1]] plots[[2]] plots[[3]] Now let’s make this a more generalized function with 2 arguments - data, and rbc_age_group. Edit the code chunk below to create a new function, make_plot2. Remember to: wrap the code in curly braces on the preceding and following line preface the first curly brace with function(arguments) - in this case, the arguments will be data, rbc_age_group. assigning the function to the name make_plot2 Remove the filter step Replace the filter step with a separate (outside the function) filter step to filter for rbc_age_group ==1, that saves the result to data_prostate_1 Then run make_plot2 rbc_age_group = 1 make_plot2 &lt;- function(arg1, arg2){ prostate %&gt;% filter(.data$rbc_age_group == .env$rbc_age_group) %&gt;% ggplot() + aes(x = bgs, y = time_to_recurrence, fill = bgs) + geom_violin() + geom_jitter(width = 0.2) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = &quot;Baseline Gleason Score&quot;, y = &#39;Preoperative PSA&#39;, title = glue(&quot;Preop PSA by baseline Gleason Score for RBC Age: {rbc_age_group}&quot;)) } data_prostate_1 &lt;- prostate %&gt;% filter(rbc_age_group == 1) make_plot2(data_prostate_1, rbc_age_group = 1) Now let’s try to use make_plot in a tidy pipeline in the chunk below. In the chunk below, you want to nest the data by rbc_age_group, and map the make_plot function over the values of rbc_age_group. Edit both of the var values in the code chunk below to rbc_age_group. Run it to see the nested tibbles and nested plots in your new dataframe. prostate %&gt;% nest(data = -var) %&gt;% mutate(plots = map(var, make_plot)) Now pull out the plots and print them with pull and walk by editing the code chunk below. Pipe this code into the pull and walk functions by adding two lines of code. Remember that the argument for pull is the plotscolumn. Remember that the argument for walk is the print function. prostate %&gt;% nest(data = -rbc_age_group) %&gt;% mutate(plots = map(rbc_age_group, make_plot)) Now let’s make this even more generalizable with the make_plot2 functiion, which has 2 arguments, for data and rbc_age_group. Edit the code chunk below to replace arg1 with the data column, and func with make_plot2. See if this runs prostate %&gt;% nest(data = -rbc_age_group) %&gt;% mutate(plots = map2(arg1, rbc_age_group, func)) %&gt;% pull(plots) %&gt;% walk(print) This version of the pipeline should run for all values of rbc_age_group. 25.3 Freestyle Now try this on your own - and use the outline mode to jump back and forth to previous code chunks to make this easier. NOTE - need to FIX, Infert REMOVED 1. Make a plot of age (on the y axis) vs by parity (x axis) in the infert dataset with violin and jitter plots, as in the prostate example prostate-plot-1. This should be filtered for education == “0-5yrs”. Edit to make appropriate axis labels as appropriate. 2. Make a non-hard-coded version, by editing the filter statement with .data and .env variables, and edit the title with glue as in prostate-plot-2. 3. Make this into a function, make_infert_plots, as in prostate-plot-3. Try this with different values for education. 4. Set the levels of education with a vector as in prostate-plot 4, make plots with the map function, and print these out, with the walk function 5. Now make a more generalizable function, make_infert_plots2, by adding a data argument, as in prostate-plot-5. 6. Now nest the data by education, and mutate up some plots with make_infert_plots, as in prostate-plot-6 7. Then pull these plots and print them out with walk, as in prostate-plot-7 8. Now make a nested and generalizable version for all values of education, using your make_infert_plots2 function, as in prostate-plot-8 25.3.1 Acknowledgement This chapter is inspired by a lesson from Claus Wilke at https://wilkelab.org/SDS375/slides/functional-programming 25.4 Read More More on these topics can be found in R for Data Science - Chapter 19: Functions R for Data Science - Chapter 21.5: Map Functions Purrr::map documentation A blog post on how to use purrr:map to make plots for all 50 states and put them into a powerpoint presentation. "],["linear-regression-and-broom-for-tidying-models.html", "Chapter 26 Linear Regression and Broom for Tidying Models 26.1 Packages needed 26.2 Building a simple base model with {lm} 26.3 Is Your Model Valid? 26.4 Making Predictions with Your Model 26.5 Choosing predictors for multivariate modeling – testing, dealing with collinearity 26.6 presenting model results with RMarkdown 26.7 presenting model results with a Shiny App", " Chapter 26 Linear Regression and Broom for Tidying Models Linear regression allows you to: estimate the effects of predictors (independent variables) on an outcome (dependent variable), assuming that there is a linear relationship Make predictions about future cases (patients) with their measured predictors on this continuous outcome. Let’s look at a simple linear model to predict annual health care expenses. library(tidyverse) library(mlbench) library(broom) library(cutpointr) ## ## Attaching package: &#39;cutpointr&#39; ## The following object is masked from &#39;package:bayestestR&#39;: ## ## auc library(janitor) library(easystats) library(medicaldata) dm_data &lt;- data(&quot;PimaIndiansDiabetes2&quot;, package = &quot;mlbench&quot;) # build model, all variables dm_mod &lt;- glm(diabetes ~ ., data = PimaIndiansDiabetes2, family = &quot;binomial&quot;) # output summary(dm_mod) ## ## Call: ## glm(formula = diabetes ~ ., family = &quot;binomial&quot;, data = PimaIndiansDiabetes2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7823 -0.6603 -0.3642 0.6409 2.5612 ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -10.0407392 1.2176743 -8.246 ## pregnant 0.0821594 0.0554255 1.482 ## glucose 0.0382695 0.0057677 6.635 ## pressure -0.0014203 0.0118334 -0.120 ## triceps 0.0112214 0.0170837 0.657 ## insulin -0.0008253 0.0013064 -0.632 ## mass 0.0705376 0.0273421 2.580 ## pedigree 1.1409086 0.4274337 2.669 ## age 0.0339516 0.0183817 1.847 ## Pr(&gt;|z|) ## (Intercept) &lt; 0.0000000000000002 *** ## pregnant 0.13825 ## glucose 0.0000000000324 *** ## pressure 0.90446 ## triceps 0.51128 ## insulin 0.52757 ## mass 0.00989 ** ## pedigree 0.00760 ** ## age 0.06474 . ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 498.10 on 391 degrees of freedom ## Residual deviance: 344.02 on 383 degrees of freedom ## (376 observations deleted due to missingness) ## AIC: 362.02 ## ## Number of Fisher Scoring iterations: 5 #tidy version tidy(dm_mod) ## # A tibble: 9 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -10.0 1.22 -8.25 1.64e-16 ## 2 pregnant 0.0822 0.0554 1.48 1.38e- 1 ## 3 glucose 0.0383 0.00577 6.64 3.24e-11 ## 4 pressure -0.00142 0.0118 -0.120 9.04e- 1 ## 5 triceps 0.0112 0.0171 0.657 5.11e- 1 ## 6 insulin -0.000825 0.00131 -0.632 5.28e- 1 ## 7 mass 0.0705 0.0273 2.58 9.89e- 3 ## 8 pedigree 1.14 0.427 2.67 7.60e- 3 ## 9 age 0.0340 0.0184 1.85 6.47e- 2 # model performance glance(dm_mod) ## # A tibble: 1 × 8 ## null.deviance df.null logLik AIC BIC deviance ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 498. 391 -172. 362. 398. 344. ## # … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt; # augment data with fitted predictions and residuals dm_data_plus &lt;- augment(dm_mod) %&gt;% mutate(pct_prob = 100 * plogis(.fitted)) %&gt;% relocate(diabetes, .fitted, pct_prob) %&gt;% arrange(-.fitted) # select a cut point for classification cp &lt;- dm_data_plus %&gt;% cutpointr(pct_prob, diabetes, pos_class = &quot;pos&quot;, method= maximize_metric, metric = sum_sens_spec) ## Assuming the positive class has higher x values cp ## # A tibble: 1 × 16 ## direction optimal_cutpoint method sum_sens_spec ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &gt;= 28.5839 maximize_metric 1.57504 ## acc sensitivity specificity AUC pos_class ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.772959 0.830769 0.744275 0.862361 pos ## neg_class prevalence outcome predictor data ## &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 neg 0.331633 diabetes pct_prob &lt;tibble [392 × 2]&gt; ## roc_curve boot ## &lt;list&gt; &lt;lgl&gt; ## 1 &lt;roc_cutpointr [393 × 10]&gt; NA summary(cp) ## Method: maximize_metric ## Predictor: pct_prob ## Outcome: diabetes ## Direction: &gt;= ## ## AUC n n_pos n_neg ## 0.8624 392 130 262 ## ## optimal_cutpoint sum_sens_spec acc sensitivity ## 28.5839 1.575 0.773 0.8308 ## specificity tp fn fp tn ## 0.7443 108 22 67 195 ## ## Predictor summary: ## Data Min. 5% 1st Qu. Median Mean ## Overall 0.8690932 3.071251 8.953085 22.94296 33.16327 ## neg 0.8690932 2.674187 6.392249 13.48437 21.10577 ## pos 3.7635587 14.863216 34.854283 62.18036 57.46376 ## 3rd Qu. 95% Max. SD NAs ## 53.11714 88.92870 99.46861 28.45645 0 ## 28.96611 69.31582 97.91551 20.49784 0 ## 80.93884 92.17256 99.46861 26.71998 0 plot(cp) plot_metric(cp) # classify based on cut point dm_data_plus &lt;- dm_data_plus %&gt;% mutate(pred_yes_dm = case_when(pct_prob &gt; cp$optimal_cutpoint ~ &quot;pred_yes_dm&quot;, TRUE ~ &quot;pred_no&quot;)) %&gt;% relocate(pred_yes_dm, .after =pct_prob) # check confusion matrix dm_data_plus %&gt;% tabyl(diabetes, pred_yes_dm) %&gt;% adorn_totals(&quot;both&quot;) %&gt;% adorn_percentages() %&gt;% adorn_pct_formatting() ## diabetes pred_no pred_yes_dm Total ## neg 74.4% 25.6% 100.0% ## pos 17.7% 82.3% 100.0% ## Total 55.6% 44.4% 100.0% #check model performance performance::check_model(dm_mod, panel = FALSE) ## $PP_CHECK ## ## $BINNED_RESID ## ## $OUTLIERS ## ## $VIF ## ## $QQ # use panel = TRUE in Rmarkdown to get 2x3 panels for 6 plots # performance::model_performance(dm_mod) ## # Indices of model performance ## ## AIC | BIC | Tjur&#39;s R2 | RMSE | Sigma | Log_loss | Score_log | Score_spherical | PCP ## ---------------------------------------------------------------------------------------------- ## 362.021 | 397.763 | 0.364 | 0.376 | 0.948 | 0.439 | -74.015 | 0.009 | 0.718 #try a different model dm_mod2 &lt;- glm(diabetes ~ glucose + mass + pedigree + age, data = PimaIndiansDiabetes2, family = &quot;binomial&quot;) # build a really simple (NULL) model as a baseline dm_mod3 &lt;- glm(diabetes ~ 1, data = PimaIndiansDiabetes2, family = &quot;binomial&quot;) summary(dm_mod3) ## ## Call: ## glm(formula = diabetes ~ 1, family = &quot;binomial&quot;, data = PimaIndiansDiabetes2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.9265 -0.9265 -0.9265 1.4511 1.4511 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.62362 0.07571 -8.237 &lt;0.0000000000000002 ## ## (Intercept) *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 993.48 on 767 degrees of freedom ## Residual deviance: 993.48 on 767 degrees of freedom ## AIC: 995.48 ## ## Number of Fisher Scoring iterations: 4 # compare models # compare_performance(dm_mod, dm_mod2, dm_mod3, rank = TRUE) # plot(compare_performance(dm_mod, dm_mod2, dm_mod3, rank = TRUE)) + labs(subtitle = &quot;Larger Area is Better&quot;) # plot(compare_performance(dm_mod, dm_mod2, rank = TRUE)) + labs(subtitle = &quot;Larger Area is Better&quot;) #test_performance(dm_mod, dm_mod2, dm_mod3) # save model to RDS saveRDS(dm_mod, &quot;dm_mod.RDS&quot;) This is a simple web application that lets users who are not familiar with R use your model. They can enter values for the 3 predictor variables (gender, age, BMI) for a new patient, and predict their annual health insurance expenses in dollars. This web app produces a table with the inputs, and shows the output when you click the PREDICT button. Try it out here: Model Predictions Shiny Web App. Enter different predictor data points, and recalculate the expenses. You can click on the About tab for an explanation of the model, and even explore the publicly shared underlying code on GitHub (link in the About tab). We will walk through how to build, test, and share your own models in this chapter. 26.1 Packages needed {tidyverse} {medicaldata} {broom} {easystats} you can install this one (Not on CRAN) with install.packages(\"easystats\", repos = \"https://easystats.r-universe.dev\") {performance} {insight} {gtsummary} Note that the base modeling function lm() comes from the {stat} package, which loads by default when you start R. 26.2 Building a simple base model with {lm} The simplest model is called the null model, with no predictors. This model uses the mean value of the outcome to estimate it. In the blood_storage (prostate cancer) dataset in {medicaldata}, the mean time to recurrence is 32.92 months. To build a simple null model, you will need two main arguments to the lm() function: the formula the data The formula follows the format dependent_variable ~ independent_variables. Note that the data argument is not the first argument, so it does not automatically play well with pipes. You can pipe in data if you make the data argument explicit, and set it to data = . Let’s look at a simple example: Copy the code chunk below and run it in your RStudio Console. medicaldata::blood_storage %&gt;% lm(TimeToRecurrence ~ NULL, data = .) ## ## Call: ## lm(formula = TimeToRecurrence ~ NULL, data = .) ## ## Coefficients: ## (Intercept) ## 32.92 The output tells you the Call - the model being run, and then all the coefficients. In the case of the NULL model, the only coefficient is the intercept. This intercept is equal to the mean value of the outcome variable, TimeToRecurrence, in months. With no other predictor variables, this is the best estimate available for time to recurrence. We can also output the results as a nice tibble, using the tidy() function from the {broom} package. medicaldata::blood_storage %&gt;% lm(TimeToRecurrence ~ NULL, data = .) %&gt;% broom::tidy() ## # A tibble: 1 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 32.9 1.61 20.5 1.04e-59 This model has only one term, the intercept. It estimates every value of time to recurrence with the mean, 32.91. This is a pretty poor model, but is a place to start. Let’s look at how good this model is, using another function from the {broom} package. We can glance() our model, again output into a nice tibble. medicaldata::blood_storage %&gt;% lm(TimeToRecurrence ~ NULL, data = .) %&gt;% broom::glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 28.6 NA NA NA ## # … with 6 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, ## # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, ## # nobs &lt;int&gt; The r.squared and adj.r.squared are both 0, so we are capturing none of the variation in the data with this null model. The log likelihood is -1502, and the AIC 3009, and BIC 3016 (these are both high because this is a crummy model). AIC is Akaike’s Information Criterion, and estimates the out-of-sample prediction error and relative quality of a statistical model. A higher number indicates more information lost. Lower numbers for AIC = higher quality models. BIC is the Bayesian Information Criterion, which like AIC, penalizes models for the number of parameters to reduce overfitting. BIC also considers the number of observations in the data, which AIC does not. Lower values of BIC are better, and BIC is generally always higher than AIC, but absolute values do not matter, only relative values when comparing models on the same dataset for the same outcome. If we improve the model (with useful predictor variables), the BIC should go down. Let’s add some predictors: Age, TVol (tumor volume), and sGS (surgical Gleason score), and see if we do better. medicaldata::blood_storage %&gt;% lm(TimeToRecurrence ~ Age + TVol + sGS, data = .) %&gt;% broom::glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0130 0.00334 28.5 1.34 0.260 3 ## # … with 6 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, ## # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, ## # nobs &lt;int&gt; We are now explaining some (about 0.33% = 100*the adjusted R-squared) of the variation with this predictor, and the log likelihood (-1472) got closer to zero, and the AIC (2954) and BIC (2972) were reduced, showing that this is a better model than the NULL model (though still not great). 26.2.0.1 Key Takeaways use lm() to build the model argument: formula = outcome ~ predictor1 + predictor2 + … argument: data = . with the pipe tidy() from {broom} to see the model table of estimates glance() from {broom}to see measures of model accuracy 26.2.0.2 Your turn with licorice! Pipe the licorice data into an lm() function, with a formula argument and the data = . argument. Use the outcome of pacu30min_throatPain. Use predictors like intraOp_surgerySize, treat, preOp_pain, preOp_gender, and preOp_smoking. Then pipe the result into the function tidy() to see the model, and (separately) into the function glance() to evaluate the model quality. Copy the code chunk below into RStudio as a start. Use tidy() to see the model table, and glance() to look at the performance of the model. medicaldata::licorice_gargle %&gt;% lm(formula = pacu30min_throatPain ~ var1 + var2 + var3, data = .) Show the solution medicaldata::licorice_gargle %&gt;% lm(formula = pacu30min_throatPain ~ intraOp_surgerySize + treat + preOp_pain + preOp_gender + preOp_smoking, data = .) %&gt;% tidy() ## # A tibble: 6 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.376 0.365 1.03 0.304 ## 2 intraOp_surgerySize 0.316 0.143 2.21 0.0283 ## 3 treat -0.690 0.154 -4.48 0.0000117 ## 4 preOp_pain 1.96 0.835 2.35 0.0197 ## 5 preOp_gender -0.265 0.160 -1.65 0.100 ## 6 preOp_smoking 0.0652 0.0956 0.682 0.496 medicaldata::licorice_gargle %&gt;% lm(formula = pacu30min_throatPain ~ intraOp_surgerySize + treat + preOp_pain + preOp_gender + preOp_smoking, data = .) %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.143 0.124 1.17 7.59 0.00000130 5 ## # … with 6 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, ## # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, ## # nobs &lt;int&gt; 26.2.0.3 Interpreting the Model Estimates For the full model with 5 predictors (shown in the hidden solution above - Click the button to show it), you get a tidied tibble with 6 rows and 5 columns. These are The first column is the term - these include the (Intercept) and each of the predictors you called in the model. the second column is the estimate. This is the point estimate of the effect of each predictor in the multivariable model. For the intraOp_surgerySize predictor, this is 0.316. This means that for each unit or level increase of intraOp_surgerySize, which is defined on a 1-3 scale from small to large, the pacu_30min_throat pain (on a 0-10 scale), increases by 0.316 points. So a large surgery (2 levels larger than small) will result in, on average, a pacu_30min_throat pain score 0.632 points higher than a small surgery. similarly, the estimate for preop_gender is -0.265. This means that for each 1 point increase in the level of preop_gender, coded as 0=male, 1=female, the pacu_30min_throat pain score goes doen by -0.265 points. In this case, that means that the average pacu_30min_throat pain score in females was 0.265 points lower than in males. the std.error column tells you about the variance of this estimate, and can help you calculate confidence intervals around the point estimated if needed. the statistic is the t value for the estimate, which allows you to calculate p values with a t test. Values with a large absolute value (farther from zero) imply a stronger effect. Values of the statistic &gt; 1.96 (absolute value) correspond to a p value &lt; 0.05. the p value is the significance of the estimate for that particular predictor variable. Low values (often &lt; 0.05) are considered significant for traditional, historical reasons (it is an arbitrary cutoff). 26.2.0.3.1 Check your work: For the full model with 5 predictors, the which predictor variable has the largest estimated effect on pacu30min_throatpain? intraOp_surgerySizepreOp_paingenderFemalesmokingtreatLicorice For the full model with 5 predictors, what is the BIC value (using the glance function)? -3647653097417 26.2.0.4 Your turn with supra! Use the supraclavicular dataset to build a model with the outcome onset_sensory, with predictors (independent variables) age, bmi, gender, and group. Output the regression table with tidy() and the model measures with glance()  Copy the code chunk below into RStudio as a start medicaldata::supraclavicular %&gt;% lm(formula = onset_sensory ~ var1, data = .) %&gt;% glance() Show Solution medicaldata::supraclavicular %&gt;% lm(formula = onset_sensory ~ age + bmi + gender + group, data = .) %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0356 -0.00504 11.5 0.876 0.481 4 ## # … with 6 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, ## # BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, ## # nobs &lt;int&gt; 26.2.0.4.1 For the full model with 4 predictors, what is the Adjusted R squared value? 0.036301.7-383.30.48-0.005 26.2.1 Producing manuscript-quality tables with {gtsummary} Let’s take your model above, and rather than pipe it into tidy() or glance(), pipe it into the tbl_regression() function from the {gtsummary} package. medicaldata::supraclavicular %&gt;% lm(formula = onset_sensory ~ age + bmi + gender + group, data = .) %&gt;% tbl_regression() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #xeujopedat .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #xeujopedat .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xeujopedat .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #xeujopedat .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #xeujopedat .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xeujopedat .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xeujopedat .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #xeujopedat .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #xeujopedat .gt_column_spanner_outer:first-child { padding-left: 0; } #xeujopedat .gt_column_spanner_outer:last-child { padding-right: 0; } #xeujopedat .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #xeujopedat .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #xeujopedat .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #xeujopedat .gt_from_md > :first-child { margin-top: 0; } #xeujopedat .gt_from_md > :last-child { margin-bottom: 0; } #xeujopedat .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #xeujopedat .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #xeujopedat .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #xeujopedat .gt_row_group_first td { border-top-width: 2px; } #xeujopedat .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xeujopedat .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #xeujopedat .gt_first_summary_row.thick { border-top-width: 2px; } #xeujopedat .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xeujopedat .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xeujopedat .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #xeujopedat .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #xeujopedat .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xeujopedat .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xeujopedat .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #xeujopedat .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xeujopedat .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #xeujopedat .gt_left { text-align: left; } #xeujopedat .gt_center { text-align: center; } #xeujopedat .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #xeujopedat .gt_font_normal { font-weight: normal; } #xeujopedat .gt_font_bold { font-weight: bold; } #xeujopedat .gt_font_italic { font-style: italic; } #xeujopedat .gt_super { font-size: 65%; } #xeujopedat .gt_two_val_uncert { display: inline-block; line-height: 1em; text-align: right; font-size: 60%; vertical-align: -0.25em; margin-left: 0.1em; } #xeujopedat .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #xeujopedat .gt_asterisk { font-size: 100%; vertical-align: 0; } #xeujopedat .gt_slash_mark { font-size: 0.7em; line-height: 0.7em; vertical-align: 0.15em; } #xeujopedat .gt_fraction_numerator { font-size: 0.6em; line-height: 0.6em; vertical-align: 0.45em; } #xeujopedat .gt_fraction_denominator { font-size: 0.6em; line-height: 0.6em; vertical-align: -0.05em; } Characteristic Beta 95% CI1 p-value age -0.02 -0.20, 0.15 0.8 bmi 0.00 -0.39, 0.38 >0.9 gender 3.0 -1.9, 7.8 0.2 group 2.4 -2.2, 7.1 0.3 1 CI = Confidence Interval This produces a nice looking table, suitable for Rmarkdown documents, with output to Word or Powerpoint. You can even convert this to other formats: to a tibble with as_tibble() to a gt object with as_gt() then use gt formatting to a flextable object with as_flextable() then add formatting with flextable 26.2.1.1 Takeaways for Linear Modeling start with data - pipe into model lm(formula, data = .) model can be piped into tidy() for an estimates table model can be piped into glance() for measures of the model model can be piped into tbl_regression() for a publication-quality table 26.3 Is Your Model Valid? Key assumptions of linear regression Homogeneity of variance (homoscedasticity): The error variance should be constant Linearity: the relationships between the predictors and the outcome variable should be linear Independence: The errors associated with one observation are not correlated with the errors of any other observation Normality: the errors should be normally distributed. Technically normality is necessary only for hypothesis tests to be valid. These assumption can be checked easily with the {performance} package in the {easystats} meta-package ({tidyverse} is another meta-package of packages). In the code chunk below, we assign the model to the object name supra_model, and then run check_model from the {performance} package. supra_model &lt;- medicaldata::supraclavicular %&gt;% lm(formula = onset_sensory ~ age + bmi + gender + group, data = .) performance::check_model(supra_model, panel = FALSE) ## $PP_CHECK ## ## $NCV ## ## $HOMOGENEITY ## ## $OUTLIERS ## ## $VIF ## ## $QQ This produces a nice set of six plots in the Plots tab) with some guidance in the subtitles on how to interpret the plots. There is a less pretty version in base R, using plot(model_name), which also works to produce four of these 6 plots. You can also formally test for heteroscedasticity. The variance of your residuals should be homogenous. check_heteroscedasticity(supra_model) ## OK: Error variance appears to be homoscedastic (p = 0.427). A green output that starts with OK for check_heteroscedasticity, indicating homoscedasticity (homgeneous residual variance), is good. 26.4 Making Predictions with Your Model We can use the linear model to make predictions about the individual observations in our data, or in future data. Let’s start with adding model predictions to each observation in our dataset. This is often called the training data as it was the data the model was trained on. You can add predictions (fitted results) to your dataframe with the augment() function from the {broom} package. We augment this dataframe with the model predictions, and then relocate them to the beginning (leftmost columns) of the tibble. supra_data_plus &lt;- augment(supra_model) %&gt;% relocate(onset_sensory, .fitted, .resid) %&gt;% arrange(-.fitted) supra_data_plus ## # A tibble: 100 × 12 ## onset_sensory .fitted .resid .rownames age bmi gender ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 19 16.5 2.46 52 18 22.1 1 ## 2 10 16.5 -6.51 99 19 24.4 1 ## 3 14 16.5 -2.50 24 19 25.1 1 ## 4 4 16.5 -12.5 6 21 22.0 1 ## 5 8 16.3 -8.30 87 28 30.4 1 ## 6 9 16.3 -7.27 74 31 21.0 1 ## 7 6 16.3 -10.3 19 28 39.8 1 ## 8 39 16.2 22.8 39 31 29.6 1 ## 9 38 16.2 21.8 48 32 24.4 1 ## 10 3 16.2 -13.2 43 32 35.4 1 ## # … with 90 more rows, and 5 more variables: group &lt;dbl&gt;, ## # .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, ## # .std.resid &lt;dbl&gt; The dataframe supra_data_plus includes a prediction of the outcome (.fitted) for each observation. We can compare these predictions to the outcome (onset_sensory) and see how the residuals (.resid) are calculated (onset_sensory minus .fitted). The Cook’s D variable (.cooksd) is a measure of how large the effect on the model would be if you deleted that particular observation. Large values for Cook’s distance sugest that these observations are outliers that pull the model in one direction (have high leverage), and indicate an influential data point. Review any observation with a .cooksd &gt; 1 carefully. 26.4.1 Predictions from new data You can also input new observations (in a data frame) to the model, and predict the outcome for these observations. First, we need to create a dataframe that matches the predictor variables for the supra_model. You might get a dataframe of new observations from a colleague. It is important that this is in the same format, with exactly the same variable names as the original data. new_data &lt;- tibble(age = c(27, 38, 51), bmi = c(30.4, 34.2, 41.1), gender = c(2, 1, 1), group = c(2, 1, 2)) new_data ## # A tibble: 3 × 4 ## age bmi gender group ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 27 30.4 2 2 ## 2 38 34.2 1 1 ## 3 51 41.1 1 2 To make predictions with the new data, you use the base {stats} function predict(), with arguments for the model, and the new data. predict(object = supra_model, newdata = new_data) ## 1 2 3 ## 19.27186 13.62400 15.77457 This gives us predictions for each of the 3 rows of the new_data dataframe, of the outcome onset_sensory. 26.5 Choosing predictors for multivariate modeling – testing, dealing with collinearity interactions 26.5.1 Challenges 26.6 presenting model results with RMarkdown 26.6.1 Challenges 26.7 presenting model results with a Shiny App 26.7.1 Challenges "],["logistic-regression-and-broom-for-tidying-models.html", "Chapter 27 Logistic Regression and Broom for Tidying Models 27.1 The Model Summary 27.2 Evaluating your Model Assumptions 27.3 Converting between logit, odds ratios, and probability", " Chapter 27 Logistic Regression and Broom for Tidying Models Logistic regression allows you to: Estimate the effects of predictors (independent variables) on an dichotomous outcome (dependent variable), like alive/dead, remission/not in remission. Make predictions about future cases (patients) with their measured predictors on this continuous outcome. Let’s look at a simple logistic model to predict recurrence in prostate cancer. prostate &lt;- medicaldata::blood_storage %&gt;% janitor::clean_names() prostate %&gt;% glm(formula = recurrence ~ fam_hx + b_gs, data = ., family = binomial) -&gt; prostate_model prostate_model ## ## Call: glm(formula = recurrence ~ fam_hx + b_gs, family = binomial, ## data = .) ## ## Coefficients: ## (Intercept) fam_hx b_gs ## -3.4485 -0.8983 1.1839 ## ## Degrees of Freedom: 313 Total (i.e. Null); 311 Residual ## (2 observations deleted due to missingness) ## Null Deviance: 281.9 ## Residual Deviance: 246.8 AIC: 252.8 We use the glm() function from the base stats package, which is for generalized linear model. This function can use a variety of model families, including logistic, poisson, gamma, quasibinomial, gamma, etc., and the family of models needs to be specified in the family argument. The formula for the dependent variable (outcome) ~ independent variables (predictors) is the same as with linear modeling with the lm() function. We specify the dataset with the data argument, and when we the pipe, we set data = .. When you print out the prostate model, you get the Call (the glm function and arguments) the coefficients for the intercept and each predictor the degrees of freedom how many observations were deleted due to missingness (IMPORTANT, DO NOT BLOW BY THIS), two values for Deviance and AIC. Let’s walk through what these mean: the coefficients estimate how much a change of one unit in each predictor will affect the outcome (in logit units - more about this later). The degrees of freedom are related to the number of observations, and how many predictors you have used. If you look at the mean value in the prostate dataset for recurrence, it is 0.1708861, which means that 17% of the participants experienced a recurrence of prostate cancer. If you are calculating the mean of 315 of the 316 observations, and you know the overall mean of all 315, you (mathematically) know the value of the last observation - recurrence or not - it has no degrees of freedom. So for 316 observations, you have n-1 or 315, degrees of freedom. For each predictor in your model you ‘use up’ one degree of freedom. The degrees of freedom affect the significance of the test statistic (T, or chi-squared, or F statistic). how many observations were deleted due to missingness - the logistic model will only work on complete cases, so if one of your predictors or the outcome is frequently missing, your effective dataset size will shrink rapidly. You want to know if this is an issue, as this might change which predictors you use (avoid frequently missing ones), or lead you to consider imputation of missing values. Null Deviance and Residual Deviance. The null deviance is measured for the null model, with only an intercept. The residual deviance is measured for your model with predictors. Your residual deviance should be lower than the null deviance. You can even measure whether your model is significantly better than the null model by calculating the difference between the Null Deviance and the Residual Deviance. This difference [281.9 - 246.8 = 35.1] has a chi-square distribution. You can look up the value for chi-square with 2 degrees (because you had 2 predictors) of freedom. Or you can calculate this in R with pchisq(q = 35.1, df=2, lower.tail = TRUE) which gives you a p value of 1. The AIC is Aikaike’s Information Criterion, which estimates prediction error. A lower values is better when comparing similar models. 27.1 The Model Summary You can get more information from a summary of the model. summary(prostate_model) ## ## Call: ## glm(formula = recurrence ~ fam_hx + b_gs, family = binomial, ## data = .) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.2216 -0.5089 -0.4446 -0.2879 2.5315 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.4485 0.4347 -7.932 0.00000000000000215 ## fam_hx -0.8983 0.4785 -1.877 0.0605 ## b_gs 1.1839 0.2193 5.399 0.00000006698872947 ## ## (Intercept) *** ## fam_hx . ## b_gs *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 281.88 on 313 degrees of freedom ## Residual deviance: 246.81 on 311 degrees of freedom ## (2 observations deleted due to missingness) ## AIC: 252.81 ## ## Number of Fisher Scoring iterations: 5 Again, you get the call (to orient you to which model). The Deviance Residuals should have a Median near zero, and be roughly symmetric around zero. If the median is close to zero, the model is not biased in one direction (the outcome is not over- nor under-estimated). Now, in addition to estimates, we get standard errors for each estimate (which can be used to calculate confidence intervals), z statistic values for each predictor, and the resulting p value (calculated with the statistic and the degrees of freedom). As a general rule of thumb, a z value with an absolute value of &gt; 1.96 should have a p value less than 0.05, and an absolute value &gt; 2.576 should have a p value of less than 0.01. These values should sound familiar from the normal distribution (95% and 99% confidence interval Z values). 27.2 Evaluating your Model Assumptions You can do this in base R with plot(model), but there is a prettier version in the {performance} package in the {easystats} meta-package. Just run the check_model() function on your model. You can set the argument panel = TRUE for a multipanel figure, or panel = FALSE for single figures in the Plots tab. Try it both ways to see which you prefer. If the multipanel seems too small, click on the Zoom button in the Plots tab to make it bigger. check_model(prostate_model, panel = FALSE) ## $PP_CHECK ## ## $BINNED_RESID ## ## $OUTLIERS ## ## $VIF ## ## $QQ This generates graphs with nice subtitles to help you interpret the output. Big deviations should make you worry about one or more of the model assumptions, and may require rescaling one of your predictors. If all is well, you want to look at how your model predictors actually predict the outcome. You make a nicer looking regression table with the tidy() function from the {broom} package. prostate_model %&gt;% broom::tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -3.45 0.435 -7.93 2.15e-15 ## 2 fam_hx -0.898 0.478 -1.88 6.05e- 2 ## 3 b_gs 1.18 0.219 5.40 6.70e- 8 This model has 3 terms: an intercept, and two predictors. The family history predictor (fam_hx) is not significant, but trends toward an association with a decreased odds of recurrence, while the baseline Gleason score (b_gs) is significant and is associated with an 18% increased log-odds of recurrence for each extra point in the Gleason score. Note that this is expressed in logit, or log-odds, not probability, which can take some finagling to get to percentages and probabilities. A positive estimate indicates that increasing that predictor will be associated with increasing odds of the outcome. Conversely, a negative estimate indicates that increasing that predictor will be associated with decreasing odds of the outcome. The quantity log[p/(1-p)] is called the logarithm of the odds, also known as the log-odds or logit. Despite this being commonly written as “log”, it is not base 10 logarithms, but the natural log, with the base e (2.718…). Why would anyone use logit? It is really hard to model zeroes and ones (dichotomous outcomes). The logit is a link function, or a way to convert zeroes and ones to a continuous function that does not cross zero or one. Once you have a continuous function, you can use generalized linear models to model it. Why not just model probability? For complicated mathematical reasons, it was easier to convert probabilities to odds and then take the natural log. There will be times when using logistic regression that it will be fairly painful to convert between probabilities and odds and logit units. But R has functions to do that for us. We just have to watch out for when we have probabilities vs. odds vs. logit. One way is to look at the range of the estimates. Probabilities always have a range from zero to 1. Logit units generally range from about -4 to +4, with zero meaning an equal probability of no event or the event outcome occurring. Odds ratios can range from very small (but positive) numbers to very large positive numbers. You can see this by re-running broom::tidy() with the exp = TRUE option. This will exponentiate the logit, or log-odds estimates, to give us the estimates as odds ratios. prostate_model %&gt;% broom::tidy(exp = TRUE) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.0318 0.435 -7.93 2.15e-15 ## 2 fam_hx 0.407 0.478 -1.88 6.05e- 2 ## 3 b_gs 3.27 0.219 5.40 6.70e- 8 Now the estimates are all positive, with the previously most negative (intercept was logit -3.45) now being a small positive number of 0.0318, and the most positive (b_gs was logit 1.18) now a larger positive odds ratio of 3.27. These odds ratios versions of the estimates are more easily interpretable than logit scores. Odds ratios of less than one means that an increase in that predictor makes the outcome less likely to occur, and an odds ratio of greater than one means that an increase in that predictor makes the outcome more likely to occur. For example, the odds ratio estimate of 0.407 means that for someone with a positive family history of prostate cancer, the odds of their having a recurrence are 59.3% ((1-0.407) x 100) lower than someone without a family history. Similarly, for each unit increase in the baseline Gleason score (b_gs), the odds of recurrence increase by 227% ((3.27-1) x 100). 27.3 Converting between logit, odds ratios, and probability Let’s step back and try a real-world example. If the percent probability of snow on january 10th is 72%, then p, the probability is 0.72. The probability of no snow (1-p) is 1-0.72 = 0.28. The odds of snow are p/(1-p) = 0.72/0.28 = 2.57. If we take the natural log of these odds, this gives us an estimate in logit terms, with ln(2.57) = 0.944. This estimate in logit units is what you get as a default estimate from logistic regression. To convert these logit estimates back to probability, you need to do the reverse. First, exponentiate the logit estimate of 0.944, exp(0.944) = 2.57. This is the odds ratio. To convert odds to probability - calculate odds / (1 + odds) = 2.57/3.57 = 0.72, which is the probability. To get the percent probability, you can multiply the probability by 100 to get 72%. Fortunately, R has functions to help us do this sort of conversion. You just have to be able to recognize which units (logit, odds, or probability) that you are looking at. We can look at the overall quality of the model with the glance() function in {broom}. Let’s look at 2 versions of the model, one with fam_hx only, and one with both predictors. prostate %&gt;% glm(recurrence ~ fam_hx, data = ., family = binomial) %&gt;% broom::glance() ## # A tibble: 1 × 8 ## null.deviance df.null logLik AIC BIC deviance ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 289. 315 -142. 288. 296. 284. ## # … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt; prostate %&gt;% glm(recurrence ~ fam_hx + b_gs, data = ., family = binomial) %&gt;% broom::glance() ## # A tibble: 1 × 8 ## null.deviance df.null logLik AIC BIC deviance ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 282. 313 -123. 253. 264. 247. ## # … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt; You can see that adding the baseline Gleason score improves the model, as it lowers both AIC and BIC. This is not surprising, as it was a significant predictor. You can add predicted (fitted) values and residuals for each observation in your dataset with broom::augment() model &lt;- prostate %&gt;% glm(recurrence ~ fam_hx + b_gs, data = ., family = binomial) augment(model) ## # A tibble: 314 × 10 ## .rownames recurrence fam_hx b_gs .fitted .resid ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 3 0.103 1.13 ## 2 2 1 0 2 -1.08 1.66 ## 3 3 0 0 3 0.103 -1.22 ## 4 4 0 0 1 -2.26 -0.445 ## 5 5 0 0 2 -1.08 -0.764 ## 6 6 0 0 1 -2.26 -0.445 ## 7 7 0 0 1 -2.26 -0.445 ## 8 8 1 0 1 -2.26 2.17 ## 9 9 0 0 1 -2.26 -0.445 ## 10 10 0 0 2 -1.08 -0.764 ## # … with 304 more rows, and 4 more variables: ## # .std.resid &lt;dbl&gt;, .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, ## # .cooksd &lt;dbl&gt; Note that the fitted data are both positive and negative, with a range within +/- 4. This should tell you that they are in logit (log-odds) units (ln(p/1-p)), in which 0 is a 50% probability of either outcome. We can do a variety of other things with this model in base R. Let’s look at a few. If you just want the coefficients and a 95% confidence interval for each (in logit units), you can use the coef() and the confint() functions. coef(model) # estimated coefficients for predictors ## (Intercept) fam_hx b_gs ## -3.4485111 -0.8982539 1.1839261 confint(model) # 95% confidence interval for coefficients ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) -4.3420982 -2.63133270 ## fam_hx -1.9305806 -0.02633492 ## b_gs 0.7619884 1.62560896 If you would prefer these as odds ratios, you can exponentiate these. exp(coef(model)) # show the odds ratios, rather than log-odds for coefficients ## (Intercept) fam_hx b_gs ## 0.03179294 0.40728019 3.26717619 exp(confint(model)) # 95% confidence interval for coefficients, now expressed as odds ratios ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.0130092 0.07198247 ## fam_hx 0.1450639 0.97400882 ## b_gs 2.1425321 5.08151255 Notice that these are all now greater than zero, rather than from -4 to 4. You can also use functions to make predictions (fitted) from your data, or even from new data. The default output is in logit units, but if you use the “response” type argument in the predict() function, you get probabilities on a zero to 1 scale. predict(model) %&gt;% head() # predictions for each observation ## 1 2 3 4 5 ## 0.1032671 -1.0806590 0.1032671 -2.2645850 -1.0806590 ## 6 ## -2.2645850 # in the prostate dataset on the logit scale # from around -4 (very unlikely to have event) # to +4 (very likely to have event) predict(model, type = &quot;response&quot;) %&gt;% head() # predictions for each observation in ## 1 2 3 4 5 ## 0.52579385 0.25338133 0.52579385 0.09409879 0.25338133 ## 6 ## 0.09409879 # the prostate dataset using 0-1 probabilities # (can multiply by 100 to get percent probability # if you prefer) You can then classify these probabilities as likely (&gt;0.5) or unlikely (&lt;=0.5) and compare these class predictions to the true recurrence outcomes. By calculating the mean of the observations that match, you can calculate an overall accuracy of your classification model. probabilities &lt;- predict(model, type = &quot;response&quot;) predicted.classes &lt;- ifelse(probabilities &gt; 0.5, 0, 1) # predictions as pos or neg mean(predicted.classes == prostate$recurrence) ## Warning in predicted.classes == prostate$recurrence: longer ## object length is not a multiple of shorter object length ## [1] 0.193038 # calcuated accuracy You can even make predictions on new data (in this case, a random 3% sample of the original dataset) predict(model, newdata = slice_sample(prostate, prop = 0.03), type = &quot;response&quot;) ## 1 2 3 4 5 ## 0.52579385 0.25338133 0.09409879 0.04058836 0.12143477 ## 6 7 8 9 ## 0.09409879 0.52579385 0.04058836 0.25338133 Let’s see how this works with another dataset, from which we will use predictors to classify diabetes cases. We will start by loading the data into dm_data, and building an “all predictors” model, by specifying the formula predictors as “.” - this means to use all other variables (except the outcome variable) as predictors. Look at the model output for problems. data(&quot;PimaIndiansDiabetes2&quot;, package = &quot;mlbench&quot;) dm_data &lt;- PimaIndiansDiabetes2 rm(PimaIndiansDiabetes2) # build model, with all variables dm_mod &lt;- glm(diabetes ~ ., data = dm_data, family = &quot;binomial&quot;) dm_mod ## ## Call: glm(formula = diabetes ~ ., family = &quot;binomial&quot;, data = dm_data) ## ## Coefficients: ## (Intercept) pregnant glucose pressure ## -10.0407392 0.0821594 0.0382695 -0.0014203 ## triceps insulin mass pedigree ## 0.0112214 -0.0008253 0.0705376 1.1409086 ## age ## 0.0339516 ## ## Degrees of Freedom: 391 Total (i.e. Null); 383 Residual ## (376 observations deleted due to missingness) ## Null Deviance: 498.1 ## Residual Deviance: 344 AIC: 362 Did you notice that 376 observations were deleted due to missingness? We can use the vis_miss() function from the {visdat} package to figure out which are the problem variables. visdat::vis_miss(dm_data) ## Warning: `gather_()` was deprecated in tidyr 1.2.0. ## Please use `gather()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. It looks like triceps and insulin measurements were missing fairly often. Would a model without these measures be better? Let’s try. dm_mod_miss &lt;- glm(diabetes ~ glucose + pressure + mass + pedigree + age, data = dm_data, family = &quot;binomial&quot;) dm_mod_miss ## ## Call: glm(formula = diabetes ~ glucose + pressure + mass + pedigree + ## age, family = &quot;binomial&quot;, data = dm_data) ## ## Coefficients: ## (Intercept) glucose pressure mass ## -9.014890 0.034567 -0.007433 0.088641 ## pedigree age ## 0.923290 0.034523 ## ## Degrees of Freedom: 723 Total (i.e. Null); 718 Residual ## (44 observations deleted due to missingness) ## Null Deviance: 931.9 ## Residual Deviance: 685.7 AIC: 697.7 Apparently not. Even with all the missing data, the AIC of the reduced model is 697.7, and the AIC of the full model was 362. This suggests that insulin and triceps measurements are pretty helpful in predicting diabetes. Let’s look at how well the full model works, with our usual battery of model functions. # output summary(dm_mod) ## ## Call: ## glm(formula = diabetes ~ ., family = &quot;binomial&quot;, data = dm_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7823 -0.6603 -0.3642 0.6409 2.5612 ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -10.0407392 1.2176743 -8.246 ## pregnant 0.0821594 0.0554255 1.482 ## glucose 0.0382695 0.0057677 6.635 ## pressure -0.0014203 0.0118334 -0.120 ## triceps 0.0112214 0.0170837 0.657 ## insulin -0.0008253 0.0013064 -0.632 ## mass 0.0705376 0.0273421 2.580 ## pedigree 1.1409086 0.4274337 2.669 ## age 0.0339516 0.0183817 1.847 ## Pr(&gt;|z|) ## (Intercept) &lt; 0.0000000000000002 *** ## pregnant 0.13825 ## glucose 0.0000000000324 *** ## pressure 0.90446 ## triceps 0.51128 ## insulin 0.52757 ## mass 0.00989 ** ## pedigree 0.00760 ** ## age 0.06474 . ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 498.10 on 391 degrees of freedom ## Residual deviance: 344.02 on 383 degrees of freedom ## (376 observations deleted due to missingness) ## AIC: 362.02 ## ## Number of Fisher Scoring iterations: 5 # test model assumptions check_model(dm_mod) # tidy version of estimates tidy(dm_mod) ## # A tibble: 9 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -10.0 1.22 -8.25 1.64e-16 ## 2 pregnant 0.0822 0.0554 1.48 1.38e- 1 ## 3 glucose 0.0383 0.00577 6.64 3.24e-11 ## 4 pressure -0.00142 0.0118 -0.120 9.04e- 1 ## 5 triceps 0.0112 0.0171 0.657 5.11e- 1 ## 6 insulin -0.000825 0.00131 -0.632 5.28e- 1 ## 7 mass 0.0705 0.0273 2.58 9.89e- 3 ## 8 pedigree 1.14 0.427 2.67 7.60e- 3 ## 9 age 0.0340 0.0184 1.85 6.47e- 2 # model performance glance(dm_mod) ## # A tibble: 1 × 8 ## null.deviance df.null logLik AIC BIC deviance ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 498. 391 -172. 362. 398. 344. ## # … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt; OK, let’s make some predictions, and convert these to percent probability (0-100 range). # augment data with fitted predictions and residuals dm_data_plus &lt;- augment(dm_mod) %&gt;% mutate(pct_prob = 100 * plogis(.fitted)) %&gt;% relocate(diabetes, .fitted, pct_prob) %&gt;% arrange(-.fitted) dm_data_plus ## # A tibble: 392 × 17 ## diabetes .fitted pct_prob .rownames pregnant glucose ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pos 5.23 99.5 446 0 180 ## 2 neg 3.85 97.9 229 4 197 ## 3 pos 3.61 97.4 547 5 187 ## 4 pos 3.37 96.7 207 8 196 ## 5 pos 3.27 96.3 160 17 163 ## 6 neg 3.18 96.0 488 0 173 ## 7 pos 3.02 95.3 44 9 171 ## 8 pos 2.86 94.6 371 3 173 ## 9 neg 2.58 93.0 745 13 153 ## 10 pos 2.50 92.4 260 11 155 ## # … with 382 more rows, and 11 more variables: ## # pressure &lt;dbl&gt;, triceps &lt;dbl&gt;, insulin &lt;dbl&gt;, ## # mass &lt;dbl&gt;, pedigree &lt;dbl&gt;, age &lt;dbl&gt;, .resid &lt;dbl&gt;, ## # .std.resid &lt;dbl&gt;, .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, ## # .cooksd &lt;dbl&gt; # arrange puts the high-probability cases first set.seed(1234) dm_data_plus %&gt;% slice_sample(n=10) %&gt;% select(diabetes:.rownames) ## # A tibble: 10 × 4 ## diabetes .fitted pct_prob .rownames ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 neg -2.16 10.4 624 ## 2 neg -2.80 5.75 225 ## 3 neg 0.0250 50.6 609 ## 4 neg -0.158 46.1 192 ## 5 pos -0.491 38.0 486 ## 6 pos 0.146 53.6 697 ## 7 neg 0.00702 50.2 646 ## 8 neg -1.38 20.1 182 ## 9 neg 0.355 58.8 658 ## 10 neg -2.69 6.37 761 # a random sample of 10 You can see the relationship between the fitted values (on the logit scale) and the percent probability of diabetes. Some of the high-probability folks are still negative (rowname 609), while others with lower predicted probability alread have diabetes (rowname 486). A fancier way to classify based on your predictions is to pick an optimal cutpoint, with the {cutpointr} package. You can do this with a number of different metrics you can choose, and different methods. The code chunk below demonstrates some of the helpful output from {cutpointr}. There are 15 different methods, and 20 differnt metric options to choose from, at this website. Here we start with maximize_metric and sum_sens_spec. # select a cut point for classification cp &lt;- dm_data_plus %&gt;% cutpointr(pct_prob, diabetes, pos_class = &quot;pos&quot;, method= maximize_metric, metric = sum_sens_spec) ## Assuming the positive class has higher x values cp ## # A tibble: 1 × 16 ## direction optimal_cutpoint method sum_sens_spec ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &gt;= 28.5839 maximize_metric 1.57504 ## acc sensitivity specificity AUC pos_class ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.772959 0.830769 0.744275 0.862361 pos ## neg_class prevalence outcome predictor data ## &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 neg 0.331633 diabetes pct_prob &lt;tibble [392 × 2]&gt; ## roc_curve boot ## &lt;list&gt; &lt;lgl&gt; ## 1 &lt;roc_cutpointr [393 × 10]&gt; NA summary(cp) ## Method: maximize_metric ## Predictor: pct_prob ## Outcome: diabetes ## Direction: &gt;= ## ## AUC n n_pos n_neg ## 0.8624 392 130 262 ## ## optimal_cutpoint sum_sens_spec acc sensitivity ## 28.5839 1.575 0.773 0.8308 ## specificity tp fn fp tn ## 0.7443 108 22 67 195 ## ## Predictor summary: ## Data Min. 5% 1st Qu. Median Mean ## Overall 0.8690932 3.071251 8.953085 22.94296 33.16327 ## neg 0.8690932 2.674187 6.392249 13.48437 21.10577 ## pos 3.7635587 14.863216 34.854283 62.18036 57.46376 ## 3rd Qu. 95% Max. SD NAs ## 53.11714 88.92870 99.46861 28.45645 0 ## 28.96611 69.31582 97.91551 20.49784 0 ## 80.93884 92.17256 99.46861 26.71998 0 plot(cp) plot_metric(cp) You can then use the cutpoint to classify observations, and see how accurate your model is. # classify based on cut point dm_data_plus &lt;- dm_data_plus %&gt;% mutate(pred_dm = case_when(pct_prob &gt; cp$optimal_cutpoint ~ &quot;pred_yes&quot;, pct_prob &lt;= cp$optimal_cutpoint ~ &quot;pred_no&quot;)) %&gt;% mutate(pred_dm = factor(pred_dm, levels = c(&quot;pred_no&quot;, &quot;pred_yes&quot;))) %&gt;% relocate(pred_dm, .after = pct_prob) # check confusion matrix dm_data_plus %&gt;% tabyl(diabetes, pred_dm) %&gt;% adorn_totals(&quot;both&quot;) %&gt;% adorn_percentages() %&gt;% adorn_pct_formatting() ## diabetes pred_no pred_yes Total ## neg 74.4% 25.6% 100.0% ## pos 17.7% 82.3% 100.0% ## Total 55.6% 44.4% 100.0% You can also check model assumptions, and model performance, even against competing models. Let’s build some competing models below. #check model assumptions performance::check_model(dm_mod, panel = FALSE) ## $PP_CHECK ## ## $BINNED_RESID ## ## $OUTLIERS ## ## $VIF ## ## $QQ # use panel = TRUE in Rmarkdown # to get 2x3 panels for 6 plots # performance::model_performance(dm_mod) ## # Indices of model performance ## ## AIC | BIC | Tjur&#39;s R2 | RMSE | Sigma | Log_loss | Score_log | Score_spherical | PCP ## ---------------------------------------------------------------------------------------------- ## 362.021 | 397.763 | 0.364 | 0.376 | 0.948 | 0.439 | -74.015 | 0.009 | 0.718 #try a simpler model dm_mod2 &lt;- glm(diabetes ~ glucose + mass + pedigree, data = dm_data, family = &quot;binomial&quot;) tidy(dm_mod2) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -8.46 0.668 -12.7 8.65e-37 ## 2 glucose 0.0379 0.00347 10.9 9.62e-28 ## 3 mass 0.0810 0.0142 5.69 1.27e- 8 ## 4 pedigree 0.867 0.296 2.93 3.40e- 3 glance(dm_mod2) ## # A tibble: 1 × 8 ## null.deviance df.null logLik AIC BIC deviance ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 975. 751 -365. 738. 756. 730. ## # … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt; # build a really simple (NULL) model as a baseline dm_mod3 &lt;- glm(diabetes ~ 1, data = dm_data, family = &quot;binomial&quot;) summary(dm_mod3) ## ## Call: ## glm(formula = diabetes ~ 1, family = &quot;binomial&quot;, data = dm_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.9265 -0.9265 -0.9265 1.4511 1.4511 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.62362 0.07571 -8.237 &lt;0.0000000000000002 ## ## (Intercept) *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 993.48 on 767 degrees of freedom ## Residual deviance: 993.48 on 767 degrees of freedom ## AIC: 995.48 ## ## Number of Fisher Scoring iterations: 4 # compare models # compare_performance(dm_mod, dm_mod2, dm_mod3, rank = TRUE) # plot(compare_performance(dm_mod, dm_mod2, dm_mod3, rank = TRUE)) + labs(subtitle = &quot;Larger Area is Better&quot;) # plot(compare_performance(dm_mod, dm_mod2, rank = TRUE)) + labs(subtitle = &quot;Larger Area is Better&quot;) # save model to RDS for later use in predictions or web apps. saveRDS(dm_mod, &quot;dm_mod.RDS&quot;) 27.1 Choosing predictors for multivariate modeling – testing, dealing with collinearity interactions 27.1.1 Challenges 27.2 presenting model results with RMarkdown 27.2.1 Challenges 27.3 presenting model results with a Shiny App 27.3.1 Challenges "],["a-gentle-introduction-to-shiny.html", "Chapter 28 A Gentle Introduction to Shiny 28.1 What is Shiny? 28.2 The Basic Structure of a Shiny App 28.3 The User Interface Section Structure 28.4 The Server Section Structure 28.5 How to Run an App 28.6 Building a Very Simple App (Version 1) 28.7 Edit this App (Version 2) 28.8 Building a User Interface for Inputs and Outputs 28.9 Building a Functioning Server Section 28.10 Building a Simple Shiny App (Version 3) 28.11 Publishing Your Shiny App on the Web 28.12 More to Explore", " Chapter 28 A Gentle Introduction to Shiny 28.1 What is Shiny? Shiny is both an R package {shiny}, and a web framework for building web applications using R. The {shiny} package helps you share your models, plots, or tables so that other people can use them in an interactive way. Anyone with access to the web can look up your Shiny app model on their phone (or computer), enter a few values for predictor variables, and get an estimate for a continuous or a dichotomous outcome, without doing any calculations on their own. This is a really powerful way to share your models. It is often valuable to publish (serve) your model on a website before publishing in a medical journal, so that you can put the link to the Shiny app for the model in the manuscript. It can sound a little intimidating to set out to build a web application if you have limited (or no) programming experience, but the {shiny} package and the teaching materials at the end of this chapter are designed to make it manageable. 28.2 The Basic Structure of a Shiny App A Shiny app has two sections, as seen in the diagram below: the user interface (ui) section which accepts inputs from users, and displays output values to users the server section, which ingests the input, processes the data, renders output values to HTML, and sends these back to the user interface (ui). You can see the flow of inputs and return values in the diagram below. Note that IDs and values in the user interface section are referred to with quotes (“ID”, “value”), and IDs and values in the server section are referred to with dollar signs (input$ID, output $value). This can be a source of confusion (and syntax errors in your code), as the same user inputs and return values have different reference formats in each section of the Shiny app. Remember to use quotes to refer to input IDs and output values in the user interface section, and dollar signs to refer to the same IDs and output values in the server section. Structure of a Shiny App 28.2.1 The weirdness of a Shiny app Shiny apps use a different programming style from what we are used to for data analysis. We normally use what is called an imperative programming style, in which we write commands that are carried out in order, as soon as we run them. In contrast, Shiny apps use what is called a declarative programming style, which outlines higher-level goals, and lets the web app execute functions when it is activated by new input. When it is run, the Shiny app listens for new input, and reacts to this input by: Accepting the input in the user interface (ui) section Executing functions (making new model predictions, generating a plot) in the server section Rendering the output to HTML in the server section Displaying the output in the user interface (ui) section A Shiny app is reactive to new input. 28.3 The User Interface Section Structure The user interface (also known as the “front end” of an app is what the user sees and interacts with. This is often set up as a few sections: The titlePanel The sidebarPanel, where the user inputs data, which are assigned to specific “ID”s. The mainPanel, where resulting values from the server are displayed as output “values”. These can be model predictions, plots, tables, or other forms of R output. To set up this structure, the code needs to be structured into sections, which are built with functions, as seen below. When coding a user interface (ui) in a basic Shiny app: You add a title to the titlePanel(). You add input widgets and their labels to the sidebarPanel(). You add titles, text and output to the mainPanel(). The code to accomplish this has a similar hierarchy, as seen in the figure below. Using this basic template, you will add and define arguments to the: titlePanel() sidebarPanel() with specific typeInputs() mainPanel() with specific typeOutputs() 28.4 The Server Section Structure The server section is often described as the “back end” of the app. The user does not see what happens in the server section but this is where the actual data processing occurs. The server section accesses the inputs from the user as input$ID, where each input was assigned a unique “ID” in the user interface. The server code will then process this data to generate outputs (model predictions, tables, plots, etc.), which are rendered to HTML. These HTML rendered results are then assigned to unique output$values. Once the outputs have been assigned to unique output$values, the user interface can access them as “value” (in quotes), and output them to the mainPanel(), using the appropriate typeOutput(\"value\"). 28.5 How to Run an App The code part of running a Shiny app is always the same, and is quite simple. The function is shinyApp() and the two arguments identify the user interface (ui) and the server code (server) being used. Code to Run the Shiny App To actually run the app, activating its listening for new input, and displaying new output in the ui, you can click on the Run App button (with the green triangle icon) at the top of the Source Pane (top left pane by default). Use a keyboard shortcut - Cmd+Shift+Enter on Mac, Ctrl-Shift-Enter on Windows From the command line, call shiny::runApp() with the path argument being the path to the directory containing app.R. 28.5.1 How to Stop an App When your Shiny app is running, R is busy - the R prompt is not visible, and the console pane toolbar displays a stop sign icon. You can also tsee that the R Console pane has an “away message”. It says something like #&gt; Listening on http://127.0.0.1:8763. This tells you the URL where your Shiny app can be found. The URL 127.0.0.1 is the standard address that means “my computer”, and 8763 is a randomly assigned port number. You can open a browser on your computer, and enter that URL to open another copy of your Shiny app. You can stop your app by: Clicking the stop sign icon on the R Console pane toolbar. Closing the Shiny app window. Clicking in the Console pane, then pressing Esc (or Ctrl-C if using the command line). 28.6 Building a Very Simple App (Version 1) We will build a simple app to illustrate the flow of information between the ui and the server. Open up RStudio, in a new session. Select: File/New File/Shiny Web App. Give the Application a name, like shiny-test, leave the default structure (Single File app.R), and select the directory to save it in, then click on the Create button. You get the basic template for a basic Shiny app, with some comments to guide you. Inspect the user interface section, and the server section. Now let’s run this app to see what it does. Click on the Run App button at the top of the source pane. This should open an app window, titled Old Faithful Geyser Data. The inputSlider in the sidebarPanel starts at a default value of 30, and the mainPanel displays a histogram. You can slide the input value to a different Number of Bins, and the histogram will rapidly be redrawn, in reaction to your new input. 28.6.1 The ui section If you have screen space, arrange and resize the windows so that the source window and the Shiny app are side by side. Start by inspecting the ui section. The sidebarPanel has one input, with an ID of “bins”, with range from 1=50, and a default value of 30. Can you find the code that defines this? The mainPanel has only the plotOutput, but no code to make the plot. It is displaying a “distPlot” that was created by the server section. 28.6.2 The server section Now scroll down to the server section. This has a renderPlot function that renders a plot and assigns it to output$distPlot. It uses input$bins from the ui and the faithful dataset to create the histogram plot in base R. Note that the ui section uses quotes to refer to variables and the server section uses $ format to refer to variables. This means that the “bins” input in the ui appears in the server as input$bins, and the output$distPlot in the server section appears as “distPlot” in the ui mainPanel. 28.7 Edit this App (Version 2) Go back to your app.R session of RStudio. Notice that the Console pane does not have an active R prompt (&gt;). It is Listening on http://127.0.0.1/7787 - note that the port number - the random 4 digit number at the end, will likely be different each time. Stop running the app, by closing the app window, or clicking on the stop sign at the top right of the Console pane. In the Source Pane tab for app.R, scroll to the ui and within that, the titlePanel. Edit the title to “Medical Textbooks”. Now replace sliderInput with selectInput. Change the inputID from “bins” to “book”. Change the label from “Number of bins:” to “Select a Textbook:” Delete the min, max, and value arguments. Add a choices argument, with choices = list(\"Textbook of Surgery\", \"Textbook of Gastroenterology\", \"Diseases of the Liver and Biliary System\"). Double check that all your parentheses still match, and that you still have a comma between sidebarPanel and mainPanel. Now Run the app again. You should see a new sidebar, with a dropdown selection with 3 choices. The mainPanel should no longer work, as the expected “bins” input is no longer there. We will leave the mainPanel in its non-working state for now, and move to the server section to process the input$book data. Scroll down to the server section. Change output$distPlot to output$name. Now change the renderPlot to renderText. Now delete everything inside of renderText from # generate bins to border = 'white'). After renderText there is now only an open parenthesis followed by an open curly brace, and a few lines later these are closed with a curly brace and a parenthesis. If there is not much space between these, put in a few returns to open up some space. Then paste the following into the renderText between the paren/curly brace combinations: output$name &lt;- renderText({ if(input$book == &quot;Textbook of Surgery&quot; ) {author = &quot;David Sabiston&quot; } else if(input$book == &quot;Textbook of Gastroenterology&quot; ) {author = &quot;Tachi Yamada&quot; } else if (input$book == &quot;Diseases of the Liver and Biliary System&quot; ) {author = &quot;Sheila Sherlock&quot; } author }) This R code checks the input phrase, and assigns a particular value for author based on the phrase, much like a case_when function, but using the if/else if construction. Then it returns the value for the new variable, author. This is renderText-ed into the output$name. This will be sent to the ui as “name”. Now let’s scroll back up to the mainPanel of the ui, and use this \"name\" variable to generate an output. Find the mainPanel, and delete the plotOutput. Inside the parentheses for mainPanel, paste the following: h3(&quot;Who Wrote It?&quot;), br(), textOutput(&quot;name&quot;) This will print Who Wrote It? in header 3 format, followed by a line break, followed by the name from the server. Now we have accepted input in the ui, processed in in the server, and displayed it in the mainPanel of the ui. Our app is ready to run - click the Run App arrow. This should run a very simple Shiny app that reacts to user input and changes the output in the mainPanel to identify the correct computer for each computer catchphrase. Save this for a (later) Version 3. Hopefully, this exercise helped you understand the flow of information from the ui to the server and back to the ui. Let’s learn more about ui input widgets and server code to get you ready to build a Shiny app to share a model. 28.8 Building a User Interface for Inputs and Outputs 28.8.1 Inputs You have a number of options for obtaining input values from the users of your Shiny app. These are a variety of ‘widgets’. 28.8.1.1 Shiny Input Widgets for Your User Interface (UI) It can be hard to keep track of all the possible input options for your User Interface (UI), but this Shiny Widgets Gallery lets you scroll through a bunch of examples, and then see the code to see how to make it work (and copy, paste, and modify for your own Shiny app). Common input widgets include: Numeric (numericInput()) - good for exact values, but users can only input numbers. Good for savvy users and exact values in a big range. Slider (sliderInput()) or a slider range (min to max). Faster and less prone to errors than numeric input, but it is harder to be exact with fingers on a phone screen. Good for a small range of values. A dropdown list of choices (can be multiple if needed) (selectInput()) Radio buttons from a list of choices (radioButtons()), or true/false (single checkbox) - checkboxInput(). File upload (for a new dataset) (fileInput()) Date input (dateInput()) or Date Range (dateRangeInput()) 28.8.1.2 Assign Each Input to an input “ID” = input$ID For each input, there is an input type, and a unique function for that input type, like numericInput() for asking the user for a number. You will use the first argument to this function to assign the inputID. This is a short helpful name for this variable, like “age”, or “creatinine”. These are assigned with quotes in the user interface section. For example, you might use numericInput(inputID = \"age\", label = \"Type in the age\", min =1, max = 100, value = 35) The same variables will be referred to in the server section as input$age or input$creatinine. The arguments for each input function in the user interface are a lot alike. These include: inputID - the name to use to look up the value of the widget (as a character string). e.g. inputID = \"age\". Note that the server section will refer to this same value as input$age, while the user interface will refer to it as “age”. They are the same thing. label - the label to display to the user above the input box in the sidebarPanel. e.g. label = \"Enter numeric age here\". choices (if a list of choices is used) This argument is used for dropdown lists or multiple checkboxes. e.g. choices = list(\"Hispanic\"= 1, \"Not Hispanic\" = 2). value or selected (a default value if the user does not make a selection). The default value is used for numeric or slider input, and the default selected is used for dropdown lists or radio button lists. e.g. value = 50, or selected = 2. 28.8.2 Outputs When the server section assigns rendered outputs to output$value identifiers, you will need to refer to these in the user interface section to display them in the mainPanel. To display these, you use a typeOutput function - these include tableOutput(\"value\"), plotOutput(\"value\"), textOutput(\"value\"), etc. The server section could render a table of model outputs, and assign this to output$table1. The UI section would then be able to display this output in the mainPanel with tableOutput(\"table1\"). Note that table 1 is referred to in the ui by its value name in quotes (“table1”), rather than in the server style of output$table1. 28.8.2.1 Use the Correct Output Function for the Rendered output$ID it Comes from Each type of output from the server has a different typeOutput function used in the ui to display the output value. Be sure to use the right typeOutput function. Several examples are shown below. Server rendering &amp; assignment UI Output function renderText() -&gt; output$prediction textOutput(“prediction”) renderPrint() -&gt; output$model textOutput(“model”) renderPlot() -&gt; output$plot1 plotOutput(“plot1”) renderTable() -&gt; output$table1 tableOutput(“table1”) renderDataTable() -&gt; output$dt dataTableOutput(“dt”) Note that renderText can only handle character strings, while renderPrint can handle any printed output. A data table differs from a regular table in that it will show a page of data at a time, rather than a whole dataset, and will allow users to page through the data or sort it. 28.9 Building a Functioning Server Section The server section defaults are straightforward, and are the bread of the sandwich that you will build for data processing. The default content is server &lt;- function(input, output) { } and all of the data import, data processing, rendering of objects to HTML, and assigning outputs to output$ID happens between the curly braces. 28.9.1 Using the input values &amp; Data Data values from the user in the ui section are each assigned an inputID in quotes, like “height_cm”, or “weight_kg”. These can be accessed in the server section with input$height_cm or input$weight_kg. 28.9.2 Wrangling and Calculating You can calculate with input values, and mutate to create new variables. What does the server section below calculate? how would you access it in the ui section? server &lt;- function(input, output) { output$bc_ratio &lt;- renderText({ bc_ratio &lt;- input$bun_mg-dl/input$creat_mg_dl bc_ratio }) } Show Answers This calculates the Blood Urea Nitrogen to Creatinine ratio textOutput(“bc_ratio”) 28.9.3 Rendering to HTML Outputs Each output you create has to be rendered to HTML. This will require (after wrangling and calculating the output, and returning the value) using a render function in the server section. Each render function is for a distinct type of object, and wraps around your output with parentheses AND curly braces. Render Functions for the Server Section render function creates renderDataTable DataTable renderImage images (saved as a link to a source file) renderPlot plots renderPrint any printed output renderTable data frame, matrix, other table like structures renderText character strings renderUI a Shiny tag object or HTML 28.10 Building a Simple Shiny App (Version 3) We are now going to edit your shiny app to calculate BMI, using the formula: [Body Mass Index = weight in kg/(height in meters)^2]. Re-open the Shiny app file, app.R. Your serverPanel should still have a selectInput with 3 choices. Delete this and replace it with a numericInput() for weight, and a sliderInput() for height, as shown below sidebarPanel( numericInput( inputID = &quot;weight_kg&quot;, label = &quot;Enter weight in kilograms&quot;, value = 70), sliderInput( inputID = &quot;height_m&quot;, label = &quot;Enter height in meters&quot;, value = 1.7, min = 0.5, max = 2.7) ) Now scroll down to the server section. You can delete the if else section in renderText between the parenthesis/curly brace pairs. Change the name of the output$name to output$bmi. Then in the renderText section, calculate the BMI, as shown below. server &lt;- function(input, output) { output$bmi &lt;- renderText({ bmi &lt;- input$weight_kg/(input$height_m)^2 bmi }) } This will take the input values from the user and calculate BMI, then render it as text, and then assign it to output$bmi. Now let’s scroll back up to the mainPanel in the ui section. Your mainPanel should currently be set up to display the bc_ratio. Replace the header (h3) with “The Body Mass index is:”. Then change the textOutput argument to “bmi”. Then change the titlePanel (scroll up) to “BMI Calculator”. Now your BMI app should be ready to run. Give this a shot, and troubleshoot if needed. 28.11 Publishing Your Shiny App on the Web There are 4 major options for sharing your Shiny App on the web. ShinyApps.io is a web service that will let you share your shiny apps. You sign up for an account, then you can publish your Shiny apps for free. The free account level has a limited number of apps (10), and a limited number of hours of usage per month, but this will often do for simple models without a lot of usage. You can update your account from the free tier to accomodate more usage per month if it is popular. There are a number of tiers (Starter, Basic, Standard, and Professional) at different price points and usage volumes. Each app gets a unique web URL, which you can share with others, in the format: https://username.shinyapps.io/webapp_name/. Shiny Server - this is a good option if your app will have a lot of use and you have access to your own hardware or virtual server. You can download Shiny Server here. You can host your own Shiny apps for free, but there is a fair amount of setup involved. A walkthrough of how to do this can be found here. Set up a Shiny Server instance for on-demand use with a server service like Digital Ocean - They will host your web app for $5 per month. There is a basic walk-through on how to set this up here from Dean Attali. This is a walkthrough on how to deploy a Shiny app with Amazon Web Services (AWS). If you have access to RStudio Connect at your workplace, your RSC administrator can help you publish Shiny apps. 28.12 More to Explore This chapter is just the tip of the iceberg in terms of what you can do with Shiny web applications built in R. A gallery of great examples of what you can do with Shiny can be found at the link above. These examples also include code that you can copy and modify for your own purposes. These are great for seeing what is possible with Shiny (it is more than you would think!) Some excellent resources for learning more about Shiny web apps, dashboards, etc. can be found at: Lots of introductory material can be found at the RStudio Tutorial Webpage with 32 video lessons in 3 parts. Follow the link to get started. How to Build a Shiny App from Scratch is a nice free e-book, and a more complete introduction to building Shiny apps. The Mastering Shiny e-book takes you further under the hood, explaining how Shiny works, with excellent explanations of reactivity and more advanced Shiny programming. "],["sharing-models-with-shiny.html", "Chapter 29 Sharing Models with Shiny 29.1 Setting up and Saving Models 29.2 Building a Shiny App for the Linear Model 29.3 Building a Shiny App for the Logistic Model 29.4 Building a Shiny App for the Random Forest Model 29.5 Challenge Yourself", " Chapter 29 Sharing Models with Shiny In this chapter we will practice Sharing your predictive models with Shiny Apps for research or clinical use. 29.0.1 Packages Needed for this Chapter This chapter will require {tidyverse}. {medicaldata}, {shiny}, and {mlbench} packages. You may already have these packages installed. If not, these can be installed with the code below if you copy the code chunk to your RStudio Console pane and run these installation functions. # Install the packages below if you do not have these already installed. install.packages(&#39;tidyverse&#39;) install.packages(&#39;medicaldata&#39;) install.packages(&#39;shiny&#39;) install.packages(&#39;mlbench&#39;) We will walk through how to do this for a few example models. 29.1 Setting up and Saving Models We will start by building and saving several models. When needed, we can move these model objects (model.RDS) to the folder where the Shiny app will be located. For now, open an RStudio session in a directory where you can find the saved models, then copy and run the code chunks below to save the 3 models as *.RDS files. 29.1.1 Linear Model We will use one of the {medicaldata} datasets to build a linear model. We will use the supraclavicular dataset to model the onset of sensory function after anesthesia in minutes, using treatment group as the main predictor, adjusted for age, bmi, and gender. We will then save this linear model to an RDS file, using the saveRDS() function. library(tidyverse) library(medicaldata) library(shiny) supra_linear_model &lt;- medicaldata::supraclavicular %&gt;% lm(formula = onset_sensory ~ age + bmi + gender + group, data = .) saveRDS(supra_linear_model, &quot;linear_model.RDS&quot;) 29.1.2 Logistic Model We will use the Pima Indian dataset from {mlbench} to build a logistic model. We will classify individuals for the outcome of type 2 diabetes with 4 predictor variables. We will then save this logistic model to an RDS file, using the saveRDS() function. library(mlbench) library(shiny) data(&quot;PimaIndiansDiabetes2&quot;) dm_logit_mod &lt;- glm(diabetes ~ glucose + mass + pedigree + age, data = PimaIndiansDiabetes2, family = &quot;binomial&quot;) saveRDS(dm_logit_mod, &quot;logit_model.RDS&quot;) 29.1.3 Random Forest Model We will also use the Pima Indian dataset from {mlbench} to build a random forest model with the {tidymodels} package. This approach requires more steps, but is more flexible, as you can swap out the engine and its parameters, the pre-processing recipe, and (if needed) tune hyperparameters in the pipeline, allowing you to easily create and compare several versions of models. We will again predict the outcome of diabetes with all of the predictor variables available. We will then save this random forest model to an RDS file, using the saveRDS() function. library(tidymodels) library(mlbench) data(&quot;PimaIndiansDiabetes2&quot;) set.seed(123) splits &lt;- initial_split(PimaIndiansDiabetes2 %&gt;% na.omit(), strata = diabetes) dm_training &lt;- training(splits) dm_testing &lt;- testing(splits) dm_rf_mod &lt;- rand_forest(mtry = 4, min_n = 2, trees = 500) %&gt;% set_engine(&quot;ranger&quot;, num.threads = 8) %&gt;% set_mode(&quot;classification&quot;) rf_recipe &lt;- recipe(diabetes ~ ., data = dm_training) %&gt;% step_zv(all_predictors()) %&gt;% step_normalize(all_predictors()) rf_workflow &lt;- workflow() %&gt;% add_model(dm_rf_mod) %&gt;% add_recipe(rf_recipe) rf_fit &lt;- rf_workflow %&gt;% fit(data = dm_training) # predict(rf_fit, dm_testing, type = &quot;prob&quot;) # augment(rf_fit, dm_testing) saveRDS(rf_fit, &quot;rf_model.RDS&quot;) 29.2 Building a Shiny App for the Linear Model 29.2.1 The Default Shiny App Let’s start by opening a default Shiny app using the ‘Old Faithful’ template. This will build an app.R web app in a folder. We will need to move the file linear.RDS to this folder to have access to the model. To set up the default Shiny App, open a new RStudio session. Then Select: File/New File/Shiny Web App. Give the Application a name, like linear-model, leave the default structure (Single File app.R), and Select the directory to save it in (I put it into my experiments directory), then Click on the Create button. Now go to your file manager for your computer, and find the file named “linear_model.RDS”, and copy/paste/move it into your linear-model folder. Now your Shiny app will have access to the model. We will now edit the “Old Faithful” Shiny app to turn it into the Linear Model Shiny app that we want. Having the structure in place and editing piece by piece is pretty helpful. Let’s go step by step. Editing and coding in Shiny apps can be frustrating at first, as the structure is very particular, and a bit persnickety. Every comma, parenthesis, and curly brace is there for a reason, and it is very easy to get these wrong. It is very helpful to 1. Have the rainbow parentheses option turned on in your version of RStudio. You can turn this on with Tools/Global Options/Code/Display and select the checkbox for Rainbow Parentheses. When this is on, paired parentheses are color-matched, and when you reach the red close-parenthesis, you know you have closed all of the open expressions. 2. Watch your commas - you will need one between each input, but no comma after the last input. 3. Watch the red dot Xs at the left side of your code. Hover over these to help figure out what is wrong, and how to fix it. 4. If you are totally stuck, there are several Solution buttons to bail you out. 5. Don’t get down on yourself if you are struggling with Shiny - the syntax is hard when you are getting started, and the details of parentheses, curly braces, and commas are fairly unforgiving. Keep plugging away - you will get this working! Find and run the app.R file in the linear-model folder. This should produce the “Old Faithful” histogram, where x is the waiting time in minutes to the next eruption. Close the shiny app, and go to the app.R file. In the ui section, Change the title from “Old Faithful Geyser Data to”Linear Model Predictions for Supraclavicular Anesthesia”. Reload/Run the App to make sure this worked, and nothing else broke. 29.2.2 Editing the ui sidebarPanel for the Input Predictor Variables The linear model for predicting onset of sensory function has 4 predictors: group, gender, age, and bmi. The group value can be 1 or 2, the gender 0 or 1, the BMI ranges from 19-44, and the age ranges from 18 to 74 in the supraclavicular dataset. In the ui sidebarPanel section, replace the sliderInput for bins with one for BMI, with the appropriate range, and a default value of 32. Re-run the app to make sure that worked. Note that the mainPanel will now fail, without the “bins” input. That is OK. In the ui sidebarPanel section, copy/paste to add a similar sliderInput for age from 18 to 74, with a default of 40. Make sure you have a comma between each Input. Check to make sure you have parentheses and commas in all the right places. Feel free to undo (Cmd-Z or control-Z to start over). In the ui sidebarPanel section, add a comma, then a new selectInput for the group variable, with choices of 1 and 2. Instead of min, max, and value, you will need choices = list(1,2). Test run the App again. Copy/paste to add a similar selectInput for gender, with values of 0 and 1. Don’t forget the comma between Inputs. Test run the App again. Now add (before the first sliderInput) a header for your sidebar, with a line for h3(\"Input Values\"). Be sure to follow this with a comma before the first Input. Test run the App again. If your version is not working, check the Solution button below and compare it to your code for the ui section. You should have 5 distinct entries in the sidebarPanel - one h3 title, and 4 Inputs. Solution ui &lt;- fluidPage( # Application title titlePanel(&quot;Linear Model Predictions for Supraclavicular Anesthesia&quot;), # Sidebar with a slider input for number of bins sidebarLayout( sidebarPanel( h3(&quot;Input Values&quot;), sliderInput(&quot;bmi&quot;, &quot;Select the BMI:&quot;, min = 19, max = 44, value = 32), sliderInput(&quot;age&quot;, &quot;Select the Age:&quot;, min = 18, max = 74, value = 40), selectInput(&quot;group&quot;, &quot;Select the Group:&quot;, choices = list(1,2)), selectInput(&quot;gender&quot;, &quot;Select the Gender:&quot;, choices = list(0,1)) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;distPlot&quot;) ) ) ) Now we have the inputs all set in the sidebarPanel. Let’s take these 4 inputs (and the model) to the server section, and set it up to make predictions. 29.2.3 Editing the server section to make Predictions Scroll down to the server section of app.R. Now delete everything from output$distPlot to the }) at the end, before the final curly brace } - leave that one in place. Now you should have several lines of open space between the two curly braces. The server section will generate predictions in 4 steps, two of which are reactive to inputs. For step 1, we will read in the model. Copy this line of code into the server section: model &lt;- readRDS(\"linear_model.RDS\"). This just reads the model object in, and stores it in the variable model. No inputs yet. Run the app again to make sure it works. For step 2, we will read in the input values, and store them as a dataframe. These have to be wrapped in reactive({ }) as they are reacting to inputs. Paste this code chunk into the server section next: input_df &lt;- reactive({ data.frame(bmi = input$bmi, age = input$age, group = as.double(input$group), gender = as.double(input$gender)) }) Notice that the drop-down choice values have to be converted to doubles, which is their data type in the original dataset for modeling. This creates a function input_df() that we can use in the prediction step. Run the app again to make sure it works. For step 3, we will make the predictions. We will use the model and input_df. Again, this is reactive to the input values, so we will wrap the predict() function in reactive({ }). We will use input_df() with parentheses, like a function, as it had reactive inputs. Then we will assign this result to pred. Paste the code chunk below into the server section next. pred &lt;- reactive({ predict(model, input_df()) }) Run the app again to make sure it works. For step 4, we will render the prediction to HTML text for display in the mainPanel of the ui, and assign it to output$pred. Again, because pred had reactive inputs, we use it as pred() inside the renderText({ }) function. output$pred &lt;- renderText({pred()}) Run the app again to make sure it works. Now you should have an output$pred to display in the ui. If your version of the server section is not working, check the Solution button below and compare it to your code for the ui section. You should have 4 distinct entries in the server section - the model, the input_df, pred, and output$pred. Solution server &lt;- function(input, output) { model &lt;- readRDS(&quot;linear_model.RDS&quot;) input_df &lt;- reactive({ data.frame(bmi = input$bmi, age = input$age, group = as.double(input$group), gender = as.double(input$gender)) }) pred &lt;- reactive({ predict(model, input_df()) }) output$pred &lt;- renderText({pred()}) } 29.2.4 Editing the mainPanel in the ui section to display your Prediction Now we just have to show the text in output$pred. Scroll up to the ui section, and find the mainPanel. We will just put in some introductory text, followed by a blank line (line break), and then use a textOutput() function to show the prediction (“pred”). Copy and paste the code chunk below into the mainPanel. h3(&quot;The predicted onset of sensory perception in minutes is:&quot;), br(), textOutput(&quot;pred&quot;) Now run the app.R one more time. If all went well, you should now have a linear model Shiny app that predicts (with default values) the return of sensory function after supraclavicular anesthesia in 10.63524 minutes. You can test different inputs to see their effects on the predicted time of onset of sensory function. 29.3 Building a Shiny App for the Logistic Model Now we will turn to the logistic model for predicting the diagnosis of diabetes. Not surprisingly, serum glucose (from 44-199) will be an important predictor. We will use the logistic diabetes model that we created at the beginning of this chapter. The other 3 predictors we will use are named mass (BMI, from 18-67), pedigree (a score for diabetes frequency in ancestors), and age in years (range 21-81). 29.3.1 The Default Shiny App Again, we will start by opening a default Shiny app using the ‘Old Faithful’ template. This will build an app.R web app in a folder. We will need to move the file logit_model.RDS to this folder to have access to the model. To set up the default Shiny App, open a new RStudio session. Then Select: File/New File/Shiny Web App. Give the Application a name, like logistic-model, leave the default structure (Single File app.R), and Select the directory to save it in (I put it into my experiments directory), then Click on the Create button. Now go to your file manager for your computer, and find the file named “logit_model.RDS”, and copy/paste/move it into your logistic-model folder. Now your Shiny app will have access to the model. We will now edit the “Old Faithful” Shiny app to turn it into the Linear Model Shiny app that we want. Having the structure in place and editing piece by piece is pretty helpful. Let’s go step by step. Find and run the app.R file in the logistic-model folder. This should produce the “Old Faithful” histogram, where x is the waiting time in minutes to the next eruption. Close the shiny app, and go to the app.R file. In the ui section, Change the title from “Old Faithful Geyser Data to”Logistic Model Predictions for Diabetes”. Reload/Run the App to make sure this worked, and nothing else broke. 29.3.2 Editing the ui sidebarPanel for the Input Predictor Variables The logistic model for classifying individuals into type 2 diabetes vs not diabetes has 4 predictors: glucose, mass (bmi), predigree (score), and mass (actually BMI) These values all have the data type of numeric double. Scroll to the ui sidebarPanel section, and replace the sliderInput for bins with one for glucose, with a helpful label to guide users to enter the glucose value, with the appropriate range of 44-199, and a default value of 125. Re-run the app to make sure that worked. Note that the mainPanel will now fail, without the “bins” input. That is OK. Using this example, copy/paste the sliderInput to create input widgets for mass (range 18-67), age (range 21-81), and pedigree (score range 0.8-2.4). Pick your own user-facing labels and default values. Make sure that you have a comma between each sliderInput, but not one at the end. Re-run the app after you add each sliderInput to make sure everything still works. If you have done everything right, you should have 4 usable sliderInputs. If you are having a hard time, compare your ui section code to the code chunk Solution below. Solution # Define UI for application that draws a histogram ui &lt;- fluidPage( # Application title titlePanel(&quot;Logistic Model Predictions for Diabetes&quot;), # Sidebar with a slider input for number of bins sidebarLayout( sidebarPanel( sliderInput(&quot;glucose&quot;, &quot;Enter the glucose value:&quot;, min = 44, max = 199, value = 125), sliderInput(&quot;mass&quot;, &quot;Enter the BMI:&quot;, min = 18, max = 67, value = 42), sliderInput(&quot;age&quot;, &quot;Enter the age in years:&quot;, min = 21, max = 81, value = 51), sliderInput(&quot;pedigree&quot;, &quot;Enter the pedigree score:&quot;, min = 0.8, max = 2.4, value = 1.6) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;distPlot&quot;) ) ) ) Now we have the inputs all set in the sidebarPanel. Let’s take these 4 inputs (and the model) to the server section, and set it up to make predictions. 29.3.3 Editing the server section to make Predictions Scroll down to the server section of app.R. Now delete everything from output$distPlot to the }) at the end, before the final curly brace } - leave that one in place. Now you should have several lines of open space between the two curly braces. Add a few blank lines if you need more room. The server section will generate predictions in 4 steps, two of which are reactive to inputs. For step 1, we will read in the model. Copy this line of code into the server section: model &lt;- readRDS(\"logit_model.RDS\"). This just reads the model object in, and stores it in the variable model. No inputs yet. Run the app again to make sure it works. For step 2, we will read in the input values, and store them as a dataframe. These have to be wrapped in reactive({ }) as they are reacting to inputs. Paste this code chunk into the server section next: input_df &lt;- reactive({ data.frame(glucose = input$glucose) }) Now add 3 more lines to the data.frame() function, for input$mass, input$age, and input$pedigree. Assign these to the correct variable names, and use commas in the right places to complete the input_df. This creates a function input_df() that we can use in the prediction step. Run the app again to make sure it works. For step 3, we will make the predictions. We will use the model and input_df. Again, this is reactive to the input values, so we will wrap the predict() function in reactive({ }). We will use input_df() with parentheses, like a function, as it had reactive inputs. To get probabilities (rather than logit units, the default prediction), we need to add the argument type = \"response\". Then we will assign this result to pred. Paste the code chunk below into the server section next. pred &lt;- reactive({ predict(model, input_df(), type = &quot;response&quot;) }) Run the app again to make sure it works. For step 4, we will render the prediction to HTML text for display in the mainPanel of the ui, and assign it to output$pred. Again, because pred had reactive inputs, we use it as pred() inside the renderText({ }) function. output$pred &lt;- renderText({pred()}) Run the app again to make sure it works. Now you should have an output$pred to display in the ui. If your version of the server section is not working, check the Solution button below and compare it to your code for the ui section. You should have 4 distinct entries in the server section - the model, the input_df, pred, and output$pred. Solution server &lt;- function(input, output) { model &lt;- readRDS(&quot;logit_model.RDS&quot;) input_df &lt;- reactive({ data.frame(glucose = input$glucose, mass = input$mass, age = input$age, pedigree = input$pedigree) }) pred &lt;- reactive({ predict(model, input_df(), type = &quot;response&quot;) }) output$pred &lt;- renderText({pred()}) } 29.3.4 Editing the mainPanel in the ui section to display your Prediction Now we just have to show the prediction text in the mainPanel. Scroll up to the ui section, and find the mainPanel. Now just put in some introductory text about the probability of type 2 diabetes formatted in the header3 level with h3(\"Text\"), followed by a blank line (line break - br()), and then use a textOutput() function to show the prediction (“pred”). If you are having a hard time, you can check the Solution below Solution mainPanel( h3(&quot;The predicted probability of type 2 diabetes is:&quot;), br(), textOutput(&quot;pred&quot;) ) Now run the app.R one more time. If all went well, you should now have a logistic model Shiny app that predicts (with default values) the probability of type 2 diabetes as 0.8304044. You can test different inputs to see their effects on the predicted probability of type 2 diabetes. 29.4 Building a Shiny App for the Random Forest Model 29.5 Challenge Yourself How would you use the linear model predictions to plot these predictions compared to all of the other observations in the dataset? (Hint: augment the dataset with broom::augment predictions for each observation, then plot these with gray dots, and use the input data to make a new prediction plotted with a colored dot) How would you add confidence intervals to your predictions, and how would you add these to your displayed output in the mainPanel? "],["introduction-to-r-markdown.html", "Chapter 30 Introduction to R Markdown 30.1 What Makes an Rmarkdown document? 30.2 Trying out RMarkdown with a Mock Manuscript 30.3 Inserting Code Chunks 30.4 Including Plots 30.5 Including Tables 30.6 Including Links and Images 30.7 Other languages in code chunks 30.8 Code Chunk Options 30.9 How It All (Rmarkdown + {knitr} + Pandoc) Works 30.10 Knitting and Editing (and re-Knitting() Your Rmd document 30.11 Try Out Other Chunk Options 30.12 The setup chunk 30.13 Markdown syntax 30.14 2nd Header 30.15 Line Breaks and Page Breaks 30.16 Making Lists 30.17 The Easy Button - Visual Markdown Editing 30.18 Inline Code 30.19 A Quick Quiz", " Chapter 30 Introduction to R Markdown Rmarkdown Wizards, courtesy of Alison Horst Rmarkdown is an authoring framework for creating a variety of data-driven documents reproducibly with R. This e-book is itself a set of RMarkdown documents, assembled with the {bookdown} package. In many ways, Rmarkdown is a critical package for the R ecosystem, as it is a key enabler of reproducible reports in many formats. RMarkdown is a simple formatting syntax that allows you to mix text and code to document data analysis, and author MS Word, MS Powerpoint, HTML, PDF, web dashboards, web apps, and poster documents. Rmarkdown documents are fully reproducible and support more than a dozen output formats. If your data changes, or you decide to change a part of your analysis, you can reproduce the entire (new version) of the document with a single click of the Knit button (You can also use Cmd/Ctrl+Shift+K). This button is found at the top of the top left pane in RStudio. Knit button When you click the Knit button a document will be generated that includes both text content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this (which will run and produce output below. In this case, showing the contents of the covid_testing dataset.): covid &lt;- medicaldata::covid_testing glimpse(covid) ## Rows: 15,524 ## Columns: 17 ## $ subject_id &lt;dbl&gt; 1412, 533, 9134, 8518, 8967, 11048… ## $ fake_first_name &lt;chr&gt; &quot;jhezane&quot;, &quot;penny&quot;, &quot;grunt&quot;, &quot;meli… ## $ fake_last_name &lt;chr&gt; &quot;westerling&quot;, &quot;targaryen&quot;, &quot;rivers… ## $ gender &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;femal… ## $ pan_day &lt;dbl&gt; 4, 7, 7, 8, 8, 8, 9, 9, 9, 9, 9, 9… ## $ test_id &lt;chr&gt; &quot;covid&quot;, &quot;covid&quot;, &quot;covid&quot;, &quot;covid&quot;… ## $ clinic_name &lt;chr&gt; &quot;inpatient ward a&quot;, &quot;clinical lab&quot;… ## $ result &lt;chr&gt; &quot;negative&quot;, &quot;negative&quot;, &quot;negative&quot;… ## $ demo_group &lt;chr&gt; &quot;patient&quot;, &quot;patient&quot;, &quot;patient&quot;, &quot;… ## $ age &lt;dbl&gt; 0.0, 0.0, 0.8, 0.8, 0.8, 0.8, 0.8,… ## $ drive_thru_ind &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0… ## $ ct_result &lt;dbl&gt; 45, 45, 45, 45, 45, 45, 45, 45, 45… ## $ orderset &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1… ## $ payor_group &lt;chr&gt; &quot;government&quot;, &quot;commercial&quot;, NA, NA… ## $ patient_class &lt;chr&gt; &quot;inpatient&quot;, &quot;not applicable&quot;, NA,… ## $ col_rec_tat &lt;dbl&gt; 1.4, 2.3, 7.3, 5.8, 1.2, 1.4, 2.6,… ## $ rec_ver_tat &lt;dbl&gt; 5.2, 5.8, 4.7, 5.0, 6.4, 7.0, 4.2,… The Knit button (at the top of the top left pane of RStudio) runs render(\"file.Rmd\", output = document-type) for you in a background (clean) session of R. 30.1 What Makes an Rmarkdown document? An Rmarkdown document is a plain-text file with the *.Rmd file extension. It is composed of four types of content: The YAML header (at the top), surrounded at top and bottom by 3 dashes (—) Text narrative - the meat of your manuscript Code chunks, surrounded at top and bottom by 3 back-ticks (```), and the chunk header in braces, like {r plot-figure} Note that the first code chunk is always named setup and is often used to load libraries and set up document options. Inline code, which does calculations in your text to provide calculated values like means, medians, and p values. This essentially provides an interface like a ‘lab notebook’ for data analysis. You can use code chunks to run the analysis, and text to document what you are doing in the analysis, how it worked, and interpret the results. When the analysis is ready, you can polish up your document to produce a final manuscript. Code outputs, including tables and plots, are incorporated into the document. You can choose to show or hide the code chunks in the final document with options in the chunk header, like: {r, echo = FALSE} - runs code, but does not show it. or {r, echo = TRUE} - runs code and shows the code. 30.2 Trying out RMarkdown with a Mock Manuscript Open up RStudio. Start a new Project. Click - File - New Project… - Version Control &gt; - Git &gt; Now paste the following into the Repository URL: box https://github.com/higgi13425/rmd4medicine.git add a directory name, like rmd4medicine click on Create Project Find the Files tab in the lower right quadrant. Click to open the prep folder Click to open the mockstudy_analysis.Rmd file. This file should open in the top left quadrant of RStudio. If there are warnings at the top of the file that you need to install packages, click on the Install button. 30.3 Inserting Code Chunks You can add code to your document to process your data and display results. To insert a code chunk into your Rmarkdown document, click on the green +c button at the top center of the top left pane in RStudio. The dropdown menu will allow you to choose R code or several other computer languages, For now, click on R. This inserts a gray code chunk, which starts and stops with 3 back-ticks. The starting back-ticks are followed by braces containing a lower-case r, designating what follows as R code. You can name your individual code chunks with specific names, based on what they do. You can click to the right of the lower-case r, before the closing brace, and add a space, then a name for the code chunk. If the chunk name contains multiple words, connect these with hyphens, as in the code chunk below. Avoid spaces, periods, and underscores in chunk names. head(covid) ## # A tibble: 6 × 17 ## subject_id fake_first_name fake_last_name gender pan_day ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1412 jhezane westerling female 4 ## 2 533 penny targaryen female 7 ## 3 9134 grunt rivers male 7 ## 4 8518 melisandre swyft female 8 ## 5 8967 rolley karstark male 8 ## 6 11048 megga karstark female 8 ## # … with 12 more variables: test_id &lt;chr&gt;, ## # clinic_name &lt;chr&gt;, result &lt;chr&gt;, demo_group &lt;chr&gt;, ## # age &lt;dbl&gt;, drive_thru_ind &lt;dbl&gt;, ct_result &lt;dbl&gt;, ## # orderset &lt;dbl&gt;, payor_group &lt;chr&gt;, patient_class &lt;chr&gt;, ## # col_rec_tat &lt;dbl&gt;, rec_ver_tat &lt;dbl&gt; 30.3.1 Code Chunk Icons You may have noticed 3 small icons at the top right of each code chunk. From left to right, these are a (settings) gear, a downward arrowhead with a green baseline (run all of the preceding chunks), and a rightward (run) arrow. Check these out and experiment with them. Code Chunk Icons Icon Uses Settings Gear Allows you to Name the code chunk Set options for this chunk Run Chunks Above (down arrow) Runs all of the preceding code chunks, including the setup chunk Run Chunk (rightward arrow) Runs the entire current chunk 30.4 Including Plots You can also embed plots using code chunks, for example: covid %&gt;% ggplot() + aes(x = pan_day, y = ct_result) + geom_point() + labs(title = &quot;COVID Testing in First 100 Days of Pandemic&quot;, x = &quot;Pandemic Day, 2020&quot;, y = &quot;Cycle Threshold \\n45 is a Negative Test&quot;) Note that the echo = FALSE parameter was added to the top of the plot code chunk to prevent printing of the R code that generated the plot. This is an example of a chunk option. 30.5 Including Tables You can also use code chunks to include tables in your document. covid %&gt;% count(demo_group, gender) %&gt;% gt() %&gt;% tab_header(title = &quot;Demographics of COVID Testing&quot;, subtitle = &quot;By Group and Gender&quot;) %&gt;% tab_source_note(source_note = &quot;From CHOP, 2020&quot;) %&gt;% cols_label(demo_group = &quot;Group&quot;, gender = &quot;Gender&quot;, n = &quot;Count&quot;) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #uwpwpvrmou .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #uwpwpvrmou .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uwpwpvrmou .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #uwpwpvrmou .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #uwpwpvrmou .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uwpwpvrmou .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #uwpwpvrmou .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #uwpwpvrmou .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #uwpwpvrmou .gt_column_spanner_outer:first-child { padding-left: 0; } #uwpwpvrmou .gt_column_spanner_outer:last-child { padding-right: 0; } #uwpwpvrmou .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #uwpwpvrmou .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #uwpwpvrmou .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #uwpwpvrmou .gt_from_md > :first-child { margin-top: 0; } #uwpwpvrmou .gt_from_md > :last-child { margin-bottom: 0; } #uwpwpvrmou .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #uwpwpvrmou .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #uwpwpvrmou .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #uwpwpvrmou .gt_row_group_first td { border-top-width: 2px; } #uwpwpvrmou .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uwpwpvrmou .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #uwpwpvrmou .gt_first_summary_row.thick { border-top-width: 2px; } #uwpwpvrmou .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uwpwpvrmou .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #uwpwpvrmou .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #uwpwpvrmou .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #uwpwpvrmou .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #uwpwpvrmou .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uwpwpvrmou .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; } #uwpwpvrmou .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #uwpwpvrmou .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #uwpwpvrmou .gt_left { text-align: left; } #uwpwpvrmou .gt_center { text-align: center; } #uwpwpvrmou .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #uwpwpvrmou .gt_font_normal { font-weight: normal; } #uwpwpvrmou .gt_font_bold { font-weight: bold; } #uwpwpvrmou .gt_font_italic { font-style: italic; } #uwpwpvrmou .gt_super { font-size: 65%; } #uwpwpvrmou .gt_two_val_uncert { display: inline-block; line-height: 1em; text-align: right; font-size: 60%; vertical-align: -0.25em; margin-left: 0.1em; } #uwpwpvrmou .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; } #uwpwpvrmou .gt_asterisk { font-size: 100%; vertical-align: 0; } #uwpwpvrmou .gt_slash_mark { font-size: 0.7em; line-height: 0.7em; vertical-align: 0.15em; } #uwpwpvrmou .gt_fraction_numerator { font-size: 0.6em; line-height: 0.6em; vertical-align: 0.45em; } #uwpwpvrmou .gt_fraction_denominator { font-size: 0.6em; line-height: 0.6em; vertical-align: -0.05em; } Demographics of COVID Testing By Group and Gender Group Gender Count client female 314 client male 290 misc adult female 1214 misc adult male 1227 other adult female 126 other adult male 97 patient female 6178 patient male 6077 unidentified male 1 From CHOP, 2020 Note that this code chunk is using the {gt} package to format the table. Other popular approaches to table formatting include the {flextable} package and the knitr::kable() function. 30.6 Including Links and Images 30.6.1 Links You can add hypertext links to your text (without a code chunk) with a description in square brackets followed immediately by the URL in (parentheses), like this: [text description here](http://www.link.com) As an example, the link to the Rmarkdown cheatsheet can be found at this link. 30.6.2 Images You can add images to your text if they are in the same project as your Rmarkdown document. You have to specify the path to the image file correctly. It is often helpful to collect your images in an images folder or a figures folder. If you have already generated your figures with other R scripts, they can be placed into your manuscript document from a figures folder. You can add images to your text with an exclamation point, followed by a caption in square brackets followed immediately by the path to the image file in (parentheses), on a line of its own, separated from the text, like this: ![Caption for this Figure 1](images/figure_1.png) You can also insert an image using a code chunk and the knitr function include_image(), like this, which gives you more options to control figure size, alignment, height, and width with code chunk options: knitr::include_graphics(&#39;images/datasaurus-dozen.png&#39;) (note that echo = TRUE and eval=FALSE as code chunk options means that this code is shown, but not run) An example shown below is an image of the “datasaurus dozen” used to illustrate what summary measures can hide in data distributions, as seen below in 12 data distributions with the same mean and standard deviation, one of which happens to look like a T. Rex. You should always visualize your data. There might be a dinosaur in there. (note that echo = FALSE and eval=TRUE as code chunk options means that the code chunk used to include this image code is run, but not shown) Source for Image: https://juliasilge.com/blog/datasaurus-multiclass/ 30.7 Other languages in code chunks You can use a number of different open-source languages in addition to R if needed to do your data analysis, including SQL, shell code with Unix Bash, C, C++ via Rcpp, Stan, and D3. Any of these options can be chosen from the Insert Code button dropdown (green +c button). 30.8 Code Chunk Options When you are working through a data analysis, you usually want to display the code that led to a result. For the final manuscript, you may want to hide the code and just display the results. You can accomplish this with echo=TRUE to display code, and echo=FALSE to hide the code. Code chunk options should be added to the top of each code chunk, in the chunk header after the name of the code chunk, and separated from the chunk name (and from each other) by commas. The chunk header (material between the braces) must be written on one line. You must not break the line with a return, or it will not work. Code Chunk Options Option Values Output eval TRUE/FALSE Whether or not the code is run. echo TRUE/FALSE Show or hide the code include TRUE/FALSE Whether or not the resulting output of a code chunk is displayed in the document. FALSE means that the code will run, but will not display results. include = FALSE is often used for the setup chunk. warning TRUE/FALSE Whether warnings generated from your code will be displayed in the document. message TRUE/FALSE Whether messages generated from your code will be displayed in the document. fig.align default, left, right, center Where on the page the output figure should align. Text options should be in quotes, like fig.align = \"right\" fig.width default = 7 figure width in inches fig .height default = 7 figure height in inches error TRUE/FALSE If TRUE, will not stop building the document if there is an error in a code chunk. cache TRUE/FALSE If TRUE, will store the results and not re-run the chunk. Helpful for long, slow calculations. But watch out for this if your data change and your results do not(!!). Note that there are many more chunk options which you can use if needed, and these can be found here. 30.9 How It All (Rmarkdown + {knitr} + Pandoc) Works rmarkdown processing from rstudio.com Rmarkdown is an R-flavored version of the markdown language. This is a universal, open-source markup language for creating formatted documents from plain text. Markdown documents end with the file extension *.md. An open-source program named pandoc converts *.md documents to output documents like MS Word, PDF, HTML, MS Powerpoint, etc. When you click the Knit button or run the render() function, R Markdown feeds the .Rmd file to knitr, which executes all of the code chunks and creates a new markdown (.md) document which includes the code and its output. The markdown file generated by {knitr} is then processed by pandoc which is responsible for creating the finished format. This may sound complicated, but R Markdown makes it extremely simple by encapsulating all of the above processing into a single render() function (or the Knit button). 30.10 Knitting and Editing (and re-Knitting() Your Rmd document Find the Knit button at the top of the file, and click it to convert this *.Rmd . It will automagically knit the document first to markdown (to *.md and then from *.md to *.HTML. Scroll through the document to see what has been created. Go back to the Rmd document. Now try the following: in the YAML header, edit the author to your name edit the date to the current (or a different) date edit the output from html_document to word_document find the glimpse code chunk. After the chunk name glimpse, add a comma, then the option echo=FALSE Add this same option to a few of of the other code chunks (feel free to use copy-paste, but make sure you don’t end up with duplicate commas, as this will cause errors) Now click on the Knit button again. You should get a new version of the document, now in MS Word format, with the code chunks hidden, and a new author and date. The Word document is in a particular default format, but you can change this by specifying a template word file in the YAML header. 30.11 Try Out Other Chunk Options Try adding different chunk options, including include = FALSE eval = FALSE, echo = TRUE eval=TRUE, echo = FALSE 30.12 The setup chunk The setup chunk is a special code chunk, which is usually the first code chunk at the top of your RMarkdown document. Typically it includes two types of code: libraries to be loaded data to be loaded in the background and often looks something like this: library(tidyverse) library(here) data &lt;- read_csv(here(&quot;data/my_data_file.csv&quot;)) (note that for display purposes, I am using the chunk options eval = FALSE, echo = TRUE, while in a real setup chunk, I would use include = FALSE, which runs the code but does not display the code nor the output. I also had to change the chunk name to setup2 because only unique chunk names are allowed). Scroll to the setup chunk at the top of your Rmd document to see what a working setup chunk looks like. 30.13 Markdown syntax Markdown is a popular markup language that allows you to add formatting elements to text, including bold, italics, and code formatting. We make text Bold by surrounding the words with double asterisks. We make text Italic by surrounding the words with single underscores or single asterisks. We make text Bold and Italic by surrounding the words with triple underscores or triple asterisks. We can make text in code-font by surrounding it with single back-ticks. You can also format level 1 to level 5 headings. These are done by preceding the heading (on its own line) with 1-5 hashtags. 30.14 2nd Header 30.14.1 3rd Header 30.14.1.1 4th level Header 30.14.1.1.1 5th level header (note that I did not use a level 1 header, which would have forced a new chapter in bookdown). There is only one level 1 header in this chapter - the chapter title. 30.15 Line Breaks and Page Breaks If you simply hit return in your *.Rmd document, you will see a line break in your text. But this is only a semantic line break, as the knitted document will smush these lines together. You can create a deliberate line break by adding 2 or more spaces to the end of a line of text. This will work in any output format. The downside of this is that these line breaks are not visible, until you Knit the document. Line breaks can be inserted (for HTML output) by using html tags The HTML tag for line breaks is &lt;br&gt;. But these are kind of annoying to type, and 2 spaces at the end of each line is pretty easy, but less visible when you are looking at the text in the Rmd. Note that you need a blank line before headers for them to be recognized as headers and formatted properly 30.16 Making Lists 30.16.1 Ordered Lists You can create an ordered list by preceding items with numbers and period: First Second Third 30.16.2 Un-ordered lists You can create an un–ordered list with the - + or * keys as your leading bullets. chickadee carrot quartz 30.16.3 Nested Lists You can create nested lists by tab-indenting with a new symbol, using - for level 1, + for level 2, etc. Birds Hawk Vegetables Carrot Baby Ball Nantes Minerals Gneiss 30.17 The Easy Button - Visual Markdown Editing While it is helpful to know all of the formatting options for headers, italics, bold, etc. for troubleshooting, it is not as simple as a modern word processor with menu buttons. So RStudio came up with the Visual Markdown Editor. This can be activated for your RMarkdown document by clicking on a button in the top right of your Rmd document pane. It looks a bit like a compass, or possibly an Angstrom symbol (if you were a chemistry major). If you toggle this button to on, your Rmarkdown document now has basic word processor functions, including live formatting - What You See Is What You Get (WYSIWIG). A new submenu appears at the top of the document, with buttons to select the heading level (or normal text, or code block), as well as bold, italic, and underline. You can insert lists, tables, block-quotes, links, citations, cross-references within a document, images, horizontal lines, special characters, and math equations. There is a drop-down menu to help you format tables. You can even insert comments into a shared manuscript. 30.17.1 Try inserting a list, a table and a block-quote In your practice RMarkdown document, from the Rmarkdown Visual Editor View, Insert a short list of endoscopic procedures, types of dialysis, or interventions done through a catheter Insert a small 3 column table with columns for ethnicity (Hispanic/Non-Hispanic), retirement status (Retired/Not Retired), and Count Insert the following as a block quote: “Don’t judge each day by the harvest you reap but by the seeds that you plant.” -Robert Louis Stevenson 30.18 Inline Code You will often want to insert a result into your text, like a percent reduction in an endpoint, or a p value. Rather than copying and pasting from somewhere else (which are prone to error, or forgetting to update), you can do these calculations right in your text, using inline R code. For example, if you want to say that we evaluated NN COVID PCR tests for this study, you can calculate how many rows in your dataset with R, right in the text. To do this, you bracket the code with single back-ticks, and start with the letter r immediately after the first back-tick, so that Rmarkdown knows that R code is coming. After the lower-case r, you can insert your code expression, like nrow(covid) to give you the number of rows (observations). Putting this together, this looks like `r nrow(covid)`. When you insert this in the middle of a sentence in your text (inline code), this lets you write sentences in your manuscript like the following, which calculate the actual number in the text (and automatically update when your data changes): We evaluated 15524 COVID PCR tests in this study. 30.18.1 Try inserting some in-line R code Try this yourself. Select the correct code to insert the proper results, as illustrated below (note that these would be surrounded by single back-ticks) The mean cycle threshold in this study was r covid %&gt;% mean(ct_result, na.rm = TRUE) %&gt;% format(digits = 5) r covid %&gt;% median(ct_result, na.rm = TRUE) %&gt;% format(digits = 7) r covid %&gt;% sd(ct_result, na.rm=TRUE) %&gt;% format(digits =3) The standard deviation of the cycle threshold in this study was r covid %&gt;% mean(ct_result, na.rm = TRUE) %&gt;% format(digits = 5) r covid %&gt;% median(ct_result, na.rm = TRUE) %&gt;% format(digits = 7) r covid %&gt;% sd(ct_result, na.rm=TRUE) %&gt;% format(digits =3) Correct answers should produce this output when knit: The mean cycle threshold in this study was 44.122 and the standard deviation was 3.98. 30.19 A Quick Quiz Which code chunk option hides the code? eval = TRUEwarning = FALSEecho = FALSE Which code chunk always comes first, and includes libraries and data import steps? plotsetuptop_chunk What is the name of the code block (and the markup language it is written in), set off with 3 hyphens(—) at the very top of your Rmarkdown document, that tells pandoc how to format the final document? HTMLYAMLSPAMformatter What symbols do you use to make text bold in Rmarkdown? one asterisktwo asterisksunderscoresExclamation points Which {knitr} function do you use to add images to your document with a code chunk? paste()include_image()insert()insert_pic() "],["rmarkdown-output-options.html", "Chapter 31 Rmarkdown Output Options 31.1 Microsoft Word Output from Rmarkdown 31.2 PDF Output from RMarkdown 31.3 Microsoft Powerpoint Output from Rmarkdown", " Chapter 31 Rmarkdown Output Options While HTML is convenient for rapid output and iteration, most end-products for medical research are either Word documents, PDFs, or PowerPoint slides. There are many options for Rmarkdown output, including the markdown(*.md) document that results from Knitting. The *.md document is then translated by pandoc into options including: MS Word word_document PDF pdf_document PowerPoint powerpoint_presentation Rich Text Format document - rtf_document OpenDocument file odt_document LaTeX typesetting document latex_document ConTeXt typesetting document context_document HTML html_document Github-formatted markdown github_document And some other presentation formats xaringan::moon_reader slidy_presentation beamer_presentation ioslides_presentation 31.1 Microsoft Word Output from Rmarkdown Changing the output of a Rmarkdown document to Microsoft Word is easy - you can either: click on the dropdown arrow next to the Knit button, and select, Knit to Word, or change the output: option in the YAML header to word_document These will use the default formatting for Microsoft Word, which is OK, but may not produce the font, text color (too much blue!), or the heading sizes that you want. This is fixable, by using your own MS Word template document (aka style reference document). If you have a Word document named “my-styles.docx”, this can serve as your styles template. You just have to : Have the file in the same folder as the *.Rmd document, or in a nearby folder, preferably in the same project. Change the output: field of your YAML header to look like this: --- title: &quot;My Title&quot; author: Me date: January 7, 2027 output: word_document: reference_docx: my-styles.docx --- Note that the placement of the new-lines (returns), colons, and indents are critical to make this output format work. And if your reference.docx template is in a folder within your project, you need to put the path - i.e. styles-template/reference.docx if it is in the styles-template folder. 31.1.1 Making a Styles Reference File for Microsoft Word The easiest way to make a reference_docx file is to knit a new rmarkdown file to Microsoft Word, then reformat the resulting Word document to the format that you want. Then save the resulting word document as my-styles.docx in your project folder, usually near your Rmarkdown file. It is often helpful to have each level of heading (1-5) present in your template *.docx file, so that you can make sure that all of the header levels and the body text are formatted the way that you want them. 31.1.2 Let’s Practice This. Open RStudio. Open the rmd4medicine project that you created in the previous chapter (if you did not, go back and do this, then return here). Click on the Git tab in the top right in RStudio. Click on the downward green arrow icon to pull down the latest files in this project from GitHub. Open the prep folder, then open the style-rmd.Rmd file. Knit this document to Microsoft Word. Save the resulting document as word-styles.docx in the same directory as your Rmd file We will use this docx document as a template (after we reformat) 31.1.3 Re-formatting Your Template Open the word document, Select the Home ribbon, then the Styles window launcher (Styles Pane) in the Styles group. Select the Title. It has the Title style. Scroll down to Title style in the Styles Window. Change the font, the color, the size, etc. until you like the look. Then click on the dropdown arrow to the right of the Title style, and select ‘Update Title to Match Selection’. Then repeat the process for body text, Heading1, Heading2, Heading3, etc. There is a nice video walkthrough of where to click here. When you are happy with the formatting, then save this document as word-styles.docx. 31.1.4 Using Your New Styles Template Use File/New File/Rmarkdown to open a new *.Rmd file. Go to the top of your Rmarkdown document, to the YAML Header It should have 2 lines of 3 dashes, with a title, author, date, and output: html_document between them You need to change this to (note that these are real indent tabs) output: (indent)word_document: (indent)(indent)reference_docx: word-styles.docx Note the placement of the colons, what is on each line, and the number of indents are critical to making this work. 31.1.5 Now you are ready! Now save this document and knit - you will get the format that you intended. And you can use this template for any future Rmarkdown document - just be sure that you have word-styles.docx in the same folder where your Rmarkdown document lives. 31.2 PDF Output from RMarkdown Most of the time, you will not need PDF output. Many journals prefer that you submit MS Word files in *.docx format and image files in tiff, bmp, or jpeg formats. Then their web server will generate the PDF. But if you are determined to make your own PDFs from Rmarkdown files, you can. It just takes a bit of setup. 31.2.1 LaTeX and tinytex First, you need a translator for all of the things PDFs can do. A complete version uses TeX or LaTeX to get all kinds of typesetting functions. Most of the time, you need a much smaller subset of TeX, which is where the {tinytex} package comes in handy. To use {tinytex}, copy the code chunk below, and run it in your local RStudio console (once only). install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() More details on how to use and maintain {tinytex} can be found here. 31.2.2 Knitting to PDF Now that the installation of {tinytex} is complete, you can Knit to PDF. Open your favorite Rmarkdown document, and knit to PDF in one of two ways: Click on the dropdown arrow next to the Knit button, and select Knit to PDF. Edit the YAML output: key to pdf_document (with no quotes). Then click on the Knit button. This will open up a new window with your auto-generated PDF document. 31.3 Microsoft Powerpoint Output from Rmarkdown To get Powerpoint output, you can simply change the output: key to powerpoint_presentation. You can even add a formatted powerpoint template (like my-styles.pptx) to get the fonts and colors and headings the way you like them (remember to use the indents, like this: --- title: &quot;My Title&quot; author: Me date: January 7, 2027 output: powerpoint_presentatino: reference_docx: my-styles.pptx --- A simple Powerpoint template can be found [here](https://github.com/mfherman/mattherman/blob/master/static/ppt/template-no-title.pptx. But the knitted result often won’t be quite right out of the gate, as slides, unlike word documents, need explicit separators between slides. You will create a new slide each time you insert a Level1/2 Heading, or (more commonly) each time you have 3 or more dashes on a single line. Since most Word Documents don’t have a lot of Level1 Headings, an Rmd document that works well for MS Word will usually have way too many words/lines per Powerpoint slide. You will usually have to Start After the YAML with a Level 1 header Divide up your slides with 3-dash breaks ---. Start Each new slide with a Level 1 or Level 2 header Convert the text into pithy bullet points (usually level 3 and 4 headers and body text) Insert only one image or plot per slide (you can reformat them later in PPT) Set up your template pptx file - often enlarging the area for the image, and removing the separate title slide is helpful (as in the example template above). Oftentimes, you will just be wanting to get the images and plots from your project into PPT, and add text and formatting later. This will be less than completely reproducible, but can be faster than writing a whole PowerPoint presentation in Rmd. 31.3.1 Tables in Powerpoint You can use the {flextable} package to make formatted tables for Powerpoint. This package can be found here. Alternatively, many people use knitr::kable() to print out tables on PowerPoint slides. If you want more than basic formatting, you can use the {kableExtra} package, found here to style your kable tables. 31.3.2 Images in Powerpoint You can add a code chunk and create a plot on a slide with code in the code chunk, using fig.width and fig.height chunk options to control the dimensions of the plot. You can include a graphic (picture, other plot, etc.) in a code chunk with knitr::include_graphics(\"path/to/filename\"), with a code chunk like this: knitr::include_graphics(here(&quot;images/echocardiogram.jpg&quot;)) This is often helpful if you have already created your images or graphics, and just need to pull them into powerpoint from a folder. You can use chunk options like fig.width=12 and fig.height=7 to control the size of the image on the slide. Note that if you are using these options frequently, it may be easier to set these globally (for the whole presentation) in the setup chunk, as options in knitr:opts_chunk$set(), rather than in each code chunk. 31.3.3 Plots in Powerpoint Note that plots come in from R to Powerpoint as included graphics. They can be re-sized, but not edited, once they are in Powerpoint. This is good for reproducibility, but if you are still fiddling with your plots, it may be best to save the conversion to Powerpoint until the final version. You can put multiple plots on one slide if you use the {patchwork} package to set them up as a multipanel figure. If you want plots in PPT that are editable (less reproducible, but sometimes handy), you will want to use the {mschart} package, which will output charts in Microsoft format that can be included in PowerPoint slides and will still be editable within PowerPoint. "],["adding-citations-to-your-rmarkdown.html", "Chapter 32 Adding Citations to your RMarkdown 32.1 Goals for this Chapter 32.2 Packages Needed for this chapter 32.3 Getting Set Up for Citations 32.4 Building your First Zotero Collection 32.5 Adding References to Your Zotero Collection 32.6 Inserting References into Documents (Rmarkdown) 32.7 Inserting References into Documents (MS Word)", " Chapter 32 Adding Citations to your RMarkdown 32.1 Goals for this Chapter install Zotero and its web browser extensions learn how to use the Rmarkdown visual editor to add citations download csl (Citation Style Language) files to format citations for a particular journal style format citations in a particular csl style 32.2 Packages Needed for this chapter You will need {tidyverse}, {medicaldata}, and {rmarkdown}, and an updated version of RStudio (at least version 1.4). If needed, copy the block of code below into your RStudio and run the code to load these packages. You can remove the hashtags from the install.packages functions to install these packages if needed. # install.packages(&#39;tidyverse&#39;) # install.packages(&#39;medicaldata&#39;) # install.packages(&#39;scales&#39;) library(tidyverse) library(medicaldata) library(rmarkdown) 32.3 Getting Set Up for Citations You are probably familiar with some sort of commercial reference manager software, which can store and format references for manuscripts. Common products include EndNote and RefMan. RStudio markdown uses an open-source reference manager named Zotero. Zotero is free, and includes (separately) web browser extensions for Google Chrome, Firefox, and several other browsers. These “Zotero connectors” allow you to browse the web to a reference (i.e. browsing in PubMed), and allow you to save the reference to a Zotero library with a single click, without leaving your browser. We will start by downloading and installing Zotero and a web extension. Go to the Zotero web page here and download the most current version of Zotero for your computer system. Zotero download page Installation is straightforward - click on the downloaded file to start the installation process. 32.3.1 Installing the Zotero Connector It is very helpful to add a web extension plugin to your usual browser in order to make it easier to build libraries of references. Open your favorite browser and search for zotero connector. In Google Chrome, this will take you to the chrome web store, where you can click on the button “Add to Chrome” to install the Zotero Connector. You may have difficulties with Apple Safari 13, as currently (as of November 2021) the ZC extensions work better with Chrome and Firefox. The issue with Apple Safari 13 is expected(?) to be fixed by Mac OS Monterey, available in early 2022. When the Zotero Connector is installed and active, there should be a new icon at the top right of your browser window, to the right of the URL. This is a rectangle with a folded upper right corner, which looks a bit like a dog-eared page of a book. If you don’t see this, you may need to widen your browser window, or make sure that your extension is turned on (go to Manage Extensions) If you hover over this, the pop-up text should read, “Save to Zotero”. 32.3.2 Registration for Zotero If you choose to register for a free Zotero account, you can log in to get access to 300 MB of cloud reference storage for free. More storage is more expensive, in several tiers, up to the Unlimited plan which is $130 USD per year as of November 2021. This is not entirely necessary, but can be convenient to have a single site for your reference libraries accessible from any computer. 32.4 Building your First Zotero Collection Open the Zotero Application. In the left pane is a listing of your Collections of references. Click on File/New Collection (or right click on My Library) to start a new collection of references. Give it a name related to some area of your research in which you already know a few relevant references. In the example below, I created a Collection for “Severe UC” (ulcerative colitis). This is stored within your My Library folder Zotero collections 32.5 Adding References to Your Zotero Collection Now you can go to PubMed and search for some of your go-to references in this field. In the example below, the Zotero Connector is able to sense that this is a reference, and the dog-eared page icon is active in the top right of the web browser (in this case, Google Chrome). If I click on the icon, this reference is saved to my default library (the first listed), or I can use the dropdown arrow to save it to a different library. Switch back to Zotero to see that the reference has been added to your library. Pubmed reference Flip back to Pubmed in the browser, and search for other helpful references, and add them to your collection. Aim for at least 5 distinct references (for now) that we can add to an rmarkdown document. Include one on some sort of methodology that you commonly use, and at least one of your own published articles. When you find helpful references, click on the Zotero Connector icon to add them to your Zotero collection. Pubmed reference Tip - you can export your MyBibliography from your Pubmed NCBI page in RIS or BibTeX format, and then import these (Zoetero File/Import) into your My Publications collection in Zotero to have these handy. 32.6 Inserting References into Documents (Rmarkdown) Versions of RStudio 1.4+ are tightly integrated with Zotero, to make citations while you write easier. These are easily created in the RStudio Visual Editor. To use the RSVE, open RStudio. When you have RStudio open, start a new project, with File/New Version Control/Git. Enter this URL: https://github.com/higgi13425/rmd-cite.git. Name the project rmd-cite. Create the project. Now open the file ‘manuscript.Rmd’. Your file will be in the top left pane by default, and there will be an icon at the top right that looks a little like a geometry compass, or possibly more like an Angstrom symbol (if you were chemistry major). Click on this link to make your document more like a word processor, where “what you see is what you get” (WYSIWYG). You can control styles, bold, italic, numbering, etc. from the RSVE menus, which are found in a ribbon below the main Knit menu. You can toggle this visual editor on and off to see the underlying rmarkdown symbols, code, etc. when needed. Now scroll down to the sentence, “Insert the second citation here.” Click between the last “e” and the period to prepare to insert a reference. Click on the Insert menu, and select “@ Citation”. This opens up a window to Zotero. Zotero with Rmarkdown In the left sidebar, select Zotero, then (within the My Library folder) the particular library that you created. Now select the reference that you want to use. Then click the insert button. This will insert the reference into a references.bib file, which is the default bibliography file for each document. RStudio will create a new references.bib file if you don’t have one already. You can also insert citations with the @ toolbar button, or the shortcut Shift-Cmd-F8 (Mac)/ Shift-Ctrl-F8 (Windows). The citation will appear in the text like this: Inserted Reference You can click on the reference to see how it will be formatted in your bibliography. Insert 5 (or more) references into your document in the 5 locations where the text requests a reference. The storage and formatting of references works because of a few settings in the YAML header at the top of the document. The key one is bibliography: references.bib. This tells Rmarkdown where to look for the references for this document, and Zotero by default puts the references in this file (which can be found under your Files tab in RStudio. 32.6.1 Formatting your Bibliography The default formatting for Rmarkdown bibliographies is the Chicago style. You can choose from many other journal styles, using *.csl files. These Citation Style Language files are available for nearly every journal. You can download new *.csl files for the journals you frequently publish in from the Zotero Style Repository here. Just search for the desired Journal title, and download the *.csl file. Then move this file (copy/paste) from your Downloads folder to the working directory for this project. The *.csl file should be stored in same working directory as your rmarkdown file. You should already have 2 csl files in this directory (so download a different one for variety): gastroenterology.csl the-new-england-journal-of-medicine.csl When this new *.csl file is in place, you can change the bibiography style from the default by adding a line to the YAML header, just below the bibliography: references.bib line. Add a new line just below it with csl: the-new-england-journal-of-medicine.csl to set the format to the NEJM, or csl: gastroenterology.csl to set to the format of the journal Gastroenterology. See the formatting changes when you hover/click the references. Knit the manuscript with each of the csl formats. Now try knitting to the format of the other journal you downloaded. Zotero Search 32.7 Inserting References into Documents (MS Word) Zotero is bundled with a variety of plug-ins for word processors, which are added in the background. If you open your word processor, in this case, Microsoft Word, you will notice that you now have a Zotero tab on the ribbon menu. Zotero in Word Click on the Zotero menu tab, and you will have options to add a bibliography, or a particular citation. Zotero new citation The first time you try to add a citation or a bibliography, Zotero will ask you which journal to use as the default format. It has several examples available as defaults, but you will probably want to add formatting for journals specific to your specialty. You can change this later by clicking on Document Preferences within the Zotero menu. You can select one of the available styles, , click on the “Manage Styles” link below and to the right of the list of Citation Styles. This will bring up the Style Manager window shown below. Click on the “Get Additional Styles” button below the list of Styles. Zotero new styles This opens up the Zotero Style Repository. You can search for the Journal title that you want for your citation format. Once you have the formatting style sorted, you can start to insert footnotes or endnotes. A red search box will open. Start typing a search term, and options in your Zotero collection will pop up. Zotero style repository Click on one or more to select references. Then press the return key to insert the reference into your manuscript. Zotero reference If you like, you can change the formatting of all of the inserted references to a different style by clicking on Document Preferences in the Zotero menu. "],["cmd-line.html", "Chapter 33 Running R from the UNIX Command Line 33.1 What is the UNIX Command line? 33.2 Why run R from the command line? 33.3 How do you get started? 33.4 The Yawning Blackness of the Terminal Window 33.5 Where Are We? 33.6 Cleaning Up 33.7 Other helpful file commands 33.8 What about R? 33.9 What about just a few lines of R? 33.10 Running an R Script from the Terminal 33.11 Rendering an Rmarkdown file from the Terminal", " Chapter 33 Running R from the UNIX Command Line 33.1 What is the UNIX Command line? The command line is a very basic Terminal window with a prompt at which you can type commands, And do primitive but powerful things to your files. The UNIX computing environment was developed in the 1960s, and is still beloved and fetishized by brogrammers, who believe you are not truly a programmmer if you can’t code from the command line. This is silly. The major attraction to UNIX in the 1960s is that it was much better than punch cards. Which isn’t saying much. We have had 60 years of software advancement and user interface improvements, so we (most of time) should not have to put up with the inherent user hostility of the UNIX environment. UNIX is an early operating system, which is built around a ‘kernel’ which executes operating system commands, and a ‘shell’ which interprets your commands and sends them to the kernel for execution. The most common shell these days is named ‘bash’, which is a silly recursive brogrammer joke. You will sometimes see references to shell scripts or shell or bash programming. These are the same thing as command line programming. UNIX is a common under-the-hood language across many computers today, as the Apple iOS is built on top of UNIX, and the various versions of the LinuxOS are built on a UNIX-like kernel, with a similar command shell. The command line is often the least common denominator between different pieces of open-source software that were not designed to work together. It can occasionally be helpful to build a data pipeline from mismatched parts. However, there is a lot of low-quality user-hostile command line work involved to get it done, commonly referred to as “command-line bullshittery”. This is a common bottleneck that slows scientific productivity, and there is a vigorous discussion of it on the interwebs here and here (a counterpoint). Essentially, some argue that it is largely a waste of time and effort, while others see it as a valuable learning experience, like doing least squares regression by hand with a pencil. Running R from the command line is a bit like spending a day tuning your car’s engine by yourself. There is a case to be made that this will improve the efficiency and performance of your car, but it is also usually more efficient to pay someone else to do it, unless you are a car expert with a lot of free time. 33.2 Why run R from the command line? You can run R from the command line. It has none of the bells and whistles, nor any of the user conveniences of the RStudio Interactive Developer Environment (IDE). But it is how R was originally expected to be used when it was developed back in 2000 in New Zealand. Running R from the command line allows you to do powerful things, like process multiple files at once, which can be handy when you have multiple files of sequencing data from distinct observations, or you have a multistep data wrangling pipeline with several slow steps. For many years, this was the only way to easily apply code across multiple files to build a complex data pipeline. This is much less true today, with tools to handle file paths like the {here} and {fs} packages, run Python scripts from R with the {reticulate} package, run C++ scripts with Rcpp, and run bash, python, SQL, D3, and Stan scripts from Rmarkdown. You can use the {targets} package to manage multi-step data pipelines in different languages (similar to make). But some labs have been doing things at the command line for years, and find it hard to change. 33.3 How do you get started? First, you need to open a terminal window. And to do that, you need to find it. This is akin to getting under the hood of a car, and computer makers don’t exactly encourage it. 33.3.1 On a Mac Go to Finder/Applications/Utilities/Terminal 33.3.2 On a Windows PC Go to Applications/Terminal 33.4 The Yawning Blackness of the Terminal Window So, you have managed to open a terminal window, which has a standard UNIX prompt, ending in something like % or $. Not terribly helpful, is it? The bash shell is waiting for you to enter a command. No user interface for you! Let’s start with a introductory command, which can’t do any harm. Run the command below: whoami whoami ## peterhiggins Remember that UNIX started out as an operating system for terminals, and knowing who was logged in was a helpful thing, especially if the person logged in was being charged for mainframe time by the minute. You can string together two commands with a semicolon between them. Try the following: whoami;date ## peterhiggins ## Thu Apr 28 20:58:18 EDT 2022 OK, fine. This is sort of helpful. It was really important when you were on a terminal and paying by the minute for time on a mainframe back in 1969. And, on occasion, if you will need to use an entire computer cluster to run a script (or scripts) on a lot of data, you will likely have to use some of this command line knowledge. You can even schedule jobs (scripts) to run when your time is scheduled on the cluster with cron and crontab. At this point, it would be helpful to open a window with your Documents folder, and keep it side by side with the window in which you are reading this e-book. We will start working with files and directories, and it is helpful to see changes in your file/folder structure in real time. As we run commands in the bash shell, check them against what you see in the folder window. You may find that some files (dotfiles, starting with a period) are hidden from the user to prevent problems that occur when these are deleted. 33.5 Where Are We? OK, let’s start looking at files and directories. Start with the pwd command, which does not stand for password, but for print working directory. Run the code below in your Terminal window. pwd ## /Users/peterhiggins/Documents/RCode/rmrwr-book You can see the full path to your current directory. This can be a bit obscure if you are just looking at your folder structure, particularly at the beginning of the path. Fortunately, the {here} package handles a lot of this for you when you are working in Rstudio projects. We think of the directory as a tree, with a root - in this case, Users, and various branches as you build out folders and subfolders. We can move up and down the folders of the directory paths with the cd command, for change directory. Try this command in your Terminal Window, and see if you can figure out what it does. cd .. cd .. changes the directory up one level closer to the root directory. Note that there is a required space between cd and the ... You can also go directly to the root directory with cd /. It is straightforward to go up the directory tree, as each folder only has one parent. But it is tricky to go down the directory tree, as there are many possible branches/children, and you do not inherently know the names of these branches. We need to list the contents of your current directory with ls to know what is there. Try the ls command in your Terminal window cd /Users/peterhiggins/Documents/; ls ## (19) Higgins-Peter_Efficacy-and-Safety-of-Upadacitinib-recorded.mp4 ## (19) Higgins-Peter_Efficacy-and-Safety-of-Upadacitinib-recorded.pptx ## (19) Higgins-Peter_Efficacy-and-Safety-of-Upadacitinib-with-BackupBioIR.pdf ## (19) Higgins-Peter_Efficacy-and-Safety-of-Upadacitinib-with-BackupBioIR.pptx ## 1FQ_Crohn&#39;s Disease_23Oct2020 (002).doc ## 2 - AbbVie UC Core - Unmet Need_C2 (14) 738 PM ET.pptx ## 2 - AbbVie UC Core - Unmet Need_C2 (14) 954PM ET.pptx ## 2 - AbbVie UC Core - Unmet Need_C2 (7) Dr. H_PDRH_15Nov.pptx ## 2 - AbbVie UC Core - Unmet Need_C2_PDRH_29Sep.pptx ## 2012 resubmission ## 2020-Jun-05 AGA IMIBD meeting notest.docx ## 2020_Higgins_ClinResIBD_biosketch.doc ## 2021 AGA Invited Speaker Session Basic Hybrid Example.pdf ## 2021-07-13_Higgins_WH_signed_letter.docx ## 2021.Biobanking Program_InVivo_DRAFT-6.14.2021.docx ## 2021.Biobanking Program_InVivo_PDRH-6.21.docx ## 2021.Higgins AGA Distinguished Clinician.CO.docx ## 203ClareScenes080119 copy.pdf ## 2PM talks ## A is for Allspice.2.0.docx ## A is for Allspice.docx ## ABT combo proposal Bcl2:JAK ## ABT263_HIO_report_toWord.docx ## ACG U-ACHIEVE and U-ACCOMPLISH.docx ## ACG abstracts ## ACG21_P19_Efficacy and Safety of Upadacitinib Induction Therapy in Patients With Moderately to Severely Active Ulcerative Col_vSub.pptx ## ACLS eCard Peter Higgins.pdf ## AGA DDW 2021 ## AGA IMIBD ## AGA IMIBD Councilor Career Discussion Guide.docx ## AGA IMIBD Webinar Outline.docx ## AIBD CAM Higgins.pdf ## AIBD CAM Higgins.pptx ## AIBD SoMe Higgins.pdf ## AIBD SoMe Higgins.pptx ## AIBD agreement.docx ## AIBD20Template.pptx ## AJGeditorial w Fletcher MRI 2015 ## AMAG DDW Clear draft_PDRH comments.docx ## APG1244_Milestone_report.docx ## ARead_RAC-Review_PHiggins.docx ## AS Propsal outline.docx ## ASUC_UC_protocol_comments_2020.docx ## ASatishchandran Propsal outline.docx ## AXL_Helmsley_pre-proposal_Higgins.docx ## AXL_fibrosis_Nov_2021.pptx ## A_Woodward_Score Sheet_PDRH.docx ## AbbVie_Contract_2021_K000013379_-_Peter_Higgins.pdf ## AbbVie_Contract_K000013378_-_Peter_Higgins_2021_adhoc.pdf ## Abbott PtVideo 2010 Grant App ## Abbott Talks ## Abbvie_DocuSign_Dr_Higgins_Invoice.docx.pdf ## Abstract examples ## Accounts and Access (1) (1).docx ## Advice for participants in webinars.docx ## Advice for young mentors.docx ## Animation of NSAID.pptx ## Applicant Research Design Task T32.docx ## Autoimmune Summit.pptx ## Awais Chapter Stenosis ## BKochar_Frailty.pdf ## BM recommendation.docx ## Base R Notes.docx ## Beginners_GuideToR.pdf ## Best Practices Perils Excel ## Biorepository rebuttal for P30 Core.docx ## Biosketch for K.pptx ## Biosketch_2020_Higgins_ClinResIBD_biosketch.doc ## Book1.csv ## Brazil 2015 ## Brazil.ItineraryNov2015.docx ## Butter BCS Chicken.docx ## CADHUM ## CAS.K.candidate.background_SB_PDRH.docx ## CAS.T32.Project.Description-JS.docx ## CAS.career.goals.obj.development.training_PDRH.docx ## CB6 and JAK_stat.pptx ## CB6 manuscript YF.docx ## CC360_The Risk of SARS.R1.docx ## CC360_The Risk of SARS.docx ## CCC_2022 ## CCC_AZ_UC Case 1_COMPLETE.pptx ## CCF IBD Webcast 2020 Draft Deck_For Review.pptx ## CCFA EIC Candidate Interview Questions (candidates) jobin[1].doc ## CCFA Microbiota grant ## CCFA Reviewing ## CCFA SRA Microbiome 2011 ## CDC_proposal1.1.docx ## CIMI revise ## CLARE STOCKS.docx ## COVID Trials Feasibility ## CTSU Protocol Checklist_v1_PDRH_Jan2022.docx ## CYSIF.pdf ## CaltechCampus Tour &amp; Information Session.webarchive ## Cancel Appt Epic.ppt ## Causal.png ## CellDeath_DDW_2021_ISS.pdf ## Chu RPG Review_PDRH.docx ## Clare Higgins Final ## Clare Investment Summary.docx ## Cleaning Medical Data.docx ## Cleveland.2010Trip ## Clinical Coordination and Intense Proactive Monitoring to Improve Utilization of Resources and Reduce Expenditures in High.docx ## Clinical Research Alliance ## Closing remarks.docx ## Council Conversations Author Chat Guide.docx ## Coursera_Programming in R Notes.docx ## Cover Letter.docx ## Cover Letter.pdf ## CoverLetterPlus.pptx ## Crash&amp;Burn_ScriptV2_100318 copy.pdf ## DDW 2012 MTP Immunomodulator Talk ## DDW 2022 AGA Space Grid.xlsx ## DDW JAK for UC.pptx ## DDW2014SD16 ## DDW2021 CB6 powerpoint-2.pptx ## DDW2021_CB6_Antifibrotic_Higgins.pdf ## DDW21_JAK_Higgins.pdf ## Data workflow resources.docx ## DataCamp Courses by Topic.docx ## DeEscalationACG2016.pptx ## Demographics.pdf ## Documents ## DrHiggins IBD Data Request.xlsx ## Draft Postop IBD Surgery Care Protocols v2_SERedit.docx ## ECCO 2016 Amsterdam Schedule.docx ## ECCO 2019 UC PRO SS Abstract D1f_JP_UA_YO_AM_PDRH.docx ## ECCO 2022 UPA-UC Ext ind resp abstract_29Oct2021_PDRH.docx ## ECCO 2022 abstract^LN2 UPA-UC P3 Disease DurationExtent_v3.0_18Oct2021_PDRH.docx ## ECCO IHA3 ## ECCO2016Lycera30937.pptx ## EDI statement.docx ## EDI statement.pdf ## Editing your Rprofile.docx ## Efava.pptx ## Effect of medications on the recurrence of cancer in IBD patients.docx ## Electrical engineering interview questions.docx ## Endpoints in IBD talks ## Europe Talks ## Exploring Docker.docx ## FCP Sensor proposal draft.docx ## FDAtofaResponse.docx ## FFMI Kickstart-FinalReport 5-20-16-LJ.docx ## FITBITProtocol_28NOV2016_AbbVie.docx ## FITBITProtocol_4DEC2016_AbbVie.docx ## FMT_DDW_2021_ISS.pdf ## Faculty Covering Kinnucan Inbox.xlsx ## Falk Symposium Miami 3.09 ## Feasibility and Pilot Studies.pptx ## Feb2021_ibdTrials.pptx ## FellowshipRec_Janson Jacob_Higgins_JB.docx ## FellowshipRec_Janson Jacob_Higgins_JB.pdf ## Fibrosis ## Fibrosis Symposium ## Fibrosis lab talks ## FibrosisIBDCedars2016.pptx ## Figures-KC-JAMA.pptx ## Finance and Retirement Plans.docx ## Financial Priorities.docx ## Flexdashboard notes.docx ## GCPcitiCompletionReport8018282.pdf ## GI T32 Competitive Renewal FINAL 05242017.docx ## GREAT3 slides ## Garmin Notes.docx ## General Social Media Tips.docx ## General format for Chapters in RMRWR.docx ## General thoughts about query letters.docx ## Getting Started with REDCap.docx ## Gibson-Doherty_Editorial_2020_Article_FastAndCuriousAnAlgorithmicApp.pdf ## Gibson_accel_IFX.pdf ## Git for MDs_2.pptx ## GitHub ## Github for MDs_1.pptx ## Github for MDs_3.pptx ## Glover_RPG_Review_PDRH.docx ## GoToMeeting Chats ## Govani2020_Article_UseOfAcceleratedInductionStrat.pdf ## GradPartyHigginsInvites.xlsx ## GrandRounds ## HPI-5016 IBD Patient Contact Info.xlsx ## HS movie.docx ## Higgins AGA Webinar Slides.pptx ## Higgins Bio.docx ## Higgins Biosketch 2022.docx ## Higgins New IBD.pptx ## Higgins Other Support 2021-2.docx ## Higgins Other Support 2021.docx ## Higgins Refractory Proctitis.pptx ## Higgins biosketch2015KRao.doc ## Higgins biosketch2016KRao.doc ## Higgins-Peter_Efficacy-and-Safety-of-Upadacitinib-recorded.mp4 ## Higgins-peter.jpg ## HigginsACGMidwest2019_PerioperativeIBD.pptx ## Higgins_ACG2021.docx ## Higgins_IBD_AtoZ.pptx ## Higgins_LOS_IBDBiobank_Shah_Nusrat_2019.docx ## Higgins_LOS_KNewmanF32_letterhead_sig.docx ## Higgins_LOS_KNewmanF32_letterhead_sig.pdf ## Higgins_UM_CME_Pregnancy in IBD.pptx ## Higginslab server.pptx ## How To Assign PRO questionnaires - Inpatient ASUC.docx ## How To Log in to IBD Server.docx ## How To Log in to RStudio Server for HigginsLab.docx ## How To Log in to RStudio Server for Shiny.docx ## IBD 2020 - Honorarium reimbursement Form.docx ## IBD Biobank Cryostor.pptx ## IBD Center presentations ## IBD Clinical Trials for MDsDearborn2017.pptx ## IBD Databank Talks on the Road ## IBD House Call and Readmission Data .xlsx ## IBD Insurance Pilot Results.docx ## IBD Insurance Survey for CCFA Partners Existing.docx ## IBD J Club Miri extended induction.pptx ## IBD J Club Tofa in TNFexp.pptx ## IBD Journal Club 13Feb2017.docx ## IBD Journal Club July 11.docx ## IBD Osteoporosis 1 27 2022_PDRH.docx ## IBD Plexus meeting 21 Sep 2015 notes.docx ## IBD School ## IBD School 322 Script.docx ## IBD School 324 Script.docx ## IBD School 325 Script.docx ## IBD and biologics tweets.docx ## IBD in 20 years.pptx ## IBD inbox coverage.docx ## IBD video scripts ppt ## IBDInsuranceSurvey3.docx ## IBDMentoringConferenceCall4AbstractsPH.docx ## IBDSkinCa Copy.Data ## IBD_Deescalation_Apr_2019_PDRH.docx ## IBDforLansing2017.pptx ## IMG_0006.jpg ## IMG_0008.jpg ## IMG_1523st.jpg ## IMIBD Councilors 2020-21.docx ## IMIBD Partners insurance 2020DDW.pptx ## IMIBD Plenary Intro.pptx ## IMIBD_expanded_descriptors.xlsx ## INTERNAL_BUDGET_Abbvie_Nav_Rux_Sept_2021.xlsx ## Ideas for CCC 2023.docx ## Introduction to Application Supplement Photoacoustic.docx ## Invoice AV50559_Abbvie_PDRH_Dec2021.doc ## Invoice AV50559_Abbvie_PDRH_Dec2021.pdf ## JAK_DDW_2021_ISS.pdf ## JAMA Review on CD.docx ## JAMA.CD.Highlights_PDRH.docx ## JAMA_KC_Second JAMA.docx ## JAMA_Review_on_CD_Revisions_Tracked_Changes with edits_PDRH.docx ## JB_V1 Career Goals and Objectives 7.8.2020_PDRH.docx ## JB_V10 K23 Running Document.docx ## JB_V2 Candidate’s Background 7.7.2020_PDRH.docx ## JDix_Study_update.docx ## Jessica Sheehan Rec Letter Fellowship.docx ## Jessica Sheehan Rec Letter Fellowship.pdf ## Jessie Pfizer materials.docx ## Jessie Sheehan Projects ## Jun2021_ibdTrials [Autosaved].pptx ## Jun2021_ibdTrials.pptx ## K Award Institutional Letter of Commitment.pptx ## K Candidate Section.pptx ## K105_Melmed_PROs in Practice_MB_bb_JLS.pptx ## K23 Aims - Shirley Cohen-Mekelburg 11.14.19.docx ## K23_morph_measurements_MockupManuscript_21JAN2019.docx ## KP pdfs ## K_R_NIH_biosketches_2022.pptx ## Kelli Porzondek_Performance_Review_Sep_2021.docx ## Learning R discussion Jeremy Louissaint.docx ## Letter to Frank Hamilton.docx ## Library ## Lin_Reviewer Score_PDRH.docx ## List of Useful R Packages.docx ## Log in to IBD Server.docx ## Low Enrollers ACD.xlsx ## MCTSU QC Time to Activation (002).pptx ## MDOutReachIBDSlides ## MEI_2020_PH_W9.pdf ## MEI_2021_PH_W9.pdf ## MEI_2021_W9.pdf ## MEI_2022_PDRH_W9.PDF ## MEI_ACH_Wire Transfer Form.docx ## MIM-TESRIC PROTOCOL_Higgins_14Apr2020.docx ## MIM-TESRIC PROTOCOL_Higgins_26Aug2020.docx ## MM letterhead UMICH.docx ## Machine Learning Seville ## Managment of CD.pptx ## Manuscript v1.docx ## Manuscript v2.PDRH.docx ## McDonald, Nancy.pdf ## Megan McLeod Rec Letter Residency.docx ## MentoringAgendaDraftPH.docx ## Meta analysis TB vs CD version 3.5.docx ## Michigan Medicine Gastroenterology Social Media Initiative.docx ## Michigan Medicine Model for COVID-19 Clinical Trial Oversight DRAFT (KSB 04.17.20)-AL-PDRH.docx ## Microbiota Forceps ## Microsoft User Data ## MultidisciplinaryIBDClinicPHv2.docx ## NoStairs.docx ## NordicTrackTC9iTreadmillManual.pdf ## Noro paper ## Notes on Spatial data workshop.docx ## Oct2019payPDRH.PDF ## Odd college lists.docx ## P Singh K grant aims 8-25_PDRH.docx ## P2PEP slide 2020 ## P2PEP slide 2020.pptx ## P2PEP2021_IBD_COVID.pptx ## P2PEP2021_Intro.pptx ## P2PEP2021_Spatial.pptx ## PDRH endowed chair bio.docx ## PDRH short bio.docx ## PHcv2019.docx ## PHcv2020.docx ## PHcv2022.docx ## PHcv2022.pdf ## PRO agenda videos VINDICO.docx ## PRO letter.docx ## PROs and Endpoints ## PS_K grant aims 6-25_PDRH.docx ## PTM LOS From PDRH.docx ## PTM LOS From PDRH.pdf ## Package List recruit ## Patient Reported Outcomes Plan.docx ## Pearson 5 Notes.docx ## Perils of Excel.pptx ## Personal statement version 3!.docx ## Peter Higgins 2021 Vision Statment for the NSAC.docx ## Peter Higgins_Annual_Review.docx ## Peter_Higgins_photo_headshot.jpg ## Pfizer_Contract_for_Peter_Higgins_-_RD-20-D11.pdf ## Pictures ## Pitch Letter - S is for Saffron.docx ## Pitching Notes.docx ## Poppy Eulogy backup.docx ## Poppy Eulogy.docx ## Possible Eastern College Tour.docx ## Powerpoint ## Prashant Rec Letter.docx ## Prashant Rec Letter.pdf ## PredictingIBD_DDW_2021_ISS.html ## PredictingIBD_DDW_2021_ISS.pdf ## Prj21015 UPA UC Other PROs manuscript D1 jp_JOL wz-di_JT_PDRH.docx ## Proposal for MCTSU Study Accrual Monitoring.docx ## PtEdOnRoad ## Purdue Disclosure Form_Higgins.docx ## Pyoderma Case ## Question 16.docx ## Quiros SRA- Higgins LoS draft_PDRH.docx ## Quiros SRA- Higgins LoS draft_PDRH.pdf ## R01 Fibrosis 2012 ## R01 ML revision ## R01 MachineLearn2010 ## R01.US.Revision ## RCode ## RMed21- Intro to Spatial.pptx ## RMed21-Opening remarks Day 1.pptx ## RMed21-Opening remarks Day 2.pptx ## Ramp up clinical research_PH.xlsx ## Ramping up human subject research - MM 6-1-20 _KDA_PDRH_suggestions.docx ## Recordings ## Regueiro Chapter ## Reply_JAMA_Thiopurines.docx ## Research Statement.docx ## Research Statement.pdf ## Review Criteria for COVID Clinical Trials.docx ## Review guidelines_2017.docx ## Roasted Salted Cashews.docx ## Ryan CDA ## Ryan K ## S is for Saffron 3.0.docx ## S is for Saffron 3.1.docx ## S is for Saffron 3.2.docx ## S is for Saffron.2.0.docx ## SAINI-LOK-HIGGINS_T32 GI Fellowship Research Presentation 08292021.pptx ## SCM Mentor Letters.docx ## SEAN STOCKS.docx ## SIG_Template_IBD Program_FINAL.docx ## SPECIFIC AIMS 2_PDRH.docx ## SPir abstract 2700 ## Scanner pictures ## Scheduling Epic Schedule.pptx ## Scheduling Epic Schedules.pptx ## Scoring Sheet_Albin_PDRHiggins.docx ## Scoring Sheet_Janda_PDRHiggins.docx ## Screenwriting Contests.docx ## Sean Common App academic honors list.docx ## Sean Common App activities list.docx ## Sean Higgins Bordogni.mp4 ## Sean Higgins Brag Sheet.docx ## Sean Investment Summary.docx ## Sean Resume Tabular VBorder.docx ## Sean Resume Tabular.docx ## Sean Resume.docx ## Sean Summer Priorities 2016.docx ## Seattle Talk Dec 2012 ## SecureIBD.pptx ## Severe UC protocol ## SevereUC_Tofa_Presentation_IBD_Forum.pptx ## Shail CT ## ShareRmd.html ## Sheehan Pfizer IBD Fellowship.docx ## Sherman Prize Nominee Questions.docx ## Shoreline West Tour Information.docx ## Short PA slides.pptx ## Shotwave thread.docx ## Should we accel IFX - survey.pdf ## Signing Clinical Research Infusion Orders.pdf ## SingleCell_DDW_2021_ISS.pdf ## SkinCancer.IBD.Gentics_Yanhua_PDRH.docx ## Slade UC review Walter ## SoMe_use_2020.png ## Social Media for GI.pptx ## Soulfege t shirts ## Source Code PT1.docx ## Specialty Pharmacist Referral Process_11162021.pptx ## Specific Aims.pdf ## Stairs.docx ## Stelara paper.docx ## Stelara paper_revised_PDRH_KCC.docx ## Structure of Aim 3.docx ## Surgery Topics ## Surveys ## T32 Competitive Renewal 2017 FINAL WM.pdf ## T32 summary statement.pdf ## T32_current_text_14June2019.docx ## TOPPIC ML draft v5SCM_YL_AKW_PDRH.docx ## TabaCrohn IBD J club.docx ## Tables.docx ## Taiwan talks ## Takeda Grants_Letter of Request_IBD School Videos_Medication Series_2018_.docx ## Takeda_IBD School Videos_Submission.pdf ## Task List 2020-2.docx ## Task List 2020-5.docx ## Task List 2020.docx ## Task List 2021.docx ## Task List 2022.docx ## Tenure Plan ## Testing signatures with Adobe.pdf ## The Risk of SARS.R1.Markup.docx ## Thiopurine talks Manitoba 2011 ## Thiopurines talks Vandy ## Tidymodels.docx ## Timelines for K submission.pdf ## Timelines for K submission.pptx ## Tips for Submitting DDW Abstracts for 2022.docx ## Tofa Inpatient ## Tofa in ICI Figure Legends_Final Draft_V2.docx ## Tofa inpatient induction Protocol_02NOV2018_PHforEdits.docx ## Tofa_Presentation_2_10_2021.pptx ## Toffee Separation Tips.docx ## UC CD Impact Manuscript Tables__19Feb2021_PDRH.docx ## UC and CD Impact Manuscript_Draft 2_9Jan2022_PDRH.docx ## UC and CD Impact Manuscript_Draft1_19Feb2021_PDRH.docx ## UC2.jpg ## UCB Pt video 2010 Grant App ## UCRx_DDW_2021_ISS.pdf ## UC_protocol_comments_2020.docx ## UEGW 2010 ## UEI stidham files ## UM IBD Clinical Trials IBD referral form.docx ## UM Severe UC Protocol.docx ## UPA_U_ACHIEVE 1st draft_PDRH.docx ## Untitled.docx ## Upa ASUC Concept Page.docx ## Upa M14-234 SS3 maintenance Q and A ECCO 2022.docx ## Upa outpatient vs Prednisone_PDRH.docx ## V8 Infliximab Outcomes_PDRH.docx ## VINDICO IBD ## VINDICO_PRO.pptx ## VideoVisitSchedulingQuickApptsforProviders.pdf ## VincentChen_K specific aims 2020-10-25.docx ## VirtualPtEdMar2020.v2.pdf ## Walk Letter of Reference.docx ## Walk Letter of Reference.pdf ## WebEx ## Why not excel.docx ## Zoom ## Zwift ## Zwift-Gift-Card.pdf ## aga institute council july 2020 meeting.pdf ## algorithms_thiopurine.pdf ## base-r-cheatsheet.pdf ## biomakers_fibrosisPDRH.docx ## blue_down_arrow - Gravit Designer.html ## bmj_imputation.pdf ## bowel disease_2108_PDRH.docx ## cgh_factors_utilization.pdf ## clare_stocks_long_term.xlsx ## cycling core exercises.docx ## draft_tokenization letter Risa_Uste.docx ## early-career-faculty_Dec-2020.xlsx ## epic cancel_reschedule appointments.ppt ## epic schedule viewing_close.ppt ## escalator.html ## exercise1.xlsx ## exercise2.xlsx ## fellow graduation 2020.docx ## hershey_long_term.xlsx ## hexStickers.jpg ## higgins2x3.jpg ## iBike Rides ## imaging_stricture.xlsx ## introduce_clare.docx ## jama_cushing_crohn_review_2021.pdf ## learnr app diagram.jpg ## learnr app diagram.pptx ## letter Lowrimore.docx ## medicaldata_NHS_R_2021.pptx ## medicaldata_Rmedicine21.pdf ## medicaldata_Rmedicine21.pptx ## medicaldata_image_hex.ai ## mockstudy manuscript draft.docx ## nejm1966_beecher_ethics.pdf ## nejm_indomethacin.pdf ## nejm_statins.pdf ## orange_green_down_arrow - Gravit Designer.html ## orange_green_down_arrow- Gravit Designer.pdf ## pdrh_IBD_email.xlsx ## personal statement fellowship_PDRH.docx ## peterhiggins.jpg ## pink_down_arrow - Gravit Designer.html ## pink_up_arrow - Gravit Designer.pdf ## seq-6.pdf ## signature.docx ## signature.fld ## signature.html ## signature.pdf ## signature.png ## sorted_steno.xlsx ## stiff_bcl.R ## submitJanssen_IBD School Videos_12Jul2018.pdf ## tidyr_pivot.png ## tidyr_pivot.xcf ## tofa_checkpoint.pdf ## twitter.com.webarchive ## ucla1.jpg ## untidy_sheets.pptx ## user_testing_learnr tutorials.pptx ## wga_min20.pdf ## zwift_training_pacepartner.xlsx ## {&quot;Attachments&quot;-[{&quot;__type&quot;-&quot;F.textClipping ## ~$ Severe UC Protocol.docx ## ~$Jun2021_ibdTrials.pptx ## ~$T Review Higgins.docx ## ~$sk List 2020-5.docx ## ~$sk List 2020.docx ## ~$sk List 2021.docx You will see a listing of all files and folders in the current directory. You can get more details by adding the option (sometimes called a flag) -l cd /Users/peterhiggins/Documents/; ls -l The full listing will give you more details, including read &amp; write permissions, file size, date last saved, etc. Many UNIX commands have options, or flags, that modify what they do. Find a folder inside of your Documents folder. We will now go down a level in the directory tree. In my case, I will use the Powerpoint folder. In your Terminal window: change the directory to the Powerpoint directory list the contents of this folder cd /Users/peterhiggins/Documents/Powerpoint; ls ## 2016IBDClinTrialsforMDsDearborn.pptx ## 2016IntegratedDeckorMDsGB.pptx ## 2019 SCSG GI Symposium IBD SoA - Read-Only.pptx ## ADTC Flowchart-draft-RWS_14FEB2015.docx ## Acutely Ill IBD Patient protocol for ADTC-RWS.docx ## Annual Research Career Review 2021PH.pptx ## BE LGD Dearborn 2016.04.12.pptx ## CCF_Clinical_Trials.pptx ## CP1_Higgins Intro.pptx ## CP2_DiagnosingIBD_KC.pptx ## CP3_FMT_MMM.pptx ## CP4_SurgeryRegenbogen.pptx ## CP5_NutritionIBD_EH.pptx ## CP6_Infections2018.pptx ## CP7_PsychologicalStress_Riehl IBDFlint2018.pptx ## CP8_SteroidsWaljee.pptx ## CP9_ClinicalTrials2018.pptx ## ECCO 2022 UPA-UC ext duration oral_v2.0_3Feb2022_for QC.pptx ## ECCO22_Template.pptx ## Feasibility and Pilot Studies CTA.pptx ## FibrosisIBDCedars2016.pdf ## Getting Started in RStudio.pptx ## Higgins CCFA CTPROs in IBD.pptx ## Higgins CCFACT2017FundingClinicalResearch.pptx ## Higgins HK 2017IBD Nursing and Quality of Care.pptx ## Higgins HK 2017The Gut Microbiota and the Pathophysiology of IBD.pptx ## Higgins Microbiota for IBD Patient Ed.pptx ## HigginsDec2018AJG_SmokingStatus.pptx ## HigginsFALKMadridThioMtx2017.pptx ## Higgins_CCC_2018_Refractory_Rising_Bar_v3.pptx ## IBD and PTSD.pptx ## IBDUpdate.pptx ## IOIBD.Fibrosis.Higgins.2018.Amsterdam.pptx ## Integrated Slide Deck Dearborn 2016.04.12.pptx ## MER Stress Management Dearborn 4-14.pptx ## MichiganMedicine-IBDTemplate.potx ## Outcome Measures CTA.pptx ## PDRH RCR 2020.pptx ## PDR_Higgins_DeficienciesInIBD_AIBD2021.pptx ## PennThioMTX2017Higgins.pptx ## PragueRefractoryRisingBar2017.pptx ## Pregnancy in IBD.pptx ## Presentation1.pptx ## Regenbogen CRS for GI CME Course2016.pptx ## Senior Slide Show.pptx ## Social Media for GI.pptx ## ThomsonRectalStumpComplicationsIBD2_13.pptx ## UEGweek2020.pptx ## UMHS IBD ADTC Encounter Note-DRAFT-RWS_15FEV2015.docx ## UMHS Talk- Moving Beyond AntiTNF 4-2016 FINAL v2.pptx ## UMich COVID-19 IBD.pptx ## Update on COVID and IBD.pptx ## Vertebrate Animals for K.pptx ## VirtualPtEdMar2020.v2.pptx ## VirtualPtEdMar2022Jan.pptx ## VirtualPtEd_2022_Deck.pptx ## VirtualPtEd_2022_Feb.pptx ## Writers Room.pptx ## ibd_meds_surgery_metan.pptx Great! You moved to a new directory and listed it. Now we will get fancy, and make a new directory within this directory with the mkdir command. Try this in your Terminal window: pwd; mkdir new_files; ls You have now made a new directory (folder) within the previous directory (pwd = present working directory), named new_files. Verify this in your Documents folder. You can now make changes to this directory and list the contents (it should currently be empty). Try this out in your Terminal Window (note edit the cd command to your own directory path). cd /Users/peterhiggins/Documents/Powerpoint/new_files; ls Note that you can abbreviate the current directory with ., so that you could have also used cd ./new_files You can create a new (empty) file in this directory with the touch command. Sometimes you need to create a new file, then write data to it. Try this out touch file_name; ls You can also create a file with data inside it with the cat &gt; command. Type in the following lines into your Terminal window. When complete, type control-D to be done and return to the Terminal prompt. cat stands for concatenate. cat &gt; file2.txt cat1 cat2 cat3 Now you can list the contents of this file with the cat command below. Give this a try cat file2.txt You can also list the directory of your new_files folder with ls to see the new folder contents. Try this ls Note that you don’t need to use the Terminal to run bash commands. You can do this from an Rmarkdown file. Take a moment to run pwd in your Terminal, to get the current directory. Now open Rstudio, and a new Rmarkdown document. Copy the path to the current directory from the Terminal. Switch back to the Rmarkdown document. Select one of the R code chunks (note the {r} at the top) and delete it. Now click on the Insert dropdown at the top of the document, and insert a Bash chunk. Now you can add UNIX commands (separated by a semicolon) to this code chunk, like cd (paste in path here); pwd; ls; cat file2.txt Then run this chunk. Now you can run terminal commands directly from Rmarkdown! 33.6 Cleaning Up OK, now we are done with the file file2.txt and the directory new_files. Let’s get rid of them with rm (for removing files) and rmdir for removing directories. In order, we will - Make sure we are in the right directory - remove the file with rm file2.txt - go up one level of the directory with cd .. - remove the directory with rmdir new_files Give this a try pwd; rm file2.txt; cd ..; rmdir new_files Verify all of this in your Documents window. This is great. But you can imagine a situation in which you mistakenly rm a file (or directory) that you actually needed. Unlike your usual user interface, when a file is removed at the command line, it is gone. It is not in the trash folder. It is really gone. There is something to be said for modern user interfaces, which are built for humans, who occasionally make mistakes. Sometimes we do want files or folders back. 33.7 Other helpful file commands Here are some file commands worth knowing cat filename - to print out whole file to your monitor less filename - to print out the first page of a file, and you can scroll through each page one at a time head filename - print first 10 lines of a file tail filename - print last 10 lines of a file cp file1 file2 - copy file1 to file2 mv file1.txt file.2.txt file3.txt new_folder - move 3 files to a new folder 33.8 What about R? So now you can get around directories, and find your files in the Terminal window, but you really want to run R. You can launch an R session from the Terminal Window (if you have R installed on your computer) by typing the letter R at the Terminal prompt Launch R R You get the usual R intro, including version number, and the R&gt; prompt. Now you can run R in interactive mode with available datasets, or your own datasets. Try a few standard tidyverse commands with the mtcars dataset. Give the examples below a try. You can use q() to quit back to the terminal (and reply “n” to not save the workplace image). head(mtcars) ## mpg cyl disp hp drat wt qsec vs am ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 ## gear carb ## Mazda RX4 4 4 ## Mazda RX4 Wag 4 4 ## Datsun 710 4 1 ## Hornet 4 Drive 3 1 ## Hornet Sportabout 3 2 ## Valiant 3 1 mtcars %&gt;% filter(mpg &gt; 25) %&gt;% select(starts_with(&#39;m&#39;)|starts_with(&#39;c&#39;)) ## mpg cyl carb ## Fiat 128 32.4 4 1 ## Honda Civic 30.4 4 2 ## Toyota Corolla 33.9 4 1 ## Fiat X1-9 27.3 4 1 ## Porsche 914-2 26.0 4 2 ## Lotus Europa 30.4 4 2 33.9 What about just a few lines of R? Sometimes you will want to call R, run some code, and be done with R. You can call R, run a few lines, and quit in one go. You can add the flag -e (for evaluate) to the call to R, and put the R commands in quotes. Try the example below (note that this will not work if you are still in R - be sure you are back in the terminal with the % or $ prompt) R -e &quot;head(mtcars)&quot; or this example - note that single or double quotes does not matter - as long as they match. Try this R -e &#39;install.packages(palmerpenguins)&#39; You can also string together several commands with the semicolon between them. Try the example below. R -e &#39;library(palmerpenguins);data(penguins);tail(penguins)&#39; 33.10 Running an R Script from the Terminal Now we are stepping up a level - you have an R script that you have carefully created and saved as the myscript.R file. How do you run this from the Terminal? This is straightforward - you can call the Rscript command with your file name. Pick out a short R file you have written, make sure you are in the right directory where the file is, and use it as in the example below. Rscript myscript.R This launches R, runs your script, saves resulting output (if your script includes save or ggsave commands), closes R, and sends you back to the Terminal. Very nice! 33.11 Rendering an Rmarkdown file from the Terminal This is a little different, as you can’t just run an Rmarkdown file. Normally you would use the dropdown button to knit your file from Rstudio. But you can use the rmarkdown::render command to render your files to HTML, PDF, Word, Powerpoint, etc. Pick out a small Rmd file like output_file.Rmd below, make sure you are in the right directory where the file is, and try something like the example below. Note that this is one case where nesting different types of quotes (single vs. double) can come in handy. It helps to use single quotes around your filename and double quotes around the rmarkown::render command. Try it out Rscript -e &quot;rmarkdown::render(&#39;output_file.Rmd&#39;)&quot; So there you have it! Just enough to get you started with R from the command line. For further reading, check out these helpful links: Data Science at the Command line (e-book) R in Batch mode on Linux R tutorial for a Unix Environment The Linux Command Line: A Complete Introduction - a whole book on the topic Software Carpentry Command-Line Programs Scheduling jobs with cron "],["title-holder.html", "Title holder", " Title holder "],["references-1.html", "References", " References "]]
