[["index.html", "Reproducible Medical Research with R Chapter 1 Preface 1.1 Who This Book is For 1.2 Prerequisites 1.3 The Spiral of Success Structure 1.4 Motivation for this Book 1.5 The Scientific Reproducibility Crisis 1.6 Features of a Bookdown electronic book 1.7 What this Book is Not 1.8 Some Guideposts 1.9 Helpful Tools", " Reproducible Medical Research with R Peter D.R. Higgins, MD, PhD, MSc 2021-07-22 Chapter 1 Preface Welcome to Reproducible Medical Research with R (RMRWR). I hope that this book is helpful to you. 1.1 Who This Book is For This is a book for anyone in the medical field interested in analyzing the data available to them to better understand health, disease, or the delivery of care. This could include nurses, dieticians, psychologists, and PhDs in related fields, as well as medical students, residents, fellows, or doctors in practice. I expect that most learners will be using this book in their spare time at night and on weekends, as the health training curricula are already packed full of information, and there is no room to add skills in reproducible research to the standard curriculum. This book is designed for self-teaching, and many hints and solutions will be provided to avoid roadblocks and frustration. Many learners find themselves wanting to develop reproducible research skills after they have finished their training, and after they have become comfortable with their clinical role. This is the time when they identify and want to address problems faced by patients in their practice with the data they have before them. This book is for you. 1.2 Prerequisites Thank you for giving this e-book a try. This is designed for physicians and others analyzing health data who are interested in pursuing this field using the R computer language. We will assume that: You have access to a computer You have access to the internet You can download and install software from the internet to your computer How to download and install R and RStudio will be addressed, step by step, in Chapter 2. 1.3 The Spiral of Success Structure This book is structured on the concept of a “spiral of success”, with readers learning about topics like data visualization, data wrangling, data modeling, reproducible research, and communication of results in repeated passes. These will initially be at a superficial level, and at each pass of the spiral, will provide increasing depth and complexity. This means that the chapters on data wrangling will not all be together, nor the chapters on data visualization. Our goal is to build skills gradually, and return to (and remind students of) their previously built skills in one area and to add to them. The eventual goal is for learners to be able to produce, document, and communicate reproducible research to their community. 1.4 Motivation for this Book Most medical providers who learn R to do their own data analysis do it on their own time. They rarely have time for a semester-long course, as their clinical schedules usually will not allow it. Fortunately, a lot of people learn R on their own, and there is a strong and supportive R community to help new learners. A 2019 Twitter survey conducted by @RLadies found that more than half of respondents were largely self-taught, from books and online resources. There are a lot of good resources for learning R, so why one more? In part, because the needs of a medical audience are often different. There are distinct needs for protecting health information, generating a descriptive Table One, using secure data tools like REDCap, and creating standard medical journal and meeting output in Word, Powerpoint, and poster formats. Further, while learning from a textbook can be helpful, this e-book has the ability to include interactive features that are important for learning to write your own analysis code. Informative flipbook demonstrations will show you what steps in R code do, and learnr exercises will give you a chance to do your own coding to solve problems, right within this e-book. More and more, all science is becoming data science. We are able to track patients, their test results, and even the individual pixels (voxels) of their CT scans electronically, and use those data points to develop new knowledge. While one could argue that health care workers should collect data and bring it to trained statisticians, this does not work nearly as well as you might expect. Most academic statisticians are incentivized to develop new statistical methods, and are not very interested (nor incentivized) to do the hand-holding required to wrangle messy clinical data into a manuscript. There also are simply not enough statisticians to meet the needs of medical science. Having clinicians on the front lines with some data science training makes a big difference, whether in 1854 in London (John Snow) or in 2014 in Flint, Michigan (Mona Hanna-Atisha). Having more clinicians with some data science training will impact medical care, as they will identify local problems that would have otherwise never reached a statistician, and probably never been addressed with data otherwise. 1.5 The Scientific Reproducibility Crisis Beginning as far back as 1989, with the David Baltimore case, and increasingly and publicly through the 2010s, there has been a rising tide of realization that a lot of taxpayer-funded science is done sloppily, and that our standards as scientists need to be higher. The line between carelessly-done science and outright fraud is a thin one, and the case can be made that doing science in a sloppy fashion defrauds the funders, as it leads to results that can not be reproduced by the authors nor replicated by others. Particularly in medicine, where incorrect findings can cause great harm, we should take special care to do scientific research which is well-documented, reproducible, and replicable. This topic as a motivating force for doing careful medical research will be expanded upon in Chapter 1. 1.6 Features of a Bookdown electronic book 1.6.1 Icons There are several icons at the top left, to the right of the clickable RMWR link, that can be helpful: 1. The Table of Contents Sidebar - Click on the ‘hamburger’ menu icon (three horizontal lines) or the s key to toggle the sidebar (table of contents) on and off. Within the sidebar, you can click on whichever chapter or subsection you want. 2. This book is Searchable - Click on the magnifying glass or use the f key to toggle the Find box and search for whatever you need to find. 3. You can change the font size, font, and background by clicking on the A icon. 4. You can download the chapter with the download icon (downward arrow into a file tray) in PDF or EPUB formats. 1.6.2 Sharing At the top right, there are several icons for sharing links to the current chapter through social media. 1.6.3 Scrolling/Paging You can scroll up and down within a chapter with your mouse, or use the up and down arrow keys. You can page through chapters with the left and right arrow keys. 1.7 What this Book is Not 1.7.1 This Book is Not A Statistics Text This is not an introduction to statistics. I am assuming that you have learned some statistics somewhere in secondary school, undergraduate studies, graduate school, or even medical school. There are lots of statisticians with Ph.D.s who can certainly teach statistics much more effectively than I can. While I have a master’s degree in Clinical Research Design and Statistical Analysis (isn’t that a mouthful!) from the University of Michigan, I will leave formal teaching of statistics to the pros. If you need to brush up on your statistics, no worries. There are several excellent (and free!) e-books on that very topic, using R. Some good examples include (go ahead and click through the blue links to explore): Learning Statistics with R (LSR) Open Intro Statistics Modern Dive Teacup Giraffes We will cover much of the same material as these books, but with a less theoretical and more applied approach. I will focus on specific medical examples, and emphasize issues (like Protected Health Information) that are particularly important for medical data. I am assuming that you are here because you want to analyze your own data in your (probably) very limited free time. 1.7.2 This Book Does Not Provide Comprehensive Coverage of the R Universe This book is also far from comprehensive in teaching what is available in the R ecosystem. This book should be considered a launch pad. Many of the later chapters will give you a taste of what is available in certain areas, and guide you to resources (and links) that you can explore to learn more and do more beyond the scope of this book. The R computer language has expanded far beyond statistics, and allows you to do many powerful things to improve your workflow, make amazing graphics, and share results with others. 1.8 Some Guideposts Keep an eye out for helpful Guideposts, which look like this: Warnings This is a common syntax error, especially for beginners. Watch out for this. Tips This is a helpful tip for debugging. Try It Out Take what you have learned and try it yourself on your own computer. Challenge - take the next step and try a more challenging example. Try this more complicated example. Explore More - resources for learning more about a particular topic. If you want to learn more about Shiny apps, go to https://mastering-shiny.org to see an entire book on the topic. 1.9 Helpful Tools Throughout this book you will find flipbook code demonstrations and learnr code interactive exercises in which you can practice writing R code right in the book. Let’s explain how to use these demonstration flipbooks and learnr exercises. 1.9.1 Demonstrations in Flipbooks Flipbooks are windows in this book in which you can watch R code being built into pipelines, and see the results at each step. Each flipbook demonstrates some important code concepts, and often new functions in R. You can click on the window to activate it, and the fullscreen (4 arrows) icon to expand it to the full screen. Then use the left and right arrow keys to go forward and back in the code, one step at a time. You will want to go through these slowly, and make sure that you understand what is happening in each step. You may even want to take notes, particularly on the function syntax, as you will likely coding exercises with these functions shortly after the flipbook demonstration. Take a look at the example of a flipbook below. Activate it by clicking on it, and use the expand icon (4 arrows at the lower right) to make it full screen. You can step forward and backward through the pipeline of code with the right and left arrow keys. Watch the results of each step. 1.9.2 Learnr Coding Exercises Learnr coding exercises are windows in this book in which you can write your own R code to solve a problem. Each learnr exercise tests whether you have mastered important code concepts, and often new functions in R. If needed, you can reset to a fresh code window with the Start Over button. You can type lines of code into the window, then click on the Run Code button at the top right to run the code and get your results. Your code may not produce the right result the first time, and you will have to interpret the error message to figure out how to fix it. Rely on the text and your notes and the demonstrations to help you. If you are stuck, you can click on the Hint button to see an example of correct code, and compare it to your own. If you would like, you can even copy this code to the clipboard with the Copy button and Take a look at the example of a learnr exercise below. There is a dataset piped into a series of functions (‘verbs’), with a blank. Fill in the blank with ‘p_vol’ (without the quotes), which stands for the variable, prostate volume. Then run your code with the Run Code button to get a result. Practice using the Start Over button, the Hint button (there may be more than one - usually the last one is the solution), and the Copy To Clipboard button. When you get a table of data as a result from a code pipeline, it may have more columns (variables) than can be displayed easily. When this is the case, there will be a black arrow pointing rightward at the top right of the table of results. Click on this to scroll right and see more columns. A table of data as a result from a code pipeline may also have more rows (observations) than can be displayed easily. When this is the case, the table will be paginated, with 10 rows per page. At the bottom right of the table, there will be a clickable listing of pages, along with Previous and Next buttons. Click on these buttons (or the page number buttons) to see more pages of data to inspect and check your results. 1.9.3 Coding An important note on writing your own code: you should always have an internet search window open when you are writing code. No one can remember every function, nor the correct arguments and syntax of each function. A critical skill in writing code is searching for how to do something correctly. This is not a sign of weakness. Professional programmers google “how do I do x?” many times each day. This is how programming is done. You will often search for things like “how do I do x in R?” or “how to x in tidyverse”. This is completely normal, and to be expected. You do not have time to memorize hundreds of functions, and you may have days or even weeks between coding sessions (because of your day job), making it hard to remember all the details from your last coding session. This is not a problem. There are lots of websites that can help you solve specific problems, as you will find in the How to Find Help chapter. "],["getting-started-and-installing-your-tools.html", "Chapter 2 Getting Started and Installing Your Tools 2.1 Goals for this Chapter 2.2 Website links needed for this Chapter 2.3 Pathway for this Chapter 2.4 Installing R on your Computer 2.5 Windows-Specific Steps for Installing R 2.6 Mac-specific Installation of R 2.7 Installing RStudio on your Computer 2.8 Installing Git on your Computer 2.9 Getting Acquainted with the RStudio IDE", " Chapter 2 Getting Started and Installing Your Tools One of the most intimidating parts of getting started with something new is the actual getting started part. Don’t worry, I will walk you through this step-by step. 2.1 Goals for this Chapter Install R on your Computer Install RStudio on your Computer Install Git on your Computer Get Acquainted with the RStudio IDE 2.2 Website links needed for this Chapter While in many chapters, I will list the R packages you need, in this chapter, you will be downloading and installing new software, so I will list the links here for your reference. https://www.r-project.org https://rstudio.com/products/rstudio/download/ https://git-scm.com/downloads 2.3 Pathway for this Chapter This Chapter is part of the TOOLS pathway. Chapters in this pathway include Getting Started and Installing Your Tools Using the RStudio IDE Updating R, RStudio, and Your Packages Advanced Use of the RStudio IDE When You Don’t Want to Update Packages (Using renv) Major R Updates (Where Are My Packages?) 2.4 Installing R on your Computer R is a statistical programming language, designed for non-programmers (statisticians). It is optimized to work with data in rectangular tables of rows (observations) and columns (variables). It is a very fast and powerful programming engine, but it is not terribly comfortable or convenient. R itself is not terribly user-friendly. It is a lot like a drag racing car, which is basically a person with a steering wheel strapped to an airplane engine. drag racer Very aerodynamic and fast, but not comfortable for the long run (more than about 8 seconds). You will need something more like a production car, with a nice interior and a dashboard, and comfy leather seats. dashboard This equivalent of a comfy coding environment is provided by the RStudio IDE (Integrated Developer Environment). I want you to install both the R statistical language and the RStudio IDE, in that order. Let’s start with installing R. R is free and available for download on the web. Click on the following link to go to the r-project website to get started. This screen will look like this You can see from the blue link (download R) that you can use this link to download R, but you will be downloading it faster if you pick a local CRAN mirror. You might be wondering what CRAN and CRAN Mirrors are. Nothing to do with cranberries, fortunately. CRAN is the Comprehensive R Archive Network. Each site (mirror) in the network contains an archive of all R versions and packages, and the sites are scattered over the globe. A CRAN Mirror maintains an up to date copy of all of the R versions and packages on CRAN. If you use the nearest CRAN mirror, you will generally get faster downloads. At this point, you might be wondering what a package is… A package is a set of functions and/or data that you can download to upgrade and add features to R. It is like a downloadable upgrade to a Tesla vehicle that lets you play the video game Witcher 3 on your console, but more useful. Another useful analogy for packages is that they are like apps for a smartphone. When you buy a new smartphone, it only comes with the basic apps that allow it to work as a phone, and a few other things, like a notepad and a calculator. If you want to do cool things with your smartphone, you download apps that allow your smartphone to have new capabilities. That is what packages do for your installation of R. Now let’s get started. Click on the blue link that says “download R”. This will take you to a page to select your local CRAN Mirror , from which you will download R. cran Scroll down to your local country (yes, the USA is at the bottom), and a CRAN mirror near you. This is an example from the state of Michigan, in the USA. usa-mirrors Once you click on a CRAN Mirror site to select the location, you will be taken to the actual Download site. install Select the link for the operating system you want to use. We will walk through this with Windows first, then Mac. If you are using a Mac, skip forward to the Mac install directions in section 2.6. If you are computer-savvy enough to be using Linux, you can probably figure it out on your own (it will look a lot like these). 2.5 Windows-Specific Steps for Installing R If you are installing R on a Mac, jump ahead to the Mac-specific version below in section 2.6. On Windows, once you have clicked through, your next screen will look like this: install2 You want to download both base and Rtools (you might need Rtools later to build packages). The base link will take you to the latest version, which will look something like this. install3 Click on this link, and you will be able to save a file named R-N.N.N-win.exe (Ns depending on version number) to your Downloads folder. Click on the Save button to save it. install4 Now, go to your Downloads folder in Windows, and double click on the R installation file (R-N.N.N-win.exe). Click Yes to allow this to install. install5exe Now select your language option. install_language You will be asked to accept the GNU license - do so. Click Yes to allow this to install. Then select where to install - generally use the default- a local (often C) drive - we usually do not install on a shared network drive or in the cloud. install_drive Then select the Components - generally use the defaults, but newer computers can skip the 32 bit version. install_components In the next dialog box, accept the default startup options. install_defaults You can choose the start menu folder. The default R folder is fine. install_start If you want a shortcut icon for R on your desktop, you can leave this checked. But most people start RStudio, with R running within RStudio, rather than directly starting R. You can choose to create an R shortcut on your desktop in the next dialog box. You can delete it later if you don’t like it. install_addltasks Then the Setup Wizard will appear - click Finish, and the rest of the installation will occur. install_wizard 2.5.1 Testing R on Windows Now you want to test whether your Windows installation was successful. Can you find R and make it work? This is easy if you chose to make an R shortcut. If not, hunt for your C folder, then for OS-APPS within that folder. Keep drilling down to the Program Files folder. Then the R folder, and the current version folder within that one (R-N.N.N). Within that folder will be the bin folder, and within that will be your R-N.N.N.exe file. Double click on this to run it. The example paths below can help guide you. install_path2 install_path Opening the exe file will open a classic year 2000-era terminal window, called Rterm, with 64 bit if that is what your computer uses. The version number should match what you downloaded. The messaging should end with a “&gt;” prompt. install_term At this prompt, type in: paste(“Two to the seventh power is”, 2^7) (don’t leave out the comma or the quotes) - then press the Enter key. This should produce the following: Two to the seventh power is 128 install_test Note that you have explained what is being done in the text, and computed the result and displayed it. 2.6 Mac-specific Installation of R The installation for Mac is very similar, but the windows look a bit different. If you are working with Windows, jump ahead at this point to Installing RStudio in section 2.7. At the Download Version page, you click on the Mac Download. You will then click on the link for R-N.N.N.pkg, and allow downloads from CRAN. install_path Then go to Finder, and navigate to the Downloads folder. Click on R-N.N.N.pkg You will then click on the link for R-N.N.N.pkg, and allow downloads from CRAN. install_downloadmac Click on Continue on 2 consecutive screens to download cont1_mac cont2_mac Then you need to agree with the License Agreement, mac_license then Click on Install, and provide your Mac password for permission to install. cont1_mac When the installation is complete, click on the Close button. Accept the prompt to move the installer file to the trash. 2.6.1 Testing R on the Mac If you chose to create an R desktop shortcut, double-click on this to start. If not, go to Finder, and then your Applications folder. Scroll down to the R file. Double click on this to run it. findrmac You should get this 2000-era terminal window named R Console. The version number should match what you downloaded, and the messaging should end with a “&gt;” prompt. At this prompt, type in paste(“Two to the seventh power is”, 2^7) (DON’T leave out the comma or the quotes) rconsolemac This should result in mactestR 2.6.2 Successful testing! Awesome. You are now Ready to R! ready2R 2.7 Installing RStudio on your Computer Now that R is working, we will install RStudio. This is an IDE (Integrated Development Environment), with lots of bells and whistles to help you do reproducible medical research. teslax_dash This is a lot like adding a dashboard with polished walnut panels, a large video screen map, and heated car seats with Corinthian Leather. Not absolutely necessary, but nice to have. The RStudio IDE wraps around the R engine to make your experience more comfortable and efficient. camry_dash Fortunately, RStudio is a lot cheaper than any of these cars. In fact, it is free and open source. You can download it from the web at: rstudio Click on the RStudio Desktop icon to begin. download This will take you to a new site, where you will select the Open Source Edition of RStudio Desktop open_source This will take you to a new site, where you will select the Free Version of RStudio Desktop free Now select the right version for your Operating syxtem - Windows or Mac. 2.7.1 Windows Install of RStudio If you are installing on a Mac, jump ahead now to the Mac-specfic installation instructions. Now save the RStudio.N.N.N.exe file (Ns will be digits representing the version number) to your downloads folder. winsave Now go to your downloads folder, and double click on the RStudio.N.N.N.exe file. winlaunch Allow this app to make changes. Click Next to Continue, and Agree to the Install Location. wininstall Click Install to put RStudio in the default Start Menu Folder, and when done, click the Finish button. winsave winfinish Now select your preferred language option, accept the GNU license, Click Yes to allow this to install. Select where to install. This is generally on a local (often C:) drive, and usually not a shared network drive or in the cloud. 2.7.2 Testing Windows RStudio Now you should be ready to test your Windows installation of RStudio. Open your Start menu Program list, and find RStudio. Pin it as a favorite now. Click to Open RStudio. Within the Console window of RStudio, an instance of R is started up. Check that the version number matches the version of R that you downloaded. Now run a test at the prompt (“&gt;”) in the Console window. Type in paste(\"Three to the 5th power is\", 3^5) do not leave out the quotes or the comma Then press the enter key and this should be your result: test_result35 A successful result means that you are ready to roll in RStudio and R! 2.7.3 Installing RStudio on the Mac Start at this link: RStudio Download Select the Free RStudio Desktop Version mac_download Then click on the big button to Download RStudio for Mac. mac_download2 After the Download is complete, go to Finder and the Downloads Folder. Double click on the RStudio.N.N.N.dmg file in your Downloads folder. mac_dmg This will open a window that looks like this mac_apps Use your mouse to drag the RStudio icon into the Applications folder. Now go back to Finder, then into the Applications folder. Double click on the RStudio icon, and click OK to Open. Pin your RStudio to the Dock. Double Click to run RStudio. RStudio will open an instance of R inside the Console pane of RStudio with the version number of R that you installed, and a “&gt;” prompt. 2.7.4 Testing the Mac Installation of RStudio Type in paste(\"Three to the 5th power is\", 3^5) do not leave out the quotes or the comma Then press the enter key and this should be your result. test_result35 A successful result means that you are ready to roll in RStudio and R! ready 2.7.5 Critical Setup - Tuning Your RStudio Installation You now have ~ 7 adjustments that you need to make in your RStudio Global Settings for optimal R and RStudio use. At this point, it is a good idea to jump out of RStudio and create an “Rcode” folder on your computer, in a place that is easy to find, often at the top level in your Documents folder, to make all of your future projects easy to find. Once this Rcode folder is in place, switch back to RStudio. In the RStudio Menus, go to Tools/Global Options. A new Global Options window will open up. Click on the General tab on the left. At the top, there is a small window for identifying your Default working directory. Click on the Browse button, and browse to your new “Rcode” folder and select it. From now on, your R files and Projects will all be in one place and easy to find. In the same General tab, de-select the first 3 options turn off Restore most recently opened project at startup turn off Restore previously open source documents at startup turn off Restore .RData into workspace In the same General tab, find Save workspace to .RData on exit. Click on the dropdown menu to select “Never” These tune-ups (#2 and #3) to your RStudio will mean you will always start with a clean workspace in a new RStudio session, which will avoid a lot of potential problems later. In the same General tab, at the top, click on the Advanced tab. Then select the box for Show full path to project in window title This will show your working directory at the top of your Console Pane. This can prevent confusion and problems later. On the left, click on the Rmarkdown tab. Then de-select the option for Show output inline for all Rmarkdown documents. This will put your temporary output from Code Chunks into the larger and nicer Viewer tab. Take a look at the Appearance tab. You can change your code font, the font size, and the theme. I wouldn’t make any drastic changes at this point, but it is good to know that these options are available. Any changes here are entirely optional (and cosmetic) at this point. in the RStudio menus, select Code, then check/select two options to turn these on: Soft Wrap Long Lines - so that your code does not get too wide Rainbow Parentheses - color-codes parentheses so that you can keep track of whether you have closed all of your open parentheses (a common source of errors) Now your RStudio installation is tuned and ready to go! 2.8 Installing Git on your Computer The software program, git, is a version control system. It is the most common version control system in the world. It is free and open source, and is the foundation of reproducible computing. We won’t be doing a lot with git just yet, but it is helpful to get this installation done and out of the way. It will come up a lot when we start to discuss reproducible research and collaboration. 2.8.1 Installing Git on macOS If you are using Windows, jump ahead to Installing Git on Windows. The easiest approach on the macOS is to go to the Terminal tab in the Console pane (lower left) in RStudio. A prompt will appear that ends in a $. At that prompt, type git --version note that there are 2 dashes before version. This will tell you the current version of git (2.29.2 as of January 1, 2021), or prompt you to install git. If you want the current version of git, you can install this yourself. a. First, let’s check if you have homebrew installed. Go to the Terminal tab in the Console pane (lower left) in RStudio. A prompt will appear that ends in a $. at the prompt, type command -v brew This should return “/usr/local/bin/brew” if homebrew is installed, or will tell you “brew not found” or something similar. b. Installing homebrew At the terminal prompt($), paste in the following: /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot; Then press Enter to run it. This installs the homebrew program, which allows you to install software on macOS that does not come from the Apple App Store This will take a couple of minutes. c. Installing git Once you have homebrew installed, installing git is straightforward. At the Terminal prompt ($), type brew install git and this will quickly install. You will be prompted to click Continue buttons to complete the installation. Check your installation. At the Terminal prompt ($), type git --version and this should return a result like “git version 2.29.2”, depending on the version number. 2.8.2 Installing Git on Windows If you are using Windows, go to the website, https://git-scm.com/download/win. This will start the download automatically Go to your downloads folder and install the downloaded .exe file by clicking on it Check your installation. At the Terminal prompt ($), type git --version and this should return a result like “git version 2.29.2”, depending on the version number. 2.8.3 Installing Git on Linux If you are using Fedora, or a related version of Linux like RHEL or CentOS, use dnf At the $ prompt, type sudo dnf install git-all If you are using a Debian-based version of Linux like Ubuntu, use apt At the $ prompt, type sudo apt install git-all For other distributions of Linux, follow the instructions at https://git-scm.com/download/linux. Check your installation. At the Terminal prompt ($), type git --version and this should return a result like “git version 2.29.2”, depending on the version number. 2.9 Getting Acquainted with the RStudio IDE When you first open the RStudio IDE (Integrated Development Environment), there will be a left side pane, with tabs for Console, Terminal, Rmarkdown, and Jobs. Just for fun, go to the RStudio menus, and choose File/New File/RScript. This will open a new pane at the top left, which we will call the top left pane, or the Source pane. This pane will contain tabs for each active script or document, along with tabs for any datasets you have opened up to have a look at. The Console pane, with tabs for Console, Terminal, Rmarkdown, and Jobs, has now been pushed to the lower left quadrant. You will use the Console for interactive programming, and as a “sandbox” to test out new code. When your code works and is good enough to save, you will move it to the Source pane and save it to a Script or an Rmarkdown document. Any code that is not saved to Source will be lost (actually it will be somewhere in the History, but it can be a pain to find the version that works later - it is best to save the good stuff to a Script or .Rmd). The top right pane includes tabs for your Environment (objects, like datasets, functions, and variables you have defined), History (saving the past in case you forget to, but messy), and Connections tabs for connections to databases. Later a Git tab will be added for version control (backup) of your Source documents. The bottom right pane contains tabs for your Files, Plots, Packages, Help, and a Viewer for HTML output. This is material that is also well described in the “Basic Basics 1” section of RLadiesSydney. Check it out at Basic Basics 1. There is a nice ~ 15 minute video by Jen Richmond worth watching if you are just getting started. Note that a lot of the other material on this website (RYouWithMe) is very helpful for getting started with R. For a more complete introduction to the RStudio IDE, see Chapter TBD, titled Using the RStudio IDE. "],["a-tasting-menu-of-r.html", "Chapter 3 A Tasting Menu of R 3.1 Setting the Table 3.2 Goals for this Chapter 3.3 Packages needed for this Chapter 3.4 Website links needed for this Chapter 3.5 Setting up RPubs 3.6 Open a New Rmarkdown document 3.7 Knitting your Rmarkdown document 3.8 Your Turn to Write Text 3.9 Wrangle Your Data 3.10 Summarize Your Data 3.11 Visualize Your Data 3.12 Statistical Testing of Differences 3.13 Publish your work to RPubs 3.14 The Dessert Cart", " Chapter 3 A Tasting Menu of R In this chapter, we will introduce you to a lot of neat things that you can do with R and RStudio, and you will publish a simple data analysis on the Internet that you can share with friends and family. 3.1 Setting the Table In this chapter, you will get a rapid overview of the kind of things you can do with R. You will create an Rmarkdown document read in data wrangle your data create a data visualization and publish your findings. At the end of this chapter, you will publish your data analysis to RPubs, a free website site where you can share your data analyses and visualizations. 3.2 Goals for this Chapter Set up an RPubs account Open a New Rmarkdown document Read in Data from a file Wrangle Your Data Visualize Your Data Publish your work to RPubs Check out Interactive Plots Check out Animated Graphics Check out a Clinical Trial Dashboard Check out a Shiny App 3.3 Packages needed for this Chapter In this chapter, we will use the following R packages: tidyverse janitor rstatix medicaldata If you have not installed these packages on your computer already, we will walk you through installation after you open your Rmarkdown document (see below). 3.4 Website links needed for this Chapter In this chapter, you will need to access the RPubs website. https://rpubs.com/ 3.5 Setting up RPubs First you will need to set up a free account on RPubs. Start by opening a new tab in your browser, and navigating to this link RPubs link. It should look like the image below. Enter your name, email, username and password, (remember these, you will need them later) and click on the Register Now button, and you will be set up to use RPubs . This will bring you to this page. In the image below, we have set up an account for pdr. Click on the Here’s How You Get Started link (blue text). You are now all set up and ready to go. Now you have a place on the internet to share your R creations. On to the creation part! 3.6 Open a New Rmarkdown document Let’s get started in R. Turn on your computer, and open the RStudio application. You should see the familiar panes for the Console, Environment, and Files. You need to open up a new document to activate the Source pane. While in RStudio, click on File/New File/RMarkdown. It should look like this. Opening a New Rmarkdown Document Now you will see the window below. Rename the document from “Untitled” to “Tasting”. Enter your own name as the Author, and click the OK button. Adding Title and Author Now the file is open, and looks like the window below (with title “Tasting” and author “Peter Higgins”. Click on the save icon (like a floppy disk in the top left), and save this document as my_data_analysis.Rmd. What Your New Rmarkdown Document Looks Like You have created a new Rmarkdown document. An Rmarkdown document lets you mix data, code, and descriptive text. It is very helpful for presenting and explaining data and visualizations. An Rmarkdown document can be converted (Knit) to HTML for a web page, or to Microsoft Word, Powerpoint, PDF, and several other output formats. artwork by Allison Horst, @allison_horst Code chunks are in a gray color, and both start and end with 3 backticks (```), like this. code goes here Text can be body text, or can be headers and titles. The number of hashtags before some header text defines what level the header is. You can insert links, images, and even YouTube videos into Rmarkdown documents if it is helpful to explain your point. You can change the way that the Rmarkdown document is displayed with two buttons in the top right of the new Tasting document tab. The button with several horizontal lines that looks like an outline can be clicked to toggle an outline pane on and off. When the outline pane is on, you can click on the entries to go to a different part of your document quickly. The button that looks a bit like an Angstrom symbol (A with a circle on top), or perhaps it looks like a compass to you, allows you to turn on Visual Editing, or WYSIWIG (what you see is what you get) visual editing. When this is off, you can see the markdown codes that make the words “R Markdown” a level 2 header (right after the setup chunk), with two hashtags right before the text. But when the Visual Editing button is turned on, you can see the formatting as it will appear in the output document, and a new formatting bar appears at the top of the document below the Knit button. This formatting bar adds the ability to select text and change the heading level (top left), or make text bold, italic, underlined, etc. The formatting bar also lets you insert images, links, and tables into your document, much like you would with a word processor. The first code chunk in each Rmarkdown document is named setup. Find this code chunk in your “Tasting” document. The code chunk name (in this case, setup) comes after the left curly brace and the r ({r) at the beginning of the each code chunk. The letter r tells RStudio that what is coming on the next line is R code (RStudio can also use SQL, C++, python, and several other commputer languages). After a comma, you can define options for this code chunk. In the case of this setup chunk, the option include is set to FALSE, so that when this Rmarkdown document is knitted, this code chunk will run, but no output or warnings or messages will appear in the output document. 3.7 Knitting your Rmarkdown document While this is just an example template, you can see that there is some explanatory text, some formatting, and two code chunks. One code chunk has the option, echo = FALSE, which means that the code in that code chunk will not appear in the output document, but the results of the code chunk will appear. To see what the output of this Rmarkdown document looks like, click on the Knit button - this is at the top center of your Tasting document next to a blue ball of yarn with a knitting needle. When you knit the Rmarkdown document, the default result is an HTML output document. Notice that you can see the code for the cars code chunk, but that the setup code chunk and pressure code chunk do not appear. But you do get results: a summary of the cars dataset and a plot of the pressure data. You also have the option of knitting Rmarkdown documents to other kinds of output, including Microsoft Word, Microsoft Powerpoint, posters for medical meetings, books (like this book), and pdf documents. 3.7.1 Installing Packages Before we begin working on your Rmarkdown document, you will need to install a few R packages on your computer. Go to your Console tab (lower left in RStudio), and type in (or copy and paste in) the following 5 lines: install.packages(&#39;tidyverse&#39;) install.packages(&#39;janitor&#39;) install.packages(&#39;rstatix&#39;) install.packages(&#39;remotes&#39;) remotes::install_github(&#39;higgi13425/medicaldata&#39;) Press Enter to run these functions. These will install the 5 packages, {tidyverse}, {janitor}, {rstatix}, {remotes}, and {medicaldata}. Installing packages is like buying apps for your phone. But these apps are not loaded into your current R session unless you tell R and RStudio that you want them loaded in the current session. You do this with the library() function. 3.7.2 Loading Packages with library() Copy and paste to add the following 5 lines to your setup chunk in your “Tasting.Rmd” Rmarkdown document: library(tidyverse) library(janitor) library(rstatix) library(medicaldata) prostate &lt;- medicaldata::blood_storage %&gt;% clean_names() These functions will load 4 packages and read in data from a study of prostate cancer and blood storage into the prostate object. Now run these functions, by clicking on the green rightward arrow at the top right of the setup code chunk in your Rmarkdown document. The installation of the {tidyverse} package (it is actually a meta-package that contains multiple packages) will be quite chatty, telling you which packages are being attached, and when conflicts with identically-named functions in the {stat} package have occurred. When you call the functions, filter() and lag(), the versions from the {tidyverse} package will be used by default, and the versions from the {stats} package will be masked. The {janitor} package will tell you that it has 2 conflicts with the {stats} package, and will supercede (mask) the {stats} functions for chisq.test() and fisher.test(). If you really want to access the versions from the {stats} package, you can do so by using the package::function construction, e.g. stats::chisq.test(), which tells R that you want to use the version of the function from the {stats} package, rather than the {janitor} package. If you check the Environment tab in the top right pane of RStudio, you will find that you now have a prostate object under the Data header. You can click on the white-on-blue arrow to the left of the word prostate to get an overview of each variable, the variable type (numeric, string, etc.), and the first few values of each variable. You can also click on the word prostate in the Environment window to open up a View of the whole dataset in the Source pane (top left). You can scroll up and down the rows, or right and left in the columns to inspect the data. If you check the Console tab (lower left), you will see that when you clicked on prostate, this sent a function to the console to View(prostate). You can view any dataset in the Environment tab with this function. You can also look at your data in the Console, by running (type in and press Enter for each line) glimpse(prostate) summary(prostate) to provide some information on the contents of the prostate dataframe, and a summary of the data. 3.8 Your Turn to Write Text Underneath the setup chunk, change the text of the first header (“R Markdown”) to “Analysis of Prostate Data”. Now delete the next two paragraphs of Normal text and write something about the prostate dataset, based on the summary in the console . You write body text for your documents in Normal text, and you can add new headers by starting a line with 2 hashtags, a space, and text like this ## Headline about Prostate data Write a few sentences after your heading. You can add italics or bold text by wrapping the text to be highlighted in underscores or 2 asterisks, respectively. If you are using the Visual Editor mode, you can do these things more easily by selecting your text, and clicking on the bold or italic icons in the formatting bar. You can also change a line of text to a header by selecting it, then clicking the dropdown arrow next to the word “Normal” at the top left of the formatting bar. You can make this into one of 6 levels of Headings, or a code chunk. We often reserve the level 1 Heading for the title. Try adding some text and formatting to your text. 3.9 Wrangle Your Data Find the {r cars} code chunk. Edit the name “cars” to “wrangle” Delete the one line of code from the template - “summary(cars)” We will replace this with a few lines of code to improve the data in the prostate dataset. We will be modifying the prostate dataset (with the mutate function), particularly the variables aa and fam_hx into properly labeled categorical variables, called factors in R. Then we will save (assign) the result to the prostate_factors object, while retaining the previous version in the prostate object. Copy the lines of code below and paste them into the new wrangle code chunk. You can see in the code that we start with the prostate dataset, and then (the pipe symbol [%&gt;%] is read as “and then”) mutate the aa variable to a labelled factor, and then mutate the fam_hx variable to a labelled factor. Then the resulting dataframe is assigned to the new prostate_factors object. This version of the data will be helpful for plotting later. prostate %&gt;% mutate(aa = factor(aa, levels = c(0,1), labels = c(&quot;White&quot;, &quot;African-American&quot;))) %&gt;% mutate(fam_hx = factor(fam_hx, levels = c(0,1), labels = c(&quot;No Family History&quot;, &quot;FHx of Prostate Cancer&quot;))) -&gt; prostate_factors Note that R will not ask you if you want to over-write an object. This is just a reminder to be careful when you assign data to an object. You don’t want to re-use an object name (like prostate) and inadvertently over-write your previous work. It is fine if this is what you intended, but make sure it is that this is you want to do. It is generally a good practice to assign data to well-named objects, so that you know what they are, where they came from, and how they have changed since the last data wrangling iteration. It is generally not a good idea to over-write your data. 3.10 Summarize Your Data Now you will Insert a new R code chunk, after the wrangle chunk. First, click with your mouse to place your cursor on a blank line below the wrangle chunk. Now open a new code chunk. To do this, find the green code (C) button in the menu bar at the top of your Rmarkdown document. Click on it and select R as the language being used. You will get a gray code chunk with the {r} label at the top. Insert your cursor after the r, and before the closing brace. Add a space, then type the name, summarize. In this chunk, we will run some code to summarize three variables. Paste the four lines of code below into your new code chunk. prostate %&gt;% select(age, p_vol, preop_psa, aa, fam_hx) %&gt;% group_by(aa, fam_hx) %&gt;% summarize(across(age:preop_psa, mean, na.rm=TRUE)) This code starts with the prostate dataset, and then selects 5 of the variables. Then it groups the observations by African-American race and Family history of prostate cancer. Then it summarizes across 3 variables to get the mean value of each one (after removing any missing values). Try this out by clicking on the rightward green arrow at the top right of your summarize code chunk. This should produce a summary table of results for age, prostate volume, and preoperative PSA. Add some body text below the code chunk, with your interpretation of these results, and some hypotheses about these summary results, including the contrasts by family history and race. Depending on the sample size, some of these differences might be statistically significant. Which ones would you like to test? 3.11 Visualize Your Data Now let’s plot the prostate_factors data. Change the name of the previous plot chunk to \"visualize\". Delete the one line of code from the example. We will produce a scatterplot, faceted by African-American race and Family History. Copy and paste the code below into your Rmarkdown document in this visualize code chunk. ggplot(prostate_factors) + aes(x = p_vol, y = preop_psa, col = aa) + geom_point() + geom_smooth(method = &quot;lm&quot;) + facet_grid(aa ~ fam_hx) + labs(x = &#39;Prostate Volume&#39;, y = &quot;Preoperative PSA&quot;, title = &#39;Relationship Between Prostate Volume and Preop PSA,\\nSubdivided by Family History and Race&#39;) + theme(legend.position = &quot;bottom&quot;) This will run the code to generate a plot. Note that these steps for a ggplot are connected with + signs, while the data wrangling steps are connected with the pipe ( %&gt;% ) symbol. Let’s walk through each step of the code. First you are stating the dataset to plot, with ggplot(prostate), and then the aesthetic mappings are stated, with p_vol mapped to x, preop_psa mapped to y, and the aa variable mapped to color. This all happens in the aes(x = p_vol, y = preop_psa, col = aa) step. Then these are plotted as scatterplot points with geom_point, and linear regression lines are added with geom_smooth(method = \"lm\") (method = “lm” is for linear model). Then the plot is faceted (broken into comparison plots) by the aa and fam_hx variables with facet_grid(aa ~ fam_hx). Then labels for x and y and the title are added. Then a theme option is used to move the legend position to the bottom. You can run the visualize code chunk and see the plot by clicking on the green arrow at the top right of the code chunk. Do you think that the slopes of the linear regression lines are all the same? Are there differences or patterns? Add some text below your plot stating your interpretation of these plots, and any hypotheses generated by this visualization of the data. 3.12 Statistical Testing of Differences Based on the previous summary, it looks like the African-American patients in this dataset may have, on average, a higher preoperative PSA level. We can test this with Student’s t test. Insert a new header for Statistical Testing (or T testing). Make sure it is in the Heading 2 format. Then insert some Normal text in which you state your hypothesis. Then insert a code chunk (green C button, select the R language). Name this new code chunk “t-test”. Paste in the three lines of code below to get the results of a t test comparing preop_psa levels between African-Americans and Whites in this dataset. prostate_factors %&gt;% t_test(formula = preop_psa ~ aa, detailed = TRUE) Now write some body text below the results, with your interpretation for this result in this dataset and sample size. 3.13 Publish your work to RPubs Now you have a complete data analysis, including data wrangling, summarizing, plotting, and statistical testing. Your report combines code, results, graphics, and text introduction of the questions, and text interpretation of the results. Go ahead and Knit this Rmarkdown document to see the final result. Use the Knit button (blue ball of yarn with knitting needle) near the top left of your Rmarkdown document. Now you can share this document with others, by publishing your results and interpretation on the web. Now that you have a knitted HTML document, look for the Publish button at the top right of the HTML output (it looks sort of like a blue eye, or two blue semicircles wrapped around a blue dot). Click on this and select RPubs. The knitted HTML document will now be uploaded to RPubs, and your result can be found at https://rpubs.com/username, based on your RPubs username. You may have to click on the username dropdown at the top right to reach View Profile, which will show the documents that you have published. Click on the thumbnail of your new document to go to the full link and see it in its full HTML glory. This page will have a specific link, which will be something like at [[https://rpubs.com/username](https://rpubs.com/username/123456)](https://rpubs.com/username)/123456 , which is a dedicated link you can share with other people who want to see your results. You can share this link via email, and anyone (collaborators, mentors, friends) can see your results report from anywhere with a web connection. You did it! You should feel like an Rmarkdown rock star! artwork by Allison Horst, @allison_horst You can also knit Rmarkdown documents to: Microsoft Word (1st draft of manuscript) Microsoft Powerpoint (1st draft of presentation) Posterdown (a poster-making package - for meeting posters) so that you don’t need to copy/paste your results or plots, and you can easily re-run your analyses and produce new outputs if you get more data. This chapter should give you a taste of the powerful tools for research reproducibility in R that can make your research work more efficient and more reproducible. 3.14 The Dessert Cart Below are some examples of more neat things you can do with medical data in R. These are more advanced approaches, but completely doable when you have more experience with R. 3.14.1 Interactive Plots Below is an interactive plot. Click on the plot to activate it. Then you can hover your mouse over each point to get more details about the data. You can also use the crosshairs to draw a zoom rectangle, or use the plotly menu bar at top right to zoom in or out of a region of the data. 3.14.2 Animated Graphics Here is an example of animated graphics that you can create in R to illustrate changes in data over time. 3.14.3 A Clinical Trial Dashboard Below is an screen capture picture of a web flexdashboard to track the data in an ongoing clinical trial (which is now completed and published). You can see the actual web dashboard here. Check out the various tabs. Imagine how useful it would be to track enrollment, exclusions, missing data, and outcomes in real time. Details on how this is done can be found here, and the underlying code here. All of this work was done in R by Jenn Thompson. 3.14.4 A Shiny App The frame below shows a publicly available Shiny web application, built with R, which can help clinicians calculate the probablity of intestinal TB vs. Crohn’s disease with available clinical data. And to determine how new test results would change this estimate. The web app can be accessed here. 3.14.5 An Example of Synergy in the R Community One of the remarkable things about the open source R community is that people build all kinds of new R functions and packages that are useful to them, and then share them publicly with tools like Github so that they can be useful to others. Often combining bits of several packages leads to emergent properties - completely new creations that can only occur because all of the parts (packages) are present. The collaborative nature of the R community, in this case on Twitter (follow the #rstats hashtag), can lead to surprising collaborations and outcomes. Go ahead and play the example below, which uses rayrendering (all coded entirely in R) to show a 3D map of John Snow’s cholera case data in 1854, which led him to identify the Broad Street water pump as the source of the cholera outbreak, and led to the removal of the pump handle and the end of outbreak. If you are not familiar with John Snow and the Broad Street pump, there is a fun series of YouTube animations (parts 1-3 and an epilogue) to explain the history. Start by clicking here. "],["importing-your-data-into-r.html", "Chapter 4 Importing Your Data into R 4.1 Reading data with the {readr} package 4.2 Reading Excel Files with readxl 4.3 Bringing in data from other Statistical Programs (SAS, Stata, SPSS) with the {haven} package 4.4 Other strange file types with rio 4.5 Data exploration with glimpse, str, and head/tail 4.6 More exploration with skimr and DataExplorer 4.7 Practice loading data from multiple file types 4.8 Practice saving (writing to disk) data objects in formats including csv, rds, xls, xlsx and statistical program formats 4.9 How do readr and readxl parse columns? 4.10 What are the variable types? 4.11 Controlling Parsing 4.12 Chapter Challenges 4.13 Future forms of data ingestion", " Chapter 4 Importing Your Data into R For most of this book, we will be using datasets from the {medicaldata} package. These are easy to load. You just type into the Console pane medicaldata::scurvy and you get James Lind’s scurvy dataset (actually, a reconstruction of what it might have looked like for his 12 participants). If you want to save this data to an object in your work environment, you just need to assign this to a named object, like scurvy, like so: scurvy &lt;- medicaldata::scurvy # now print the columns for id and treatment scurvy %&gt;% select(study_id:treatment) ## # A tibble: 12 x 2 ## study_id treatment ## &lt;chr&gt; &lt;fct&gt; ## 1 001 cider ## 2 002 cider ## 3 003 dilute_sulfuric_acid ## 4 004 dilute_sulfuric_acid ## 5 005 vinegar ## 6 006 vinegar ## 7 007 sea_water ## 8 008 sea_water ## 9 009 citrus ## 10 010 citrus ## 11 011 purgative_mixture ## 12 012 purgative_mixture There are a number of medical datasets to explore and learn with, within the {medicaldata} package. However, at some point, you will want to use R to work on your own data. You may already be itching to get started on your own data. This is a good thing. Working with your own data, toward your own goals, will be a motivating example, and will help you learn R. As you go through the different chapters, use the example data and exercises to get you started and to learn the principles, and then try what you have learned on your own data. Reproducibility and Raw Data It is an important principle to always save an untouched copy of your raw data. You can copy it to a new object, and experiment with modifying it, cleaning it, making plots, etc., but always leave the original data file untouched. You want to create a completely reproducible, step-by-step trail from your raw data to your finished analysis and final report, and you can only do that if you preserve the original raw data. That is the cornerstone of your analysis. It is tempting to fix minor data entry errors, or other aspects of the raw data. Do not do this - leave all errors intact in your raw data, and explicitly make edits with explanations of - who made the edit - when it was made - what was changed - why it was made - provide a justification, and identify source documents to support the rationale. Every edit should be documented in your code, with who, when, what, and why. Now, on to the fun part. Let’s read in some data! 4.1 Reading data with the {readr} package Many of the standard data formats can be read with functions in the {readr} package. These include: read_csv() for comma-separated values (*.csv) files read_tsv() for tab-separated values (*.tsv) files read_delim() for files with a different delimiter that you can specify (instead of commas or tabs, there might be semicolons), or you can let {readr} guess the delimiter in readr 2.0. read_fwf() for fixed width files read_table() for tabular files where columns are separated by white-space. read_log() is specifically for web log files Let’s read a csv file. First, make sure that you have the {readr} package loaded (or the {tidyverse} meta-package, which includes {readr}). You can load {readr} with the library() function. library(readr) # or you can use library(tidyverse) # which will load 8 packages, including readr Note that this will not work if you do not already have the {readr} package installed on your computer. You will get an error, like this: Error in library(readr) : there is no package called 'readr' This is not a problem - you just have to install the package first. You only need to do this once, like buying a book, and putting it in your personal library. Each time you use the package, you have to pull the book off the shelf, with library(packagename). To install the {readr} package, we can install the whole {tidyverse} package, which will come in handy later. Just enter the following in your Console pane: install.packages('tidyverse') Note that quotes around ‘tidyverse’ are required, as tidyverse is not yet a known object or package in your working environment. Once the {tidyverse} package is installed, you can use library(tidyverse) without quotes, as it is a known (installed) package. OK, after that detour, we should be all caught up - you should be able to run library(tidyverse) or library(readr) without an error. Now that you have {readr} loaded, you can read in some csv data. Let’s start with a file named scurvy.csv in a data folder on GitHub. You will need to glue together the url_stem and “data/scurvy.csv” to get the full web address. Run the code chunk below to see the url_stem and the dataset. url_stem ## [1] &quot;https://raw.githubusercontent.com/higgi13425/rmrwr-book/master/&quot; read_csv(glue(url_stem, &#39;data/scurvy.csv&#39;)) ## Rows: 12 Columns: 8 ## ── Column specification ──────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (8): study_id, treatment, dosing_regimen_for_scurvy,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 12 x 8 ## study_id treatment dosing_regimen_for_scur… gum_rot_d6 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 001 cider 1 quart per day 2_moderate ## 2 002 cider 1 quart per day 2_moderate ## 3 003 dilute_sulf… 25 drops of elixir of v… 1_mild ## 4 004 dilute_sulf… 25 drops of elixir of v… 2_moderate ## 5 005 vinegar two spoonfuls, three ti… 3_severe ## 6 006 vinegar two spoonfuls, three ti… 3_severe ## 7 007 sea_water half pint daily 3_severe ## 8 008 sea_water half pint daily 3_severe ## 9 009 citrus two lemons and an orang… 1_mild ## 10 010 citrus two lemons and an orang… 0_none ## 11 011 purgative_m… a nutmeg-sized paste of… 3_severe ## 12 012 purgative_m… a nutmeg-sized paste of… 3_severe ## # … with 4 more variables: skin_sores_d6 &lt;chr&gt;, ## # weakness_of_the_knees_d6 &lt;chr&gt;, lassitude_d6 &lt;chr&gt;, ## # fit_for_duty_d6 &lt;chr&gt; Let’s look at what was extracted from the csv file. This starts after the url_stem (web address) is printed out. The Console pane has a print out of the Column specification, followed by the data in rectangular format. In the Column specification, the delimiter between items of data is identified (a comma), and a listing of variables with the character (chr) data type is printed. There are no non-character data types in this particular dataset. The ‘guessing’ of what data type is appropriate for each variable is done ‘automagically’, as the read_csv() function reads the first 1000 rows, then guesses what data type is present in each column (variable). It is often conservative, and in this case, made all of these columns into character variables (also called strings). You could argue that the col_character() assignment should be numeric for the study_id variable, or that the Likert scales used for outcomes like gum_rot_d6 and skin_sores_d6 should be coded as ordinal variables, known as ordered factors in R. You will learn to control these data types during data import with the spec() argument. The second piece of output is the data itself. This is first identified as a ‘tibble’, which is a type of data table, with 12 rows and 8 columns, in # A tibble: 12 x 8. This is followed by a header row of variable names, and just below that is the data type (&lt;chr&gt; for character) for each column. Then, on the left are gray row numbers (not actually part of the data set), followed by (to the right) rows of data. A tibble, by default, only prints out 10 rows of data, and no more columns than will fill your current console window. The other columns are listed in order at the bottom of the tibble in gray type. Now, by simply reading in the data, you can look at it, but you can’t do anything with it, as you have not saved it as an object in your working Environment. If you want to do things with this data, and make them last, you have to assign the data to an object, and give it a name. To do this, you need to use an assignment arrow, as below scurvy_data &lt;- read_csv(glue(url_stem, &#39;data/scurvy.csv&#39;)) ## Rows: 12 Columns: 8 ## ── Column specification ──────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (8): study_id, treatment, dosing_regimen_for_scurvy,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Now this is saved to scurvy_data in your working Environment. You can look in the Environment tab (top right pane in RStudio) and see that scurvy_data has now appeared in the Data section, with 12 observations of 8 variables. This is not a file written (saved) to disk, but this dataset is now available in the working environment as an assigned data object. You can now print this out at any time by typing scurvy_data into the Console, or into a script. Try this out in the Console pane. 4.1.1 Test yourself on scurvy How many limes did the British seamen in the citrus arm receive each day? 3 2 zero 1.5 You can also start with the scurvy_data object, and do things to this data object, like summarize it, or graph it, or calculate total_symptom_score in a data pipeline. Once you have assigned your data to an object, it will stick around in that R session for later use. The csv (comma separated values) format is a very common data format, and most data management programs have a way to export *.csv files. The csv format is simple, is not owned by anyone, and works across platforms. However, it can occasionally be tricky if you have commas in the middle of a variable like degree, with entries like ‘md, phd’ in a column that is mostly ‘md’. The read_csv function is pretty smart, and usually gets the number of columns right, but this is something to watch out for. Notice that read_csv had no problem with the dosing of vinegar (“two spoonfuls, three times a day”) in the scurvy dataset. If you happen to come across a tab-separated values file, read_tsv() works the same way. Both of these functions have reasonable defaults, so that most of the time, you just have to use the path to your file (usually on your hard drive, rather than on the web) as the only argument. On occasion, though, you will want to take control of some of the other arguments, and use something other than the defaults. 4.1.2 What is a path? A path is the trail through the folders in your hard drive (or on the web) that the computer needs to follow to find a particupar file. Paths can look something like: C:/Documents/Rcode/my_file.R ~/User/Documents/Rcode/my_file.R and can get pretty complicated to keep track of. One particularly nice feature of Projects in RStudio is that the project directory is always your home, or root directory. You can make your life easier by using the {here} package, which memorizes the path to your project, so you can just write here(my_file.R), and not have to worry about making a typo in a long path name. When your data has no column names (headers), read_csv will (by default) assume that the first row of the data is the column names. To fix this, add the argument, col_names = FALSE. You can also assign your own col_names by setting a vector, like c(“patient_id”, “treatment”, “outcome”) to col_names, as below read_csv(file = glue(url_stem, &#39;data/scurvy.csv&#39;), col_names = c(&quot;pat_id&quot;, &quot;arm&quot;, &quot;dose&quot;, &quot;gums&quot;, &quot;skin&quot;, &quot;weak&quot;, &quot;lass&quot;, &quot;fit&quot;)) ## Rows: 13 Columns: 8 ## ── Column specification ──────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (8): pat_id, arm, dose, gums, skin, weak, lass, fit ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 13 x 8 ## pat_id arm dose gums skin weak lass fit ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 study_… treatm… dosing_r… gum_r… skin… weakn… lass… fit_… ## 2 001 cider 1 quart … 2_mod… 2_mo… 2_mod… 2_mo… 0_no ## 3 002 cider 1 quart … 2_mod… 1_mi… 2_mod… 3_se… 0_no ## 4 003 dilute… 25 drops… 1_mild 3_se… 3_sev… 3_se… 0_no ## 5 004 dilute… 25 drops… 2_mod… 3_se… 3_sev… 3_se… 0_no ## 6 005 vinegar two spoo… 3_sev… 3_se… 3_sev… 3_se… 0_no ## 7 006 vinegar two spoo… 3_sev… 3_se… 3_sev… 3_se… 0_no ## 8 007 sea_wa… half pin… 3_sev… 3_se… 3_sev… 3_se… 0_no ## 9 008 sea_wa… half pin… 3_sev… 3_se… 3_sev… 3_se… 0_no ## 10 009 citrus two lemo… 1_mild 1_mi… 0_none 1_mi… 0_no ## 11 010 citrus two lemo… 0_none 0_no… 0_none 0_no… 1_yes ## 12 011 purgat… a nutmeg… 3_sev… 3_se… 3_sev… 3_se… 0_no ## 13 012 purgat… a nutmeg… 3_sev… 3_se… 3_sev… 3_se… 0_no In this case, when we set our own col_names, there are now 13 rows of data, and the original column headers are now listed as the first row of data. We can fix this with the skip argument within the parentheses of the read_csv() function, which has a default of 0. We can skip as many lines as we want, which can be helpful if you have an Excel file with a lot of blank lines or commentary at the top of the spreadsheet. When we set skip = 1 in this case, we get a cleaner dataset, without variable names as data. read_csv(file = glue(url_stem, &#39;data/scurvy.csv&#39;), col_names = c(&quot;pat_id&quot;, &quot;arm&quot;, &quot;dose&quot;, &quot;gums&quot;, &quot;skin&quot;, &quot;weak&quot;, &quot;lass&quot;, &quot;fit&quot;), skip = 1) ## Rows: 12 Columns: 8 ## ── Column specification ──────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (8): pat_id, arm, dose, gums, skin, weak, lass, fit ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 12 x 8 ## pat_id arm dose gums skin weak lass fit ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 001 cider 1 quart pe… 2_mod… 2_mo… 2_mo… 2_mo… 0_no ## 2 002 cider 1 quart pe… 2_mod… 1_mi… 2_mo… 3_se… 0_no ## 3 003 dilute… 25 drops o… 1_mild 3_se… 3_se… 3_se… 0_no ## 4 004 dilute… 25 drops o… 2_mod… 3_se… 3_se… 3_se… 0_no ## 5 005 vinegar two spoonf… 3_sev… 3_se… 3_se… 3_se… 0_no ## 6 006 vinegar two spoonf… 3_sev… 3_se… 3_se… 3_se… 0_no ## 7 007 sea_wa… half pint … 3_sev… 3_se… 3_se… 3_se… 0_no ## 8 008 sea_wa… half pint … 3_sev… 3_se… 3_se… 3_se… 0_no ## 9 009 citrus two lemons… 1_mild 1_mi… 0_no… 1_mi… 0_no ## 10 010 citrus two lemons… 0_none 0_no… 0_no… 0_no… 1_yes ## 11 011 purgat… a nutmeg-s… 3_sev… 3_se… 3_se… 3_se… 0_no ## 12 012 purgat… a nutmeg-s… 3_sev… 3_se… 3_se… 3_se… 0_no Now we don’t have extra column names as data, and we are back to 12 rows. Also note that in this code chunk, we put each argument to the function on its own line, with commas between them. This is a good practice, to make your code more readable. You can also set n_max to a particular number of rows to be read in (the default is infinity, or Inf) You might want a smaller number if you have a very large dataset and limited computer memory. Another important argument (option) for both read_csv and read_tsv is col_types, which lets you take control of the column types during the data What if you want to take more control of the import process with read_xxx()? You can add a col_types argument to the read_csv() function. You can copy the Column specifications from the first attempt at importing, and then make some edits. You can get the column specifications as guessed by {readr} by running the spec() function on the scurvy_data object. Try this out in the code chunk below. spec(scurvy_data) ## cols( ## study_id = col_character(), ## treatment = col_character(), ## dosing_regimen_for_scurvy = col_character(), ## gum_rot_d6 = col_character(), ## skin_sores_d6 = col_character(), ## weakness_of_the_knees_d6 = col_character(), ## lassitude_d6 = col_character(), ## fit_for_duty_d6 = col_character() ## ) This sets the data type for each column (variable). This is helpful if you want to change a few of these. Take a look at the next code chunk below. I have added the col_types argument to read_csv(), and set it equal to the Column specifications (copied from above). Then I edited study_id to col_integer(), and treatment to col_factor(). Run the code chunk below to see how this works. The glimpse function will give an overview of the new scurvy_cols object that I assigned the data to. scurvy_cols &lt;- read_csv( file = glue(url_stem, &#39;data/scurvy.csv&#39;), col_types = cols( study_id = col_integer(), treatment = col_factor(), dosing_regimen_for_scurvy = col_character(), gum_rot_d6 = col_character(), skin_sores_d6 = col_character(), weakness_of_the_knees_d6 = col_character(), lassitude_d6 = col_character(), fit_for_duty_d6 = col_character() ) ) glimpse(scurvy_cols) ## Rows: 12 ## Columns: 8 ## $ study_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, … ## $ treatment &lt;fct&gt; cider, cider, dilute_sul… ## $ dosing_regimen_for_scurvy &lt;chr&gt; &quot;1 quart per day&quot;, &quot;1 qu… ## $ gum_rot_d6 &lt;chr&gt; &quot;2_moderate&quot;, &quot;2_moderat… ## $ skin_sores_d6 &lt;chr&gt; &quot;2_moderate&quot;, &quot;1_mild&quot;, … ## $ weakness_of_the_knees_d6 &lt;chr&gt; &quot;2_moderate&quot;, &quot;2_moderat… ## $ lassitude_d6 &lt;chr&gt; &quot;2_moderate&quot;, &quot;3_severe&quot;… ## $ fit_for_duty_d6 &lt;chr&gt; &quot;0_no&quot;, &quot;0_no&quot;, &quot;0_no&quot;, … You can see that study_id is now considered the integer data type (&lt;int&gt;), and the treatment variable is now a factor (&lt;fct&gt;). You can choose as data types: col_integer () col_character() col_number() (handles #s with commas) col_double() (to specify decimal #s) col_logical() (only TRUE and FALSE) col_date(format = \"\") - may need to define format col_time(format = \"“) - col_datetime(format =”\") col_factor(levels = \"\", ordered = TRUE) - you may want to set levels and ordered if ordinal. col_guess() - is the default col_skip() if you want to skip a column The read_csv() guesses may be fine, but you can take more control if needed. This col_types() approach gives you fine control of each column. But it is a lot of typing. Sometimes you want to set all the column types with a lot less typing, and you don’t need to set levels for factors, or formats for dates. You can do this by setting col_types to a string, in which each letter specifies the column type for each column. Run the example below by clicking on the green arrow at the top right of the code chunk, in which I use i for col_integer, c for col_character, and f for col_factor. scurvy_cols2 &lt;- read_csv( file = glue(url_stem, &#39;data/scurvy.csv&#39;), col_types = &quot;ifcffff&quot;) glimpse(scurvy_cols2) ## Rows: 12 ## Columns: 8 ## $ study_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, … ## $ treatment &lt;fct&gt; cider, cider, dilute_sul… ## $ dosing_regimen_for_scurvy &lt;chr&gt; &quot;1 quart per day&quot;, &quot;1 qu… ## $ gum_rot_d6 &lt;fct&gt; 2_moderate, 2_moderate, … ## $ skin_sores_d6 &lt;fct&gt; 2_moderate, 1_mild, 3_se… ## $ weakness_of_the_knees_d6 &lt;fct&gt; 2_moderate, 2_moderate, … ## $ lassitude_d6 &lt;fct&gt; 2_moderate, 3_severe, 3_… ## $ fit_for_duty_d6 &lt;chr&gt; &quot;0_no&quot;, &quot;0_no&quot;, &quot;0_no&quot;, … 4.1.3 Try it Yourself Now try this yourself with a *.tsv file. The file strep_tb.tsv is located in the same GitHub folder, and you can use the same url_stem. In the example code chunk below, there are several blanks. Copy this code chunk (use the copy button in the top right of the code chunk - hover to find it) to your RStudio Console pane. Edit it to make the two changes listed, and run the code chunk as directed below. Fill in the second part of the read_xxx() function correctly to read this file Fill in the correct file name to complete the path This version of the code chunk will read in every column as the character data type. This is OK, but not quite right. Now edit the col_types string to make: both doses numeric (n or d) (variables 3,4) gender a factor (f) (var 5) all 4 of the baseline variables into factors (var 6-9) skip over strep_resistance and radiologic_6m - set as hyphens (-) (var 10-11) rad_num into an integer (i) (var 12) improved into a logical (l) (var 13) strep_tb_cols &lt;- read_---( file = glue(url_stem, &#39;data/----.tsv&#39;), col_types = &quot;ccccccccccccc&quot;) glimpse(strep_tb_cols) 4.2 Reading Excel Files with readxl While file types like *.csv and *.tsv are common, it is also common to use Microsoft Excel or an equivalent for data entry. There are a lot of reasons that this is not a good idea (see chapter XX and video at link), but Excel is so ubiquitous, that it is often used for data entry. Fortunately, the {readxl} package provides functions for reading excel files. The read_xl() function works nearly the same as read_csv(). But there are a few bonus arguments (options) that are really helpful. The read_excel() function includes helpful arguments like skip, col_names, col_types, and n_max, much like read_csv(). In addition, read_excel() has a sheet argument, which lets you specify which sheet in an excel workbook you want to read. The default is the first worksheet, but you can set this to sheet = 4 for the 4th worksheet from the left, or sheet = “raw_data” to get the correct worksheet. You can also set the range argument to only read in a particular range of cells, like range = “B2:G14”. Below is an example of how to read in an Excel worksheet. read_excel(path = &#39;data/paulolol.xlsx&#39;, sheet = 1, skip = 1) ## New names: ## * `` -&gt; ...2 ## * `` -&gt; ...3 ## * `` -&gt; ...4 ## * `` -&gt; ...5 ## * `` -&gt; ...6 ## # A tibble: 13 x 6 ## `Paul Investigator… ...2 ...3 ...4 ...5 ...6 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 44338 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 pat_id SBP_start SBP_e… HR_sta… HR_e… treat… ## 3 1 145 120 92 78 paulo… ## 4 2 147 148 88 87 place… ## 5 3 158 139 96 80 paulo… ## 6 4 167 166 87 88 place… ## 7 5 154 131 84 72 paulo… ## 8 6 178 177 99 97 place… ## 9 7 151 134 101 86 paulo… ## 10 8 149 148 92 93 place… ## 11 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; sbp hr &lt;NA&gt; ## 12 &lt;NA&gt; mean pau… &lt;NA&gt; 131 79 &lt;NA&gt; ## 13 &lt;NA&gt; mean pla… &lt;NA&gt; 159.75 91.25 &lt;NA&gt; 4.2.1 Test yourself on strep_tb which argument in read_excel lets you skip rows of commentary? sheet skip path which argument in read_excel lets you pick which spreadsheet tab to read? sheet skip path How many missing (NA) values are in this dataset (as run with skip =1)? what should the range argument be to read in these data cleanly? A3:F8 A1:L30 B4:K15 4.3 Bringing in data from other Statistical Programs (SAS, Stata, SPSS) with the {haven} package It is common to have the occasional collaborator who still uses one of the older proprietary statistical packages. They will send you files with filenames like data.sas7bdat (SAS), data.dta (Stata), or data.sav (SPSS). The {haven} package makes reding in these data files straightforward. ** Set up ** haven::read_sas(glue(url_stem, &quot;data/blood_storage.sas7bdat&quot;)) ## # A tibble: 316 x 20 ## rbc_age_group median_rbc_age age aa fam_hx p_vol ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 25 72.1 0 0 54 ## 2 3 25 73.6 0 0 43.2 ## 3 3 25 67.5 0 0 103. ## 4 2 15 65.8 0 0 46 ## 5 2 15 63.2 0 0 60 ## 6 3 25 65.4 0 0 45.9 ## 7 3 25 65.5 1 0 42.6 ## 8 1 10 67.1 0 0 40.7 ## 9 1 10 63.9 0 0 45 ## 10 2 15 63 1 0 67.6 ## # … with 306 more rows, and 14 more variables: t_vol &lt;dbl&gt;, ## # t_stage &lt;dbl&gt;, b_gs &lt;dbl&gt;, bn &lt;dbl&gt;, ## # organ_confined &lt;dbl&gt;, preop_psa &lt;dbl&gt;, ## # preop_therapy &lt;dbl&gt;, units &lt;dbl&gt;, s_gs &lt;dbl&gt;, ## # any_adj_therapy &lt;dbl&gt;, adj_rad_therapy &lt;dbl&gt;, ## # recurrence &lt;dbl&gt;, censor &lt;dbl&gt;, ## # time_to_recurrence &lt;dbl&gt; ** write files, add examples for Stata, SPSS ** 4.4 Other strange file types with rio Once in a while, you will run into a strange data file that is not a csv or Excel or from a common statistical package (SAS, Stata, SPSS). These might include Systat, Minitab, RDA, or others. This is when the {rio} package comes to the rescue. The name, rio, stands for R input and output. The {rio} package looks at the file extension (like .csv, .xls, .dta) to guess the file type, and then applies the appropriate method to read in the data. The import() function in {rio} makes data import much easier. You don’t always have the fine control seen in {readr}, but {rio} is an all-purpose tool that can get nearly any data format into R. Try this out with the code chunk below. Just replace the filename in the code chunk with one of the files named below. Try to use the same import() function to read scurvy.csv strep_tb.tsv paulolol.xlsx blood_storage.sas7bdat rio::import(glue(url_stem, &quot;data/filename&quot;)) ## Error in remote_to_local(file, format = format): Unrecognized file format. Try specifying with the format argument. It can be very convenient to use {rio} for unusual file types. 4.5 Data exploration with glimpse, str, and head/tail Once you have a dataset read into your working Environment (see the Environment tab in RStudio), you will want to know more about it. There are several helpful functions and packages to get you started in exploring your data. 4.5.1 Taking a glimpse with glimpse() The glimpse() function is part of the tidyverse, and is a helpful way to see a bit of all of the variables in a dataset. Let’s try this with the scurvy dataset, which we have already assigned to the object scurvy in the working Environment. Just put the object name as an argument within the glimpse() function (inside the parentheses), as below. Run the code chunk below to get the glimpse() output. glimpse(scurvy) ## Rows: 12 ## Columns: 8 ## $ study_id &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;00… ## $ treatment &lt;fct&gt; cider, cider, dilute_sul… ## $ dosing_regimen_for_scurvy &lt;chr&gt; &quot;1 quart per day&quot;, &quot;1 qu… ## $ gum_rot_d6 &lt;fct&gt; 2_moderate, 2_moderate, … ## $ skin_sores_d6 &lt;fct&gt; 2_moderate, 1_mild, 3_se… ## $ weakness_of_the_knees_d6 &lt;fct&gt; 2_moderate, 2_moderate, … ## $ lassitude_d6 &lt;fct&gt; 2_moderate, 3_severe, 3_… ## $ fit_for_duty_d6 &lt;fct&gt; 0_no, 0_no, 0_no, 0_no, … The glimpse() function output tells you that there are 12 rows (observations) and 8 columns (variables). Then it lists each of the 8 variables, followed by the data type, and the first few values (or as much as will fit in the width of your Console pane). We can see that study_id and dosing_regimen_for_scurvy are both of the character (aka string) data type, and the other 6 variables are factors. 4.5.2 Try this out yourself. What can you learn about the strep_tb dataset with glimpse()? Edit the code chunk below to find out about strep_tb. glimpse(----) 4.5.3 Test yourself on strep_tb which variable is the logical data type? baseline_esr improved patient_id which variable is the dbl numeric data type? arm patient_id rad_num How many observations are in this dataset? 4.5.4 Examining Structure with str() The str() function is part of the {utils} package in base R, and can tell you the structure of any object in R, whether a list, a dataset, a tibble, or a single variable. It is very helpful for reality-checking your data, especially when you are getting errors in your code. A common source of errors is trying to run a function that requires a particular data structure or data type on the wrong data structure or data type. Sometimes just checking the data structure will reveal the source of an error. The str() function does largely what glimpse() does, but provides a bit more detail, with less attractive formatting. Run the code chunk below to see the output of str(). str(scurvy) ## tibble [12 × 8] (S3: tbl_df/tbl/data.frame) ## $ study_id : chr [1:12] &quot;001&quot; &quot;002&quot; &quot;003&quot; &quot;004&quot; ... ## $ treatment : Factor w/ 6 levels &quot;cider&quot;,&quot;citrus&quot;,..: 1 1 3 3 6 6 5 5 2 2 ... ## $ dosing_regimen_for_scurvy: chr [1:12] &quot;1 quart per day&quot; &quot;1 quart per day&quot; &quot;25 drops of elixir of vitriol, three times a day&quot; &quot;25 drops of elixir of vitriol, three times a day&quot; ... ## $ gum_rot_d6 : Factor w/ 4 levels &quot;0_none&quot;,&quot;1_mild&quot;,..: 3 3 2 3 4 4 4 4 2 1 ... ## $ skin_sores_d6 : Factor w/ 4 levels &quot;0_none&quot;,&quot;1_mild&quot;,..: 3 2 4 4 4 4 4 4 2 1 ... ## $ weakness_of_the_knees_d6 : Factor w/ 4 levels &quot;0_none&quot;,&quot;1_mild&quot;,..: 3 3 4 4 4 4 4 4 1 1 ... ## $ lassitude_d6 : Factor w/ 4 levels &quot;0_none&quot;,&quot;1_mild&quot;,..: 3 4 4 4 4 4 4 4 2 1 ... ## $ fit_for_duty_d6 : Factor w/ 2 levels &quot;0_no&quot;,&quot;1_yes&quot;: 1 1 1 1 1 1 1 1 1 2 ... The str() output starts by telling you that scurvy is a tibble, which is a modern sort of data table. A tibble will by default only print 10 rows of data, and only the number of columns that will fit in your Console pane. Then you see [12 x 8], which means that there are 12 rows and 8 columns - the default in R is to always list rows first, then columns (R x C notation). Then you learn that this is an S3 object, that is a tbl_df (tibble), and a tbl, and also a data.frame). Then you get a listing of each variable, data type, and a bit of the data, much like glimpse(). Another extra detail provided by str() is that it tells you some of the levels of each factor variable, and then shows these as integers (how the data is actually stored). 4.5.5 Test yourself on the scurvy dataset what is the dose of cider? 25 drops 1 quart per day one-half rood how many levels of gum_rot are there? Which numeric value indicates ‘fit for duty’? Note that you can also use str() and glimpse() on a single variable. You often use this approach when you get an error message that tells you that you have the wrong data type. Try this with the strep_tb dataset variable patient_id by running the code chunk below. Imagine that you wanted to get the mean of patient_id. You got a warning that pointed out that the argument is not numeric or logical. So you run str() to find out the data structure of this variable. mean(strep_tb$patient_id) ## Warning in mean.default(strep_tb$patient_id): argument is ## not numeric or logical: returning NA ## [1] NA str(strep_tb$patient_id) ## chr [1:107] &quot;0001&quot; &quot;0002&quot; &quot;0003&quot; &quot;0004&quot; &quot;0005&quot; &quot;0006&quot; ... This shows you that patient_id is actually a character variable. If you wanted to find the mean value, you would have to change it to numeric (with as.numeric() first). The glimpse() function provides identical output to str() for a variable in a table. glimpse(strep_tb$patient_id) ## chr [1:107] &quot;0001&quot; &quot;0002&quot; &quot;0003&quot; &quot;0004&quot; &quot;0005&quot; &quot;0006&quot; ... You can choose whether you prefer the details of str() or the nicer formatting of glimpse() for yourself. 4.5.6 Examining a bit of data with head() and tail() Oftentimes, you want just a quick peek at your data, especially after a merge or a mutate, to make sure that things have gone as expected. This is where the base R functions head() and tail() can be helpful. As you might have guessed, these functions give you a quick view of the head (top 6 rows) and tail (last 6 rows) of your data. Try this out with scurvy or strep_tb. head(scurvy) ## # A tibble: 6 x 8 ## study_id treatment dosing_regimen_for_scur… gum_rot_d6 ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; ## 1 001 cider 1 quart per day 2_moderate ## 2 002 cider 1 quart per day 2_moderate ## 3 003 dilute_sulfu… 25 drops of elixir of v… 1_mild ## 4 004 dilute_sulfu… 25 drops of elixir of v… 2_moderate ## 5 005 vinegar two spoonfuls, three ti… 3_severe ## 6 006 vinegar two spoonfuls, three ti… 3_severe ## # … with 4 more variables: skin_sores_d6 &lt;fct&gt;, ## # weakness_of_the_knees_d6 &lt;fct&gt;, lassitude_d6 &lt;fct&gt;, ## # fit_for_duty_d6 &lt;fct&gt; tail(strep_tb) ## # A tibble: 6 x 13 ## patient_id arm dose_strep_g dose_PAS_g gender ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0100 Streptomycin 2 0 M ## 2 0101 Streptomycin 2 0 F ## 3 0104 Streptomycin 2 0 M ## 4 0105 Streptomycin 2 0 F ## 5 0106 Streptomycin 2 0 F ## 6 0107 Streptomycin 2 0 F ## # … with 8 more variables: baseline_condition &lt;fct&gt;, ## # baseline_temp &lt;fct&gt;, baseline_esr &lt;fct&gt;, ## # baseline_cavitation &lt;fct&gt;, strep_resistance &lt;fct&gt;, ## # radiologic_6m &lt;fct&gt;, rad_num &lt;dbl&gt;, improved &lt;lgl&gt; Note that since these are tibbles, they will only print the columns that will fit into your Console pane. You can see all variables and the whole width (though it will wrap around to new lines) by either (1) converting these to a data frame first, to avoid tibble behavior, or (2) by using print, which has a width argument that allows you to control the number of columns (it also has an n argument that lets you print all rows). Run the code chunk below to see how this is different. head(as.data.frame(scurvy)) ## study_id treatment ## 1 001 cider ## 2 002 cider ## 3 003 dilute_sulfuric_acid ## 4 004 dilute_sulfuric_acid ## 5 005 vinegar ## 6 006 vinegar ## dosing_regimen_for_scurvy ## 1 1 quart per day ## 2 1 quart per day ## 3 25 drops of elixir of vitriol, three times a day ## 4 25 drops of elixir of vitriol, three times a day ## 5 two spoonfuls, three times daily ## 6 two spoonfuls, three times daily ## gum_rot_d6 skin_sores_d6 weakness_of_the_knees_d6 ## 1 2_moderate 2_moderate 2_moderate ## 2 2_moderate 1_mild 2_moderate ## 3 1_mild 3_severe 3_severe ## 4 2_moderate 3_severe 3_severe ## 5 3_severe 3_severe 3_severe ## 6 3_severe 3_severe 3_severe ## lassitude_d6 fit_for_duty_d6 ## 1 2_moderate 0_no ## 2 3_severe 0_no ## 3 3_severe 0_no ## 4 3_severe 0_no ## 5 3_severe 0_no ## 6 3_severe 0_no print(tail(strep_tb), width = Inf) ## # A tibble: 6 x 13 ## patient_id arm dose_strep_g dose_PAS_g gender ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0100 Streptomycin 2 0 M ## 2 0101 Streptomycin 2 0 F ## 3 0104 Streptomycin 2 0 M ## 4 0105 Streptomycin 2 0 F ## 5 0106 Streptomycin 2 0 F ## 6 0107 Streptomycin 2 0 F ## baseline_condition baseline_temp baseline_esr ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 3_Poor 2_99-99.9F 4_51+ ## 2 3_Poor 4_100F+ 4_51+ ## 3 3_Poor 4_100F+ 4_51+ ## 4 3_Poor 4_100F+ 4_51+ ## 5 3_Poor 4_100F+ 4_51+ ## 6 3_Poor 4_100F+ 4_51+ ## baseline_cavitation strep_resistance ## &lt;fct&gt; &lt;fct&gt; ## 1 yes 3_resist_100+ ## 2 yes 3_resist_100+ ## 3 yes 3_resist_100+ ## 4 yes 3_resist_100+ ## 5 yes 3_resist_100+ ## 6 yes 3_resist_100+ ## radiologic_6m rad_num improved ## &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 4_No_change 4 FALSE ## 2 2_Considerable_deterioration 2 FALSE ## 3 5_Moderate_improvement 5 TRUE ## 4 2_Considerable_deterioration 2 FALSE ## 5 1_Death 1 FALSE ## 6 6_Considerable_improvement 6 TRUE It is actually much easier to see the full width and height of a data set by scrolling, which you can do when you View() a dataset in the RStudio viewer. Try this out in the Console pane, with View(strep_tb). 4.5.7 Test yourself on the printing tibbles What function (and argument) let you print all the columns of a tibble? print(x, width=Inf) filter(starts_with(“width”)) sum(na.rm=TRUE) What function (and argument) let you print all the rows of a tibble? filter(ends_with(“all”)) median(na.rm=TRUE) print(x, n=Inf) If you just want a quick view of a few critical columns of your data, you can obtain this with the select() function, as in the code chunk below. Note that if you want to look at a random sample of your dataset, rather than the head or tail, you can use sample_frac() or sample_n() to do this. This sampling can be helpful if your data are sorted, or the head and tail rows are not representative of the whole dataset. See how this is used by running the code chunk below, which uses a 10% random sample of strep_tb to check that the mutate steps to generate the variables rad_num and improved worked correctly. #check that radiologic_6m, rad_num, and improved all match strep_tb %&gt;% sample_frac(0.1) %&gt;% select(radiologic_6m, rad_num, improved) ## # A tibble: 11 x 3 ## radiologic_6m rad_num improved ## &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 5_Moderate_improvement 5 TRUE ## 2 4_No_change 4 FALSE ## 3 4_No_change 4 FALSE ## 4 1_Death 1 FALSE ## 5 6_Considerable_improvement 6 TRUE ## 6 6_Considerable_improvement 6 TRUE ## 7 3_Moderate_deterioration 3 FALSE ## 8 1_Death 1 FALSE ## 9 3_Moderate_deterioration 3 FALSE ## 10 5_Moderate_improvement 5 TRUE ## 11 1_Death 1 FALSE 4.6 More exploration with skimr and DataExplorer Once you have your data read in, you usually want to get an overview of this new dataset. While there are many ways to explore a dataset, I will introduce two: skim() from the {skimr} package create_report() from the {DataExplorer} package You can get a more detailed look at a dataset with the {skimr} package, which has the skim() function, gives you a quick look at each variable in the dataset, with simple output in the Console. Try this out with the strep_tb dataset. Run the code chunk below, applying the skim() function to the strep_tb dataset. skimr::skim(strep_tb) Table 4.1: Data summary Name strep_tb Number of rows 107 Number of columns 13 _______________________ Column type frequency: character 1 factor 8 logical 1 numeric 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace patient_id 0 1 4 4 0 107 0 Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts arm 0 1.00 FALSE 2 Str: 55, Con: 52 gender 0 1.00 FALSE 2 F: 59, M: 48 baseline_condition 0 1.00 FALSE 3 3_P: 54, 2_F: 37, 1_G: 16 baseline_temp 0 1.00 FALSE 4 4_1: 43, 3_1: 32, 2_9: 25, 1_9: 7 baseline_esr 1 0.99 FALSE 3 4_5: 65, 3_2: 36, 2_1: 5, 1_0: 0 baseline_cavitation 0 1.00 FALSE 2 yes: 62, no: 45 strep_resistance 0 1.00 FALSE 3 1_s: 65, 3_r: 34, 2_m: 8 radiologic_6m 0 1.00 FALSE 6 6_C: 32, 5_M: 23, 1_D: 18, 3_M: 17 Variable type: logical skim_variable n_missing complete_rate mean count improved 0 1 0.51 TRU: 55, FAL: 52 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist dose_strep_g 0 1 1.03 1.00 0 0 2 2 2 ▇▁▁▁▇ dose_PAS_g 0 1 0.00 0.00 0 0 0 0 0 ▁▁▇▁▁ rad_num 0 1 3.93 1.89 1 2 5 6 6 ▇▅▁▆▇ 4.6.1 Test yourself on the skim() results How many females participated in the strep_tb study? 52 59 48 What proportion of subjects in strep_tb were improved? 0.493 50.514 0.55 What is the mean value for rad_num in strep_tb? 3.93 1.89 4.7 A fancier approach is taken by the DataExplorer package, which can create a full html report with correlations and PCA analysis. Copy and run the chunk below to see what the Data Profiling Report looks like. DataExplorer::create_report(strep_tb) ## ## ## processing file: report.rmd ## | | | 0% | |. | 2% ## inline R code fragments ## ## | |.. | 5% ## label: global_options (with options) ## List of 1 ## $ include: logi FALSE ## ## | |.... | 7% ## ordinary text without R code ## ## | |..... | 10% ## label: introduce ## | |...... | 12% ## ordinary text without R code ## ## | |....... | 14% ## label: plot_intro ## | |........ | 17% ## ordinary text without R code ## ## | |.......... | 19% ## label: data_structure ## | |........... | 21% ## ordinary text without R code ## ## | |............ | 24% ## label: missing_profile ## | |............. | 26% ## ordinary text without R code ## ## | |.............. | 29% ## label: univariate_distribution_header ## | |............... | 31% ## ordinary text without R code ## ## | |................. | 33% ## label: plot_histogram ## | |.................. | 36% ## ordinary text without R code ## ## | |................... | 38% ## label: plot_density ## | |.................... | 40% ## ordinary text without R code ## ## | |..................... | 43% ## label: plot_frequency_bar ## | |....................... | 45% ## ordinary text without R code ## ## | |........................ | 48% ## label: plot_response_bar ## | |......................... | 50% ## ordinary text without R code ## ## | |.......................... | 52% ## label: plot_with_bar ## | |........................... | 55% ## ordinary text without R code ## ## | |............................. | 57% ## label: plot_normal_qq ## | |.............................. | 60% ## ordinary text without R code ## ## | |............................... | 62% ## label: plot_response_qq ## | |................................ | 64% ## ordinary text without R code ## ## | |................................. | 67% ## label: plot_by_qq ## | |................................... | 69% ## ordinary text without R code ## ## | |.................................... | 71% ## label: correlation_analysis ## | |..................................... | 74% ## ordinary text without R code ## ## | |...................................... | 76% ## label: principal_component_analysis ## | |....................................... | 79% ## ordinary text without R code ## ## | |........................................ | 81% ## label: bivariate_distribution_header ## | |.......................................... | 83% ## ordinary text without R code ## ## | |........................................... | 86% ## label: plot_response_boxplot ## | |............................................ | 88% ## ordinary text without R code ## ## | |............................................. | 90% ## label: plot_by_boxplot ## | |.............................................. | 93% ## ordinary text without R code ## ## | |................................................ | 95% ## label: plot_response_scatterplot ## | |................................................. | 98% ## ordinary text without R code ## ## | |..................................................| 100% ## label: plot_by_scatterplot ## output file: /Users/peterhiggins/Documents/RCode/rmrwr-book/report.knit.md ## /usr/local/bin/pandoc +RTS -K512m -RTS /Users/peterhiggins/Documents/RCode/rmrwr-book/report.knit.md --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash --output /Users/peterhiggins/Documents/RCode/rmrwr-book/report.html --lua-filter /Library/Frameworks/R.framework/Versions/4.0/Resources/library/rmarkdown/rmarkdown/lua/pagebreak.lua --lua-filter /Library/Frameworks/R.framework/Versions/4.0/Resources/library/rmarkdown/rmarkdown/lua/latex-div.lua --self-contained --variable bs3=TRUE --standalone --section-divs --table-of-contents --toc-depth 6 --template /Library/Frameworks/R.framework/Versions/4.0/Resources/library/rmarkdown/rmd/h/default.html --no-highlight --variable highlightjs=1 --variable theme=yeti --include-in-header /var/folders/93/s18zkv2d4f556fxbjvb8yglc0000gp/T//RtmpJ78fmR/rmarkdown-str11ef9438d9e14.html --mathjax --variable &#39;mathjax-url:https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#39; ## ## Output created: report.html 4.6.2 Test yourself on the create_report() results What percentage of the baseline_esr values were missing? 9.3% 0.09% 0.93% 1% How many total data points (‘observations’) are there in the strep_tb dataset? 10,000 1,391 107 You can choose which you prefer, the simpler approach of {skimr} vs the fancier reports of {DataExplorer.} 4.7 Practice loading data from multiple file types 4.8 Practice saving (writing to disk) data objects in formats including csv, rds, xls, xlsx and statistical program formats 4.9 How do readr and readxl parse columns? 4.10 What are the variable types? Numeric - integer and double Character - string Logical - TRUE/FALSE Dates - an unholy formatting mess Factors - categorical variables nominal factors (no order) ordinal factors (with a meaningful order) 4.11 Controlling Parsing 4.12 Chapter Challenges There is a file named “paulolol.xlsx” with the path of ‘data/paulolol.xlsx’. A picture of this problematic file is shown below. paulolol Read in this file with the {readxl} package. Just for fun, try this see how this turns out with no additional arguments. Be sure to skip the problematic non-data first few rows Be sure to exclude the problematic non-data calculations below the table. Solution to Challenge 1: paulolol.xlsx Show Me Solution 1 read_excel(path = &#39;data/paulolol.xlsx&#39;, skip = 3, n_max = 8) ## # A tibble: 8 x 6 ## pat_id SBP_start SBP_end HR_start HR_end treatment ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 145 120 92 78 paulolol ## 2 2 147 148 88 87 placebo ## 3 3 158 139 96 80 paulolol ## 4 4 167 166 87 88 placebo ## 5 5 154 131 84 72 paulolol ## 6 6 178 177 99 97 placebo ## 7 7 151 134 101 86 paulolol ## 8 8 149 148 92 93 placebo Our intrepid Investigator has inserted a chart in place of his data table on sheet 1, and moved the data table to a 2nd sheet named ‘data’, and placed in the top left corner, at the suggestion of his harried statistician, in a new file with the path of ’data/paulolol2.xlsx\" Try reading this file in with read_excel() read the help file help(read_excel) to figure out how to read in the data from this excel file. Solution to Challenge 2: paulolol2.xlsx Show Me Solution 2 read_excel(path = &#39;data/paulolol2.xlsx&#39;, sheet = &#39;data&#39;) ## # A tibble: 8 x 6 ## pat_id SBP_start SBP_end HR_start HR_end treatment ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 145 120 92 78 paulolol ## 2 2 147 148 88 87 placebo ## 3 3 158 139 96 80 paulolol ## 4 4 167 166 87 88 placebo ## 5 5 154 131 84 72 paulolol ## 6 6 178 177 99 97 placebo ## 7 7 151 134 101 86 paulolol ## 8 8 149 148 92 93 placebo 4.13 Future forms of data ingestion https://www.datacamp.com/community/tutorials/r-data-import-tutorial?utm_source=adwords_ppc&amp;utm_campaignid=1658343521&amp;utm_adgroupid=63833880415&amp;utm_device=c&amp;utm_keyword=%2Bread%20%2Bdata%20%2Br&amp;utm_matchtype=b&amp;utm_network=g&amp;utm_adpostion=&amp;utm_creative=469789579368&amp;utm_targetid=aud-392016246653:kwd-309793905111&amp;utm_loc_interest_ms=&amp;utm_loc_physical_ms=9016851&amp;gclid=Cj0KCQjwxJqHBhC4ARIsAChq4auwh82WzCiJsUDzDOiaABetyowW0CXmTLbUFkQmnl1pn4Op9xcCcdQaAhMWEALw_wcB reading data from the web with readr reading data from Google Sheets with googlesheets reading data from web tables with rvest "],["wrangling-rows-in-r-with-filter.html", "Chapter 5 Wrangling Rows in R with Filter 5.1 Goals for this Chapter 5.2 Packages needed for this Chapter 5.3 Pathway for this Chapter 5.4 Logical Statements in R 5.5 Filtering on Numbers - Starting with A Flipbook 5.6 Filtering on Multiple Criteria with Boolean Logic 5.7 Filtering Strings 5.8 Filtering Dates 5.9 Filtering Out or Identifying Missing Data 5.10 Filtering Out Duplicate observations 5.11 Slicing Data by Row 5.12 Randomly Sampling Your Rows 5.13 Further Challenges 5.14 Explore More about Filtering", " Chapter 5 Wrangling Rows in R with Filter In this chapter, we will introduce you ways to wrangle rows in R. You will often want to focus your analysis on particular observations, or rows, in your dataset. This chapter will show you how to include the rows you want, and exclude the rows you don’t want. The main function in the tidyverse for doing this is filter(). The filter() function uses logical statements, or conditions, to decide whether to keep or exclude rows in your dataset. Figure 5.1: Artwork by Allison Horst, https://www.allisonhorst.com 5.1 Goals for this Chapter Understand logical statements Filter rows based on numeric values Filter rows based on string values and regex Filter rows based on dates Combine filters with boolean logic symbols, like AND (&amp;) and OR(|) Filter on missing (NA) values Filter rows for duplicates/distinct observations Slicing rows based on row number/position Sampling rows as random samples 5.2 Packages needed for this Chapter {tidyverse} {lubridate} {medicaldata} 5.3 Pathway for this Chapter This Chapter is part of the DATA WRANGLING pathway. Chapters in this pathway include What is Tidy Data? Filtering Rows Counting, Grouping, and Summarizing Rows Arranging and Ranking Rows Selecting Columns Mutating to Make New Variables Rearranging Untidy data with {tidyr} and {pivotr} 5.4 Logical Statements in R Logical statements in R are important for defining something that is TRUE or FALSE for each row, or observation, in your data. A typical logical statement involves a variable, and some criteria to test each observation of that variable, resulting in a TRUE or FALSE for each row. Typical examples of logical statements include: sbp &gt; 140 troponin_i &gt; 9.0 creatinine &gt;= 2.5 gfr &lt;= 60 Each of these examples tests each row in the database with the stated criterion, and results in a vector of TRUE or FALSE values for each row (observation) of the variable in the logical statement. The filter() function will act on these TRUE and FALSE values to include (TRUE) or exclude (FALSE) the observations from the result. 5.5 Filtering on Numbers - Starting with A Flipbook This flipbook will show you step-by-step examples of how to filter rows of observations based on logical statements involving numbers. In each logical statement, a variable will be compared with a numeric value via a mathematical operator. These operators can include comparisons with greater than (&gt;) greater than or equal to (&gt;=) less than (&lt;) less than or equal to (&lt;=) equal to (==) notice two equals signs to test equality near() - an equality test that works with tiny decimal differences The general format for filter statements is: filter(variable [comparison operator] value), like filter(sbp &gt; 140) will keep only rows with systolic blood pressure values greater than 140. If you have not used a flipbook before, you can click on the frame below to activate it, then use the right and left arrow keys to move forward and back through the examples. With each forward step in the code on the left, examine the resulting output on the right. Make sure you understand how the output was produced. In many of the examples, a select() function is used to reduce the number of columns before the filter() step to make the results more clear. You saw several examples of filtering, including Example Code equality to a value filter(recurrence == 1) greater than a value filter(preop_psa &gt; 20) near a value filter(near(preop_psa, 10)) near a value with a set tolerance filter(near(preop_psa, 17, tol = 1.5)) between 2 values filter(between(preop_psa, 10, 13)) in a list of values filter(preop_psa %in% c(10,13,17)) 5.5.1 Your Turn - learnr exercises Try this out yourself, with interactive coding exercises below, written with the {learnr} package. For each exercise, you can type in the code required,by the instructions, then click on the blue Run Code button at the top right of the exercise to test your code. Watch out for a red x in the left margin - that identifies a coding error - hover over it for an explanation of what is wrong. If you get stuck, you can click on the Hint button to get a hint, and if needed, the Copy Clipboard button to copy the hint so that you can paste it into the Exercise box. 5.6 Filtering on Multiple Criteria with Boolean Logic You can use multiple filters on your data, and combine these with the boolean symbols AND (symbol &amp;) OR (symbol | ) - the vertical line, often on the keyboard above the backslash character ( \\ ) XOR - exclusive OR, so that the whole logical statement is true if either statement A is true, or statement B is true, but NOT if both are true. parentheses and combinations thereof. NEGATION - an exclamation point ( ! ) placed before an equals sign or a variable in a logical statement makes it mean the opposite. and you can use parentheses as well, to control the order of operations. You saw several examples of filtering, including Example Code AND filter(age &gt; 65 &amp; t_vol&gt;1) OR filter(age &gt; 69 | t_vol &gt; 1) exclusive OR (XOR) filter(xor(age &gt; 69, t_vol &gt; 1)) AND with negation (!) filter(age &gt; 64 &amp; aa != 1) 5.6.1 Your Turn - learnr exercises Try this out yourself, with interactive coding exercises below, written with the {learnr} package. For each exercise, you can type in the code required,by the instructions, then click on the blue Run Code button at the top right of the exercise to test your code. Watch out for a red x in the left margin - that identifies a coding error - hover over it for an explanation of what is wrong. If you get stuck, you can click on the Hint button to get a hint, and if needed, the Copy Clipboard button to copy the hint so that you can paste it into the Exercise box. 5.7 Filtering Strings You can use == to test exact equality of strings, but you can also use str_detect from the {stringr} package for more flexible matching, and combine it with the magic of regex (regex ~ Regular expressions) to do complicated filtering on character string variables in datasets. The typical formats for string filtering are filter(variable == “string”) for an exact match, or filter(str_detect(variable, pattern = “string”)) [note two parentheses at the end to completely close the parentheses with str_detect] You saw several examples of filtering strings, including Example Code matches “oma” filter(str_detect(diagnosis, “oma”)) negated match filter(!str_detect(diagnosis, pattern = “Hodgkin”)) regex for wild card filter(str_detect(diagnosis, pattern = “lympho.+ic”)) regex wild card filter(str_detect(diagnosis, “myelo.*”)) exact match filter(diagnosis == “myelofibrosis”) 5.7.1 Your Turn - learnr exercises Try matching strings yourself, in the interactive coding exercises below, written with the {learnr} package. For each exercise, you can type in the code required,by the instructions, then click on the blue Run Code button at the top right of the exercise to test your code. Watch out for a red x in the left margin - that identifies a coding error - hover over it for an explanation of what is wrong. If you get stuck, you can click on the Hint button to get a hint, and if needed, the Copy Clipboard button to copy the hint so that you can paste it into the Exercise box. 5.8 Filtering Dates You can use the {lubridate} package to format strings for logical tests, and filter your observations by date, month, year, etc. Dates are commonly formatted in ISO 8601 format, or YYYY-MM-DD, for 4-digit year, 2-digit month, and 2-digit day. The {lubridate} package can convert these to dates if the ymd() function is applied to a string formatted this way. The typical formats for date filtering are filter(date == ymd(“2002-10-06”)) for an exact match, or filter(between(datevar, ymd(“2020-01-01”, ymd(“2020-06-30”)) filter(datevar &gt; today() - years(1)) for the past year &lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream You saw several examples of filtering dates, including Example Code between 2 dates filter(between(fake_date, ymd(“2020-03-01”), ymd(“2020-03-31”) 24 months prior to today filter(fake_agvhd_date &gt; today() - months(24)) filter to weekdays 1 or 7 (weekend) filter(wday(fake_bmt_date) %in% c(1,7)) 5.8.1 Your Turn - learnr exercises Try matching dates yourself, in the interactive coding exercises below, written with the {learnr} package. For each exercise, you can type in the code required,by the instructions, then click on the blue Run Code button at the top right of the exercise to test your code. Watch out for a red x in the left margin - that identifies a coding error - hover over it for an explanation of what is wrong. If you get stuck, you can click on the Hint button to get a hint, and if needed, the Copy Clipboard button to copy the hint so that you can paste it into the Exercise box. 5.9 Filtering Out or Identifying Missing Data You can use the is.na(), drop_na() and negation with ! to help identify and filter out (or in) the missing data, or observations that are incomplete. Common formats for this include is.na(variable) - filters for observations where the variable is missing !is.na(variable) - filters for observations where the variable is not missing janitor::remove_empty(“rows”) - removes completely empty rows (helpful if you are importing a messy dataset) drop_na() - without filter, this drops any observations with at least one NA value in the row. drop_na(variable1, variable2) - without filter, this drops any observations with at least one NA value iin the variable1 or variable2 columns. 5.9.1 Working with Missing data A common need when doing DEV (Data Exploration and Validation) is to find your missing datapoints, so that you can figure out why they are missing, and whether you can fix this using other records, including the medical record. Another common scenario, when you can’t find or fix missing data, is that you want to drop observations which are missing in a particular variable. Sometimes you will need to drop whole empty rows, which usually happens because of poor formatting of the data (often in a spreadsheet). Sometimes you will need to drop observations that have missing data in any field, which is important in modeling. Linear and logistic models generally only run on complete cases, where all predictors and outcomes are non-missing. You can also impute missing data points, if there are not too many (&lt;10%), and these are truly missing completely at random (MCAR), which is often not the case. Usually there is a bias, so that particular types of study participants have more missing data than others. The packages {missForest} and {mice} can be helpful for imputation when MCAR assumptions are met. Imputation is beyond the scope of this chapter, and should generally be done in consultation with a statistician, as the assumptions involved are very important. You saw several examples of filtering missing data, including Example Code filtering for the missing observations in a variable fi lter(is.na(fake_date)) filter for the non-missing data in a variable filter (!is.na(fake_dx_date)) removing empty rows remove_empty(“rows”) removing incomplete cases drop_na() 5.9.2 Your Turn - learnr exercises 5.10 Filtering Out Duplicate observations You can use the distinct() function and the {janitor} package to help you find duplicated observations/rows for fixing or removal from your dataset. Common formats for this include distinct(dataset) - filters for rows that are distinct from all the other rows (non-duplicates). janitor::get_dupes(dataset) - filters for observations that are duplicated in all variables, counts them, and displays them in duplicated groups. You saw several examples of filtering duplicate data, including Example Code distinct observations distinct(dataset) finding duplicates get_dupes(dataset) making the complementary dataset dataset_whole %&gt;% anti_join(data_subset) 5.11 Slicing Data by Row You can use the slice() family of functions to cut out a chunk of your observations/rows by position in the dataset.. Common formats for this include slice(X:Y) - filters for rows X to Y by position (row number). slice_max(variable, n = N) - filters for observations with the maximum N values for a single variable. slice_min(variable, prop = 0.0N) - filters for observations with the minimum proportion of 0.0N values for a single variable. slice_head and slice_tail - filter for observations by position in the top (or bottom) N or top/bottom proportion. You saw several examples of slicing data, including Example Code slice by position slice(100:200) slice by top values in variable slice_max(age, n=20) slice by bottom values in variable slice_min(pan_day, prop = 0.01) slice by top or bottom position slice_tail(prop = 0.01) 5.12 Randomly Sampling Your Rows You can use the slice_sample() function to take a random subset of large datasets, or to build randomly selected training and testing sets fo modeling. Common formats for this include slice_sample(prop = 0.N) - randomly samples N % of your rows. slice_sample(n = N) - randomly samples N of your rows. You saw several examples of sampling data, including Example Code sampling a proportion slice_sample(prop = 0.3) sampling N rows slice_sample(n = 50) 5.12.1 Your Turn - learnr exercises 5.13 Further Challenges Try filtering on your own computer. Install R and Rstudio, as in Chapter 2. Then make sure you have the following packages installed, as in the Installing Packages chapter: tidyverse medicaldata Read in data from the medicaldata datasets with data(name) Then try some of the following challenges filter the cytomegalovirus dataset for rows with time to acute gvhd &lt; 6 months and CD3 dose &gt; 4 filter the opt dataset for Birthweight between 2500 grams and 3500 grams, or Apgar &gt;=6 filter the covid_testing dataset for females tested in the first 30 days of the pandemic with positive results 5.14 Explore More about Filtering Some helpful resources include: Suzan Baert’s blog series on dplyr, part 3 about filtering is found here. The tidyverse guide to filtering, found here. A blog post on filtering by Michael Toth, found here. The RYouWithMe series from RLadiesSyndey post on filtering here. "],["interpreting-error-messages.html", "Chapter 6 Interpreting Error Messages 6.1 The Common Errors Table 6.2 Examples of Common Errors and How to fix them 6.3 Errors Beyond This List 6.4 When Things Get Weird 6.5 References:", " Chapter 6 Interpreting Error Messages Especially when you are starting out, it can be very difficult to interpret error messages, because these can be quite jargon-y. Let’s start with a table of the most common error messages, and the likely cause in each case. Note that when reading an error message, there are two parts - the part before the colon, which identifies in which function the error occurred, and the part after the colon, which names the error. A typical error message is usually in the format: Error in Where the error occurred : what the error was here is an example Error in as_flextable(.) : object 'errors' not found On the left, you are being told that the error occurred when the as_flextable() function was called. This can be helpful if you have run a long pipeline of functions, as it helps you isolate the problem. On the right, you are being told what the error was. In this case, the function looked for the object errors in the working environment (see your Environment tab at the top right in RStudio), and could not find it. Note that sometimes syntax errors caused by missing components (a missing comma, a missing parenthesis, a missing pipe symbol %&gt;% , or a missing + sign in a ggplot pipe) will cause an error in the next function in the pipeline. Watch out for this, especially when the function where the error is found looks fine - often it occurs because there is a missing piece just before this function. Then we will walk through examples of how to create each error, and how to fix them, one by one. 6.1 The Common Errors Table Examine the error message from R, particularly the part that comes after the colon (:). The error messages listed in the left column will be what appears after the colon (:) Common Error Messages in R Error Message What it Means could not find function This usually means that you made a typographical error in the function name (including Capitalization - R is case-sensitive), or that the package you are intending to use (which contains the function) is not installed - with `install.packages(‘package_name’)` or the package is not loaded - with `library(package)` object ‘object-name’ not found This usually means that the function looked for an object (like a data frame or a vector) in your working environment (check your Environment pane) and could not find it. This commonly happens when you mistype the name of the object (double-check this, easy to fix), or you did not actually create or save this object to your working environment - confirm by checking your Environment tab at the top right in RStudio. filename does not exist in current working directory (‘path/to/working/directory’) This usually means one of three things: you mistyped the name of the file, or part of the path, you are not in the directory where the file is, or the file you thought you had saved does not exist (check your Files tab in the lower right pane in RStudio). error in if This usually means that you have an *if* statement that is trying to make a branch-point decision, but the logical statement that you wrote is not providing either a TRUE or a FALSE value. The most common reasons are typographical errors s in the logical statement, or an NA in one of the underlying values, which yields an NA from the logical statement. You may need to use a `na.rm = TRUE` option in your logical statement (the na.rm issue often comes up with means and medians). error in eval This usually occurs when you are trying to run a function on an object that does not exist in your environment. Check to make sure in your Environment pane, and consider that you may not have saved/assigned the object in a previous step. Alternatively, you may have a typographical error in the object name. Worth checking. cannot open This usually occurs when you are attempting to access or read a file that either does not exist, or is not in the folder that you thought it was. Check your working directory and find the file in your file structure. This can often be prevented by working in RStudio projects and using the here() function for paths to files. no applicable method This usually occurs when you are using a function that expects a particular data structure (vector, list, dataframe), but you have given it a different data structure as the input. Check the data structure of your object, and check the documentation for your function. For example, if you want to use a function that acts on vectors, this function will not work on a dataframe object. You may have to use the `pull(variable)`function to “pull” this variable out of the dataframe into a vector before using this function. subscript out of bounds you are trying to access an item in an environment object (like a vector, dataframe, or list) that does not exist, like the 9th item in a vector that is 7 items long, or the negative 2nd row of a dataframe. Check the length of the item, and the math that you used to count the item number (loops that go too long are often a culprit) replacement has [x] rows, data has [y] rows This usually occurs when you are trying to code for a new variable, or replace a variable in a dataframe. But somehow (missing values, NAs), what you are trying to add to the dataframe is not the same length (number of rows) as the rest of the existing dataframe. Use a length() function to check your building of this vector at each step, to figure out where your length went wrong. package not available for R version x.y.z This occurs when you are trying to install a package, and your R version is newly updated. The problem is that the package version available on CRAN has not caught up to your shiny new version of R. This can happen after an R update when the package developer is working on updating their package, but the new version has not made it onto CRAN yet. This is often fixable if you know where the developer stores their development code (usually on GitHub). For example, if the package is {medicaldata}, and the developer’s Github userid is higgi13425, then you can install the development version of this package with remotes::install_github('higgi13425/medicaldata'). This assumes that you have already installed and loaded the {remotes} package. non-numeric argument to a binary operator A binary operator, like + or *, is a mathematical operation that takes two values (operands) and produces another value. It gets grumpy when trying to do math on things that are not numbers. A typical input to produce this error would be 1 + 'one' - one operand is numeric, and the other 'one' is a character string - the non-numeric argument. object of type closure is not subsettable This occurs when you try to extract a subset of something - but it is actually a function, not an object. This most commonly occurs when you try to subset a particular object that does not exist, like df$patient_id or data$sbp, when you have not created the objects df or data. The reason you get this strange error message, rather than simply Error: object 'df' not found , is that df() and data() are defined functions in base R. It is good practice to avoid naming any objects data or df for this reason. It gets very confusing, and this is best avoided. 6.2 Examples of Common Errors and How to fix them 6.2.1 Missing Parenthesis This is a very common error. It is easy to lose track of how many sets of parentheses you have open in putting together a complicated function. Here is an example, where a closing parenthesis is missing from a mutate() function. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt;65 &amp; aa == 1 ~ 1, TRUE ~0) %&gt;% filter(older_aa ==1) In this case, no output is produced, and the console does not return to the &gt; prompt. Instead, it offers a + prompt - in effect, asking you for something more. If you type in an extra closing parenthesis (after the filter function), it will give you an error. The error you get is: Error: Problem with `mutate()` input `older_aa`. x no applicable method for ‘filter_’ applied to an object of class “c(‘double’, ‘numeric’)” ℹ Input `older_aa` is ``%&gt;%`(…)`. R identifies a problem with the input “older_aa” to mutate - the parentheses are not closed. It then fails on the next function - filter, and gives you a strange error message - filter_ applied to… - because the input to the filter step (the next step after the error) was incoherent. This can be a bit confusing. But if you inspect the input older_aa, you will find the mis-matched parentheses. This is much easier to find with “rainbow parentheses” turned on in Tools/Global Options. When this option is on, you can be sure your parentheses are right when you end on red. Sometimes the very thin parentheses of some fonts can make it difficult to identify your red close parenthesis. The red vs. yellow parentheses may be easier to differentiate if you use a bold font for your code. You can change this with RStudio Tools/Global Options/Appearance. I am currently using FiraCode-Bold and the Crimson Editor theme in RStudio, which seems to help me. In this case, adding the missing parenthesis to the mutate step fixes it. Parentheses that end on red are all right. 6.2.2 An Extra Parenthesis What if you go the other way, with an extra parenthesis after some misguided copy-paste adventures? Let’s see what happens. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt;65 &amp; aa == 1 ~ 1, TRUE ~0))) %&gt;% filter(older_aa ==1) In this code block, you will end up with two red closing parentheses, and when you click to the right of the final closing parenthesis, there will be no matching highlighted open parenthesis (note that the preceding closing parentheses both have matching highlighted open parentheses. Both of these are clues that this last one is an extra. The error you get from R is Error in filter(older_aa == 1) : object ‘older_aa’ not found The left side of the error message identifies the filter step as where the error occurs, and the right side of the error message states that the error is an object not found. The error occurs when R gets to the next function. It also tells you that older_aa was not successfully created - suggesting that the problem is in the step before the filter function. In this case, removing the extra parenthesis from the mutate step fixes it. 6.2.3 Missing pipe %&gt;% in a data wrangling pipeline This is a common error. It is easy to cut out one of your %&gt;% connectors when you are editing/debugging a data wrangling pipeline. Here is an example, where a %&gt;% is missing. Can you spot it? prostate %&gt;% select(t_vol, p_vol, age, aa) mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt;65 &amp; aa == 1 ~ 1, TRUE ~0)) %&gt;% filter(older_aa ==1) In this case, the error you get is: Error in mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt; 65 &amp; aa == : object ‘t_vol’ not found The left side of the error message identifies the mutate step as where the error occurs, and the right side of the error message states that the error is an object not found. This is a bit misleading, as the problem is not in the mutate step. But mutate is where the pipeline crashes, as it can not find the variable t_vol. You have to backtrack upwards line-by-line to find the error. Every line of a data wrangling pipeline should end in %&gt;%. Since this is such a common error, this should be one of your “usual suspects”. And the select line, just above the mutate line, is where the problem is. In this case, adding the missing %&gt;% to the end of the select step fixes your data wrangling pipeline. Use one function per line in a pipeline. Check every data wrangling pipeline to make sure each step (except the last) ends in a pipe %&gt;% 6.2.4 Missing + in a ggplot pipeline This is a common error. It is easy to cut out one of your + connectors when you are editing/debugging a ggplot. Here is an example, where a + is missing in the middle of a ggplot pipeline. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% ggplot(aes(x = factor(t_vol), y =p_vol)) geom_boxplot() + labs(x = &quot;tumor volume&quot;, y = &quot;prostate volume&quot;) + theme_minimal() In this case, you get a ggplot output, but without any boxplots. It is also missing your custom labels for the x and y axes, and the theme you wanted. Essentially, the code stops running after the initial ggplot() statement and the remaining lines of code are ignored. This can be pretty puzzling, as you do get a plot, but not what you intended. There is a partial plot in the Plots tab, but you get a somewhat helpful error in the Console. The error you get is: Error: Cannot add ggproto objects together. Did you forget to add this object to a ggplot object? R identifies a problem with the last 3 lines of code, starting with geom_boxplot() - it can not add these ggproto objects (the components of a ggplot) to the existing plot. It asks, “Did you forget to add?” which should be a clue that there is a missing + sign between lines of ggplot code. Since the theme and labels are the defaults, and there are no boxplots, suggest that these last 3 lines were not run at all, and that the missing plus sign should be found just before these lines of code. In this case, adding the missing + to the end of the ggplot step fixes your plot. Use one function per line in a pipeline. Check every ggplot pipeline to make sure each step (except the last) ends in a plus sign + 6.2.5 Pipe %&gt;% in Place of a + This is a common error. It is easy to start with your dataset, do some data wrangling steps with the pipe %&gt;% and keep piping out of habit, even after you start your ggplot. Unfortunately, once you start to ggplot, you have to use + as your code connector. Having a pipe instead will cause an error. Here is an example, where a %&gt;% is used instead of + in a ggplot pipeline. It usually happens at the beginning of the ggplot, when you are still in piping mode. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% ggplot(aes(x = factor(t_vol), y =p_vol)) %&gt;% geom_boxplot() + labs(x = &quot;tumor volume&quot;, y = &quot;prostate volume&quot;) + theme_minimal() In this case, you will not get a ggplot output, and you will get an error in the console. The error you get is: Error: `mapping` must be created by `aes()` Did you use %&gt;% instead of +? The error message identifies the aes() step as where the error occurs. R identifies a problem that causes the aes function to fail to create a mapping. The first line is not very helpful (other than identifying aes() as a problem), but in the next line, R asks, “Did you use %&gt;% instead of +?” which is very helpful. Once you know this, look at the line where aes() failed. This is where there is a pipe in place of a plus. In this case, replacing the %&gt;% with a + fixes your plot. 6.2.6 Missing Comma Within a Function() This is a common error. It is easy to start a series of arguments to a function, like multiple variables in a mutate step, and miss a comma between them. Here is an example, where a comma is missing in a series of mutate steps. Note that it is a good habit to put one mutate step on each line, with each line ending in a comma. This will help you find the missing comma if (no, when) you make this mistake. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt;65 &amp; aa == 1 ~ 1, TRUE ~0) age_decade = floor(age / 10)) %&gt;% filter(older_aa ==1) In this case, you will not get a tibble output, and you will get an error in the console. The error you get is: Error in filter(older_aa == 1) : object ‘older_aa’ not found The left side of the error message identifies the filter step as where the error occurs, and the right side of the error message states that the error is an object not found. R identifies a problem that causes the filter function to fail, but this is actually a problem in the line prior. The variable older_aa was not created and is not available to filter. It should have been created in the mutate step, but this step is where the failure occurred. Because you formatted the mutate step with one mutate statement per line, it is easy to check each line for a comma - and the older_aa line is missing its comma. In this case, adding a comma at the end of the older_aa line (after “TRUE ~0)” fixes your data wrangling pipeline. 6.2.7 A Missing Object This is a common error. You may have created or modified a dataframe, but forgot to assign it to a new object name. Or maybe you did this assignment in a different session, but have not done it in your current session. Or maybe you made a typographical error in calling the object (“covvid” instead of “covid”). Either way, this object is not yet loaded into your computing environment (the Environment tab). In this example, we request data from the {medicaldata} package, but forget to assign it to an object. So it does not exist when we try to use it to start a pipeline. This does not work. medicaldata::covid_testing covid %&gt;% select(subject_id, age, result, ct_result, patient_class) %&gt;% mutate(high_titer = case_when(ct_result &lt; 18, TRUE ~ 0), age_decade = floor(age / 10)) %&gt;% filter(age &gt;50) In this case, you will not get a tibble output, and you will get an error in the console. The error you get is: Error in select(., subject_id, age, result, ct_result, patient_class) : object ‘covid’ not found The portion to the left of the comma identifies where the error occurs - in the select step. The portion to the right of the comma identifies the error. This one is easy. The object ‘covid’ was not found. You can check your Environment pane, and it will not be there. What the coder intended was to call medicaldata::covid_testing and assign it (with an arrow) to a new object named covid. But that assignment did not happen, and R is unable to guess what you meant. In this case, adding an assignment arrow -&gt; to the end of the medicaldata::covid_testing line and then covid completes the assignment, creates the covid object, and fixes your data wrangling pipeline. 6.2.8 One Equals Sign When you Need Two This is a very common error. The equals sign is commonly used in two ways in R. To assign a parameter or argument of a function, like x = p_vol, or ratio = p_vol/t_vol, or color = “blue”. In all of these assignment cases, you use one equals sign. To test a logical statement, like age == 60, or fam_hx == 1, or location == “Outpatient”. In all of these logical tests, you use two equals signs. It is very common to use one equals sign in a logical statement. This causes errors. Watch the last filter step below. prostate %&gt;% select(t_vol, p_vol, age, aa) %&gt;% mutate(ratio = t_vol/p_vol, older_aa = case_when(age &gt;65 &amp; aa == 1 ~ 1, TRUE ~0), age_decade = floor(age / 10)) %&gt;% filter(older_aa ==1) In this case, the error you receive is very helpful: Error: Problem with `filter()` input `..1`. x Input `..1` is named. ℹ This usually means that you’ve used `=` instead of `==`. ℹ Did you mean `older_aa == 1`? The problem is with the filter step. The error starts out very jargon-y. “input `..1`. x Input `..1` is named” - means the input to filter is actually named (an assignment). But then it gets a lot more helpful. It recognizes that you have made a common error, and suggests an appropriate fix. In this case, adding a 2nd equals sign in the filter step fixes your data wrangling pipeline. Testing for equality with == is a big problem with real numbers, rather than integers. Computers use algorithms to do math which are not quite exact, leading to small differences in decimals. The == equality test is very strict, so that something like sqrt(2)^2 == 2 is FALSE because of small differences far to the right of the decimal point, which can trip you up. You can see these if you run the modulo 2: sqrt(2)^2 %% 2, which gives you the remainder after you divide by 2, which is the very tiny 0.0000000000000004440892. In this situation, you should use the near() function, as near(sqrt(2)^2, 2) is TRUE. The near function has a built-in tolerance of 0.00000001490116, which will be able to handle any computer-generated small, stray decimals. You can set your own tolerance argument if needed. 6.2.9 Non-numeric argument to a binary operator This happens when you try to do math on things that are not numbers. It usually occurs when you have a variable(column) that looks like it is numeric (it contains numbers), but somewhere along the way it became a character string variable. This often occurs when data are being entered into a spreadsheet, and one value in the column has characters in it. This often happens when you have a column of systolic blood pressures, and one value is entered as “this was not done”, or “102, but taken standing up”. Having comments, even if only one character string in a column in Excel makes the whole column into the character string data type. This is not apparent until you try to do math with this variable, as in data %&gt;% mutate(mean_art_pressure = sbp/3 + 2/3* dbp) This will give you the error: Error in mutate(mean_art_pressure: non-numeric argument to binary operator To fix this, you will have to Determine which variable, sbp or dbp, is non-numeric (glimpse(data) will help). Review the values of the problem variable (possibly with table()) to find which is non-numeric. Fix these values manually in your code, and document with comments Which values are being fixed (e.g. sbp for subject 007, at visit 2) data$sbp[subject == 007 &amp; visit == 2] &lt;- 102 What the original value was, and what the new value will be Who made the change to the data Why the data change was made On what date the data change was made Never over-write your original data - keep a complete audit trail! 6.3 Errors Beyond This List This is where the internet comes in handy. Whatever errors you can create, someone has already run into. And they have asked for help on the internet, and most of the time, someone has helped them solve their error. You should copy your entire error message, and paste it into a web search. Google will often yield multiple similar examples, with various ways to solve the problem. In a future chapter, we will explore more effective ways to seek help, using a minimal reprex to describe your problem accurately to other people on sites like community.rstudio.com. Remember that the error may have occurred because of a problem in the previous line of code (missing parenthesis, comma, etc.), so don’t forget to check one line above. The Add-One-Line debugging strategy is another good strategy. Select the code for your pipeline from the beginning to 2 lines of code before the error. If that runs without errors, add one line to your selection, and run it. Keep adding lines to your selection and running until you hit the error. Then try to find the problem and fix it. 6.4 When Things Get Weird 6.4.1 Restart your R Session (Shift-Cmd-F10) If you are running code that has worked before, and it is not working now, it is possible that you have created something odd in your working Environment that is interfering with your code. Sometimes it is an old object from a previous session (it is always better to start from a clean slate). Completely restart your R session (click on Session/Restart R, or use the keyboard shortcut), make sure the Environment is clean, then run your code from start to finish to give it a new try. Sometimes a clean slate will make all the difference. 6.5 References: These are some helpful general references on troubleshooting in R, particularly focused on error messages encountered by beginners to R. Click on some of the links to check these out. https://bookdown.org/yih_huynh/Guide-to-R-Book/trouble.html https://medium.com/analytics-vidhya/common-errors-in-r-and-debugging-techniques-f11af3f1c7d3 https://rpubs.com/Altruimetavasi/Troubleshooting-in-R https://github.com/noamross/zero-dependency-problems/blob/master/misc/stack-overflow-common-r-errors.md https://www.r-bloggers.com/2016/06/common-r-programming-errors-faced-by-beginners/ https://www.r-bloggers.com/2015/03/the-most-common-r-error-messages/ https://rpubs.com/Altruimetavasi/Troubleshooting-in-R https://statisticsglobe.com/errors-warnings-r "],["tips-for-hashtag-debugging-your-pipes-and-ggplots.html", "Chapter 7 Tips for Hashtag Debugging your Pipes and GGPlots 7.1 Debugging 7.2 The Quick Screen 7.3 Systematic Hunting For Bugs in Pipes 7.4 Systematic Hunting For Bugs in Plots 7.5 Hashtag Debugging 7.6 Pipe 2 7.7 Plot 2 7.8 Plot3 7.9 Pipe 3", " Chapter 7 Tips for Hashtag Debugging your Pipes and GGPlots 7.1 Debugging While layering functions with piping in the tidyverse and the plus sign in ggplots is a great way to keep your code clear, your will invariably find some bugs in your code. This can be pretty frustrating, especially when you spend a lot of time staring at the code and it finally turns out to be something trivial, like a missing parenthesis, a pipe in place of a + (or the reverse), a missing aes() statement, or a mistyped function (fitler instead of filter). So how can you systematically approach debugging a pipe or plot, and find and fix your problem efficiently without spending tons of time? 7.2 The Quick Screen Start by checking for common errors - check for The Big 5. Click just to the right of each final parenthesis on each line. Does this result in highlighting the first parenthesis on that line of code? If not, you are probably missing a closing parenthesis. When piping, check that you have a proper pipe %&gt;% at the end of each line, except for the last line. When plotting, check that you have a plus sign + at the end of each line, except for the last line. Error message about a missing function - Error in function(arguments) : could not find function \"func\". Either you are calling a function from a library you have not loaded, or you mis-typed the function. If the library was not loaded, go back to the top and load the library that this function comes from. If you mis-typed the function (for example, /selcet), fix the typographical error. The error message will tell you which function seems to be missing. Error message about a missing object - Error: object 'xxxobj' not found. Make sure that you have typed the names of the dataframe and the variables correctly. Each of these is a data object. The error message will tell you which object seems to be missing. Your Turn: Search for The Big 5 in the pipe below (Hint - there are 5 Bugs to Be Found) You should end up with 4 columns of 10 rows, sorted by efficiency, when all of the bugs have been fixed. mtcars %&gt;% filter(cyl &gt;4) %&gt;% select(mpg, hp, displ) %&gt;% mutate(efficiency = hp/disp ) + filtre(efficiency &gt; 0.5) %&gt;% arrange(desc(efficiency) %&lt;% slice(1:10) ## Error: &lt;text&gt;:8:0: unexpected end of input ## 6: arrange(desc(efficiency) %&lt;% ## 7: slice(1:10) ## ^ mtcars %&gt;% filter(cyl &gt;4) %&gt;% select(mpg, hp, displ) %&gt;% # watch for typos in object names mutate(efficiency = hp/disp ) + # watch for misplaced + vs %&gt;% filter(efficiency &gt; 0.5) %&gt;% # watch for typos arrange(desc(efficiency) %&lt;% # watch for mistyped pipes slice(1:10) ## Error: &lt;text&gt;:8:0: unexpected end of input ## 6: arrange(desc(efficiency) %&lt;% # watch for mistyped pipes ## 7: slice(1:10) ## ^ mtcars %&gt;% filter(cyl &gt;4) %&gt;% select(mpg, hp, displ) %&gt;% mutate(efficiency = hp/disp ) %&gt;% filter(efficiency &gt; 0.5) %&gt;% arrange(desc(efficiency)) %&gt;% # watch for missing closing parentheses slice(1:10) ## Error: Can&#39;t subset columns that don&#39;t exist. ## x Column `displ` doesn&#39;t exist. 7.3 Systematic Hunting For Bugs in Pipes What if it is not one of the Big 5 Pipe Bugs, and you need to hunt systematically? Let’s start by adding a pipe to the last line (slice(1:10)), and then a return() as a new line following slice(1:10), so that you now have an 8 line pipe. Note it will be blue - this is OK. It just shows that this is an important function. Adding return() just returns the result of the previous 7 lines, which does not seem like much, but it makes it a lot easier to use our debugging MVP, the hashtag. First, in debugging a pipe, you want to be able to quickly select and run lines repeatedly. You can do this with your mouse, but it is slow and sometimes inaccurate. You can do this faster with the keyboard. To run a whole pipe, just click anywhere in the pipe and press the key combination: Cmd-Shift-Return on the Mac Ctrl-Shift-Return on PC With this key combination, you don’t have to use your mouse to select lines to run. This will run the whole pipe or plot. Now you have to take control of exactly which lines of the pipe will run. You want to run a series of experiments to isolate the bug. Start by running the pipe from the top. Run just your first line (data) by putting a hashtag just before the first pipe %&gt;% and pressing Cmd-Shift-Return while still on that line. If that works, delete the hashtag and repeat the process on the 2nd line. You can stop running the pipe after only 2 lines by placing a hashtag just before the 2nd pipe. Then press your Cmd-Shift-Return key combo to run just the first 2 lines. If that works, try running the first 3 lines, by deleting the hashtag on line 2, and putting it just before the pipe on line 3 (Copy-Paste can help). Use this approach to run successively more lines of the pipe in the code chunk below. In which line of the pipe below do you hit the first error (bug)? iris %&gt;% filter(Sepal.Length &lt;5) %&gt;% select(Sepal.Length, Sepal.Width, Species) %&gt;% mutate(Sepal.Area = Sepal.Lngth * Sepal.Width) %&gt;% filter(Sepal.Area &gt;10) %&gt;% arrange(desc(Sepal.Area) %&gt;% slice(1:10) %&gt;% return() ## Error: &lt;text&gt;:10:0: unexpected end of input ## 8: return() ## 9: ## ^ iris %&gt;% filter(Sepal.Length &lt;5) %&gt;% select(Sepal.Length, Sepal.Width, Species) %&gt;% mutate(Sepal.Area = Sepal.Lngth * Sepal.Width) %&gt;% # watch for typos filter(Sepal.Area &gt;10) %&gt;% arrange(desc(Sepal.Area) %&gt;% # watch for missing parentheses slice(1:10) %&gt;% return() ## Error: &lt;text&gt;:10:0: unexpected end of input ## 8: return() ## 9: ## ^ Great. Now you know the bug is somewhere in lines 4-7. You can selectively turn off one line at a time by putting a hashtag at the beginning of the line. Use this approach to turn off line 5 (filter). Does this fix the pipe? If not, try lines 4,6,7 individually. Turning off which one gets rid of the error/changes the error? Changing the error means that you made it at least a little bit farther before a new error occurred. Now hunt in the ‘commented out/hashtagged’ line for an error you can fix. Once you think you have fixed it, try running the pipe up to and including that line (hashtag just before pipe in that line). Does that work? If yes, you have made progress. Keep going line by line until you find and fix the next bug, until you can get the whole pipe to run. 7.4 Systematic Hunting For Bugs in Plots The Big 5 Common Errors in Plots are slightly different. Click just to the right of each final parenthesis on each line. Does this result in highlighting the first parenthesis on that line of code? If not, you are probably missing a closing parenthesis. When plotting, check that you have a plus sign + at the end of each line, except for the last line. Missing aes() mapping. It is easy to get excited about your ggplot and declare variables in the ggplot statement, and forget about wrapping the mapping of x and y in an aesthetic function. Check to make sure that every time you map variables in your data to a plot component (x,y, color, shape, size, etc.) that this occurs inside an aes() call. Error message about a missing function - Error in function(arguments) : could not find function \"func\". Either you are calling a function from a library you have not loaded, or you mis-typed the function. If the library was not loaded, go back to the top and load the library that this function comes from. If you mis-typed the function (for example, /selcet), fix the typographical error. The error message will tell you which function seems to be missing. Error message about a missing object - Error: object 'xxxobj' not found. Make sure that you have typed the names of the dataframe and the variables correctly. Each of these is a data object. The error message will tell you which object seems to be missing. ## Your Turn to Debug a Plot Before hashtag-debugging the plot below, we will cap the plot code with an additional final line, by adding a + to the final line theme_minimal() and a new following line: NULL. This functions like return does for pipes - we can now use hashtags to turn off lines without causing errors because of missing pipes. Go ahead and add the + and NULL to the plot in the code chunk below. ## Error in theme_mnimal(): could not find function &quot;theme_mnimal&quot; Now use the same hashtag approach to run the lines of the plot code sequentially, adding one line at a time, with a hashtag placed just before the pipe, and using the Cmd-Shift-Return key combination. When you hit an error, use the hashtag at the beginning of each line that is a suspect in this Bug Hunt, and turn off one line at a time until you isolate the Bug. Then try to fix it, and run the whole plot chunk one line at a time. 7.5 Hashtag Debugging The elements of Hashtag Debugging of a code chunk are simply stated in 7 steps: Add a placeholder line to the end of the code chunk %&gt;% return() for data pipes + NULL for plots Insert a hashtag just before the line extender (%&gt;% for datapipes, + for plots) near the top of the code chunk Use Cmd-Shift-Return or Ctrl-Shift-Enter to run the code up to the hashtag. Keep running more lines by moving the hashtag down one line until you hit an error. Try to find and fix the error in that line. If needed, put a hashtag at the beginning of a line to turn off that line and run the rest of the pipe. If the error IS resolved (or changes), you have fixed the first error. Now run incrementally more lines (steps 2-4) until you have found a new error or completely fixed the pipe. Use your new skills. Figure out which Star Wars characters are overweight (by human standards) Try debugging the data pipe below. Work through each step in the process. 7.6 Pipe 2 starwars %&gt;% filter(height &lt;180) %&gt;% select(name, height, mass, gender homeworld, species) %&gt;% mtate(bmi = mass^2/height) %&gt;% filter(bmi&gt;=25) %&lt;% select(name, height, mass, bmi) %&gt;% arrange(dsc(bmi)) %&gt;% slice(1:15) ## Error: &lt;text&gt;:3:37: unexpected symbol ## 2: filter(height &lt;180) %&gt;% ## 3: select(name, height, mass, gender homeworld ## ^ Good work! Now try debugging the problematic plotting code chunk below. There are multiple errors. Work through each step in the process. 7.7 Plot 2 murders %&gt;% ggplot(x = population/10^6, y = total, label = abb) + geom_abline(intercept = log10(r), lty=2, col=darkgrey) + geom_point(aes(color==region), size = 3) + geom_text_repel() + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) ## Error in layer(data = data, mapping = mapping, stat = StatIdentity, geom = GeomAbline, : object &#39;darkgrey&#39; not found ggtitle(&quot;US Gun Murders in 2010&quot;) + scale_color_discrete(name=&quot;Region&quot;) ## NULL Here is another complex plot. Work through each step to completely debug this one. When it works, it will make a heatmap of measles cases in the US by month and year, with the introduction of the measles vaccine marked as an important event. 7.8 Plot3 us_contagious_diseases %&gt;% filter(!state%in%c(&quot;Hawaii&quot;,&quot;Alaska&quot;) &amp; disease == the_disease) %&gt;% mutate(rate = count / population * 10000 * 52 / weeks_reporting) %&gt;% mutate(state = reorder(state, rate) %&gt;% ggplot(aes(year state, fill = rate)) %&gt;% geom_tile(color = &quot;grey50&quot;) + scale_x_cntinuous(expand=c(0,0)) + scale_fill_gradientn(colors = brewer.pal(9, &quot;Reds&quot;), trans = &quot;sqrt&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) + theme_minimal() + theme(panel.grid = element_blank()) + ggttle(the_disease) + ylab(&quot;&quot;) + xlab(&quot;&quot;) ## Error: &lt;text&gt;:5:19: unexpected symbol ## 4: mutate(state = reorder(state, rate) %&gt;% ## 5: ggplot(aes(year state ## ^ Now let’s try a final data pipe debugging. Use your skills to make this one work. 7.9 Pipe 3 gapminder %&gt;% filter(year == 1965) %&gt;% filter(!is.na(infant_mortality) %&gt;% mutate(adult_survival = life_expectancy/infant_mortality) %&gt;% select(country, adultsurvival, continent) %&gt;% group_by(continent) %&gt;% summarize(mean_adult_surv = mean(adult_survival), sd_adult_surv = stdev(adult_survival)) %&gt;% arrange(mean_adult_surv) ## Error: &lt;text&gt;:11:0: unexpected end of input ## 9: arrange(mean_adult_surv) ## 10: ## ^ "],["finding-help-in-r.html", "Chapter 8 Finding Help in R 8.1 Programming in R 8.2 Starting with Help! 8.3 The Magic of Vignettes 8.4 Googling the Error Message 8.5 You Know What You Want to Do, but Don’t Know What Package or Function to Use 8.6 Seeking Advanced Help with a Minimal REPREX", " Chapter 8 Finding Help in R Because the path to something valuable is never easy. Learning R programming is hard, and at times will be frustrating. You will need help frequently. But the journey is a lot easier if you learn how to ask the R community for help when you are stuck. There is a very good chance that someone else has been stuck in the same place, and that there is good advice available. 8.1 Programming in R Saving programs and data in R is critical to producing reproducible medical research. But for most people, coding is not easy, comes with lots of syntax errors and cryptic error messages, and can be frustrating. One of the key skills in programming in R is finding help when you are stuck. In this chapter, we will explain several ways to find help in R, moving from the simple to the more complex. 8.2 Starting with Help! The simplest approach to getting help in R is to use the help() function. In the console, you can type help(“lm”) or help(“geom_boxplot”) or help(“filter”) to make the reference materials appear in the Help tab in the lower right quadrant of RStudio. Note that there may be more than one match - in which case it will show you a list. The search for help(“filter”) is a good example, as many packages have function that includes ‘filter’ in the name. In the code block below, use help(‘filter’) to find the details on the filter function from the dplyr package. help() You can also get help by going directly to the Help tab, and entering a term in the search box (top right of the Help window, with a magnifying glass icon), and pressing return. Help will take you directly to the documentation of a package or a function, which includes Description Usage (generic function with argument defaults) Arguments - explanation of each argument Details provided by the package author Examples (which can sometimes be cryptic) This is useful if you are just trying to remember the arguments to a function and/or their defaults, but is often not terribly helpful for beginners trying to understand how to use a package or a particular function. Let’s explore the dplyr::filter documentation a bit. There are 3 arguments listed, though … is a bit cryptic. The first argument is the .data, which is often piped in. The … argument lets you insert a variety of logical statements to filter by, and the .preserve argument defaults to recalculating grouping structure after filtering (default value is FALSE). The Details section recommends filtering before grouping for better speed (because of that recalculation of grouping structure). There are details on how grouping and rownames are affected by filter, followed by a mention of 3 variants of filter, filter_all, filter_if, and filter_at, which allow you to work on selections of variables. This is followed by some examples of the use of filter. 8.3 The Magic of Vignettes While the function documentation that you can find with help() explains the nuts and bolts (and arguments) for a function, it does not tell you much about the intended use, or the structure of the data that you should use this function on. Wrangling data into the right structure is often critical to successfully using a function. This makes package vignettes very helpful. The browseVignettes(‘package_name’) function can help you find the available vignettes for a given package. Edit the code block below to browse vignettes for the tidyr package. Remember to use quotes. this will open a webpage list to read the vignette and see examples of the use of tidyr. browseVignettes() ## starting httpd help server ... done Another useful approach is to search on the web for vignettes. You can Google “flextable vignettes” fo find examples of how to use the flextable package. Jump to a web browser and try it out. You will see nice explanations, and examples of code that you can copy and paste back into RStudio and run - either in the Console (interactively), or in a script. Try out a few of the layout and formatting examples with flextable. Often when you are first using a package, the vignette is the best place to start to get oriented to the intended use of the package and its functions. Several newer packages have dedicated documentation websites make with the pkgdown package. You can search for these by Googling packagename and “tidyverse”\" for packages in the tidyverse. Google the package website for the forcats package. Read the Home text, then check out the Reference page (tab) for each function. The Articles or “Get Started” tab often contains examples. A general strategy is to Google “ package in R”. Try googling a few others, like ggpubr RVerbalExpressions sf quanteda 8.4 Googling the Error Message It is very common to get an obscure error message. These are intended to be helpful, but often are not. A few common error messages and their usual causes are: Error Message Common Causes “could not find function x” package not installed, or function misspelled “subscript out of bounds” trying to find the 15th item in a vector when there are only 12 “error in if” an if statement trying to deal with non-logical data or NAs. “cannot open” trying to read a file that can’t be found “object x not found” using an argument that needs quotes without the quotes. If there are no quotes, R assumes that you are looking for an object already defined in the Environment tab. An R error message cheat sheet can be found [here] (http://varianceexplained.org/courses/errors/) Over time, you will learn to recognize common errors. But until you do (and even after you do), a helpful way out of a frustrating error message is to copy the error message, and paste it into a Google search. Some one has had that error before and asked for help on the internet. You can learn from their experience, and see what solutions other folks have come up with. Run the code block below to generate an error, then google the error message and see if you can figure out how to fix it. ## Error: stat_smooth requires the following missing aesthetics: x and y The problem is that the ggplot function does not include the aesthetics layer around x and y - aes(x, y) is required inside the ggplot() function to tell ggplot that the variables time and conc should be mapped to x and y. 8.5 You Know What You Want to Do, but Don’t Know What Package or Function to Use 8.5.1 CRAN Task Views There are two general approaches to this problem. If you know a general topic area, and are looking for packages, the CRAN Task Views can be really helpful in finding a package that does what you need. CRAN Task Views are lists of packages that do useful things in a certain topic area. Pick your task, and it will supply a useful list of likely packages, with a description of what the package does and a link to the documentation. Look for packages to help you block randomize patients into clinical trials on CRAN [here] (https://cran.r-project.org/web/views/). The ClinicalTrials link will take you to an extensive list of packages that help with a variety of needs for clinical trials. You will fairly quickly find the blockrand package is one that suits your needs, though there are a few other options available. 8.5.2 Google is Your Friend Try googling “How to do” task “in R”. Try “How do do block randomization in R”. Several options come up, including the blockrand package. Try another one - google “how to put significance bars in a ggplot in R”. Several options come up, including ggsignif and ggpubr. 8.6 Seeking Advanced Help with a Minimal REPREX There is a large R community, and many experienced people are willing to help you when you are stuck. However, it can be very difficult to accurately explain your problem to someone who is not at your computer. This problem has led to the concept of the minimal REProducible EXample (minimal REPREX), and the reprex package. The reprex package helps you post a useful example on websites like the RStudio Community or Stack Overflow to ask for help. A minimal reproducible example includes: 1. All of the libraries needed 2. a small (‘toy’) dataset, with no extra columns (just the ones needed), and a limited number of rows (often 5-6). 3. Your code, which is not quite working, or producing a surprising result 4. A clear explanation of the result you are trying to get with your code (sometimes this is a jpeg of a graph, or a table of what you want the data to look like after processing). There is a nice explanation of how to reprex for beginners here. More resources and details can be found in the RStudio Community FAQ here Before we start making a reprex of our own, let’s look at a few examples on RStudio Community. https://community.rstudio.com/t/could-not-plot-geometric-point/42558 https://community.rstudio.com/t/how-to-subset-a-data-frame-by-a-rowvalue/43514/8 https://community.rstudio.com/t/pivot-wider-tidyselect-and-col-how-to-exclude-variables/41191 https://community.rstudio.com/t/new-to-r-would-like-to-find-a-way-to-find-the-mean-of-each-states/39161 Now you have a feel for what a reprex looks like, and how folks ask and answer questions on RStudio Community. So let’s imagine that you are trying to plot data on blood pressure for men and women, and you want to color the points differently for men and women. But you don’t know how. Let’s start with the 4 steps to a reprex. In a new script you need to: include all libraries needed. In this case, library(tidyverse) covers any data wrangling and ggplot2 Include your data. This should be a minimal or ‘toy’ dataset. Be SURE you are not including any fields that are Protected Health Information (PHI) or identifiers. You can use a built-in dataset (https://www.rdocumentation.org/packages/datasets/versions/3.6.1) and select() a few key variables and filter() down to a reasonable number of rows (or use head() to get 6 rows), or take your own data and select only the columns needed and use filter() or head() for a minimal number of rows. Make sure not to use any Protected Health Information (PHI).Then use dput() to add the data to the reprex and assign it to an object build a toy dataset from scratch with data.frame Use the datapasta package to copy in some data from a website or spreadsheet 8.6.0.1 Built in datasets In the code chunk below, examine the built-in dataset, infert. Then select only education, age, and parity. Then use head() to get only 6 rows. Assign this to data and print it out glimpse(datasets::infert) ## Rows: 248 ## Columns: 8 ## $ education &lt;fct&gt; 0-5yrs, 0-5yrs, 0-5yrs, 0-5yrs, 6-1… ## $ age &lt;dbl&gt; 26, 42, 39, 34, 35, 36, 23, 32, 21,… ## $ parity &lt;dbl&gt; 6, 1, 6, 4, 3, 4, 1, 2, 1, 2, 2, 4,… ## $ induced &lt;dbl&gt; 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2,… ## $ case &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ spontaneous &lt;dbl&gt; 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,… ## $ stratum &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, … ## $ pooled.stratum &lt;dbl&gt; 3, 1, 4, 2, 32, 36, 6, 22, 5, 19, 2… glimpse(datasets::infert) ## Rows: 248 ## Columns: 8 ## $ education &lt;fct&gt; 0-5yrs, 0-5yrs, 0-5yrs, 0-5yrs, 6-1… ## $ age &lt;dbl&gt; 26, 42, 39, 34, 35, 36, 23, 32, 21,… ## $ parity &lt;dbl&gt; 6, 1, 6, 4, 3, 4, 1, 2, 1, 2, 2, 4,… ## $ induced &lt;dbl&gt; 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2,… ## $ case &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ spontaneous &lt;dbl&gt; 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,… ## $ stratum &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, … ## $ pooled.stratum &lt;dbl&gt; 3, 1, 4, 2, 32, 36, 6, 22, 5, 19, 2… data &lt;- infert %&gt;% select(education, age, parity) %&gt;% head() data ## education age parity ## 1 0-5yrs 26 6 ## 2 0-5yrs 42 1 ## 3 0-5yrs 39 6 ## 4 0-5yrs 34 4 ## 5 6-11yrs 35 3 ## 6 6-11yrs 36 4 8.6.0.2 Filtering your dataframe object In the code chunk below, examine the local dataset, emerg_dept, which has counts of ED arrivals, how many breached the UK 4 hour guarantee, and how many got admitted. Then select() only org_code, attendances, breaches, and admissions. Then arrange() to have the top attendances at the top, use top_n(10) to get only the top 10 rows. Assign this to data and print it out emerg_dept ## # A tibble: 50 x 6 ## period org_code type attendances breaches admissions ## &lt;date&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018-07-01 RRK 1 32209 6499 11332 ## 2 2018-07-01 R1H 1 28357 6294 7986 ## 3 2018-07-01 RW6 1 23887 4641 6282 ## 4 2018-07-01 R0A 1 22012 4669 6818 ## 5 2018-07-01 RDU 1 21043 1941 6519 ## 6 2018-07-01 RAL 1 20481 2529 4530 ## 7 2018-07-01 RF4 1 19303 4606 5004 ## 8 2018-07-01 RWE 1 18890 4861 4522 ## 9 2018-07-01 RXF 1 18828 2731 3981 ## 10 2018-07-01 RQM 1 18560 1064 4130 ## # … with 40 more rows emerg_dept %&gt;% select(org_code, attendances:admissions) %&gt;% arrange(desc(attendances)) %&gt;% top_n(10) -&gt; data ## Selecting by admissions data ## # A tibble: 10 x 4 ## org_code attendances breaches admissions ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 RRK 32209 6499 11332 ## 2 R1H 28357 6294 7986 ## 3 RW6 23887 4641 6282 ## 4 R0A 22012 4669 6818 ## 5 RDU 21043 1941 6519 ## 6 RF4 19303 4606 5004 ## 7 RR8 17889 3507 5345 ## 8 RTG 17591 2757 5302 ## 9 RJE 16622 3758 5855 ## 10 R1K 11922 3038 6098 8.6.0.3 Toy datasets Run the code below to build a toy dataset with patient_id, sbp, dbp. Then edit the code to add 4 values for heart rate and 4 values for respiratory rate df &lt;- data.frame( patient_id = 1:4, sbp = c(151, 137, 129, 144), dbp = c(92, 85, 79, 66) ) df ## patient_id sbp dbp ## 1 1 151 92 ## 2 2 137 85 ## 3 3 129 79 ## 4 4 144 66 8.6.0.4 Fun with Datapasta Example datapasta is a package for pasting data. It is super-helpful when you just want to quickly get a bit of data from a website or a spreadsheet. Go to the website, https://en.wikipedia.org/wiki/Health_insurance_coverage_in_the_United_States, and find the large table named, “Percent uninsured (all persons) by state, 1999–2014”. Carefully copy the table without the title line. Then use the Addins dropdown to “Paste as Tribble” into your code file. Assign the resulting tibble to an object named ins_data. You will get funny names for the columns. Change these with names(ins_data), and assign state and 1999:2014 to the names. Filter to get rid of DC and United states. You should end up with 50 rows. ins_data &lt;- tibble::tribble( ~V1, ~V2, ~V3, ~V4, ~V5, ~V6, ~V7, ~V8, ~V9, ~V10, ~V11, ~V12, ~V13, ~V14, ~V15, ~V16, ~V17, &quot;Division&quot;, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, &quot;Alabama&quot;, 12, 12.5, 12.4, 12.2, 12.5, 12, 14, 15.1, 11.7, 11.5, 16.4, 15.5, 13, 14.8, 13.6, 12.1, &quot;Alaska&quot;, 18.3, 17.4, 14.8, 18, 17.5, 15.3, 16.9, 16.4, 17.6, 19.6, 17.2, 18.1, 18.2, 19, 18.5, 17.2, &quot;Arizona&quot;, 19.4, 16.4, 16.7, 16.4, 16.4, 16.2, 19.1, 20.8, 17.8, 19.1, 18.9, 19.1, 17.3, 18, 17.1, 13.6, &quot;Arkansas&quot;, 13.9, 14.1, 16.4, 16.5, 17.2, 15.9, 17.2, 18.6, 15.7, 17.6, 19, 18.5, 17.5, 18.4, 16, 11.8, &quot;California&quot;, 19, 17.5, 18, 16.5, 17.3, 17.5, 18, 17.8, 17.5, 18.1, 19.3, 19.4, 19.7, 17.9, 17.2, 12.4, &quot;Colorado&quot;, 14.1, 12.9, 14.6, 14.5, 15.3, 15.2, 16.2, 16.5, 16, 15.4, 14.5, 12.9, 15.7, 13.7, 14.1, 10.3, &quot;Connecticut&quot;, 7.3, 8.9, 8.2, 8.6, 9.4, 10.3, 10.1, 8.7, 8.6, 9.4, 11.1, 11.2, 8.6, 8.1, 9.4, 6.9, &quot;Delaware&quot;, 9.7, 8.5, 8.5, 9.2, 9.6, 13.1, 11.6, 11.9, 10.6, 10.7, 13, 11.3, 10, 10.8, 9.1, 7.8, &quot;District of Columbia&quot;, 14, 12.8, 12.3, 13, 12.7, 12, 12.4, 10.9, 9.3, 9.4, 12.4, 12.8, 8.4, 7.9, 6.7, 5.3, &quot;Florida&quot;, 17.4, 16.2, 16.9, 15.6, 17, 18.3, 19.5, 20.3, 19.8, 19.4, 21.7, 20.7, 19.8, 21.5, 20, 16.6, &quot;Georgia&quot;, 14.2, 13.9, 14.7, 14.6, 15.2, 15.7, 17.9, 17.3, 17.2, 17.1, 20.5, 19.5, 19.2, 19.2, 18.8, 15.8, &quot;Hawaii&quot;, 9.2, 7.9, 8.2, 8.8, 8.6, 8.5, 8.1, 7.9, 6.9, 7.3, 7.4, 7.7, 7.8, 7.7, 6.7, 5.3, &quot;Idaho&quot;, 18.2, 15.4, 15.7, 16.9, 17.7, 14.5, 14.4, 15.1, 13.6, 15.4, 15.1, 19.1, 16.9, 15.9, 16.2, 13.6, &quot;Illinois&quot;, 11.9, 12, 11.8, 12.4, 13.8, 12.5, 13.2, 13.5, 13, 12.2, 14.2, 14.8, 14.7, 13.6, 12.7, 9.7, &quot;Indiana&quot;, 8.9, 10.1, 10.1, 11.5, 12.2, 12.4, 13.1, 11.3, 11, 11.3, 13.7, 13.4, 12, 13.4, 14, 11.9, &quot;Iowa&quot;, 7.8, 8.1, 6.8, 9, 10.4, 8.8, 8.1, 9.9, 8.8, 9, 10.8, 12.2, 10, 10.1, 8.1, 6.2, &quot;Kansas&quot;, 11.2, 9.6, 9.8, 9.4, 10.1, 10.6, 10, 12.1, 12.4, 11.8, 12.8, 12.6, 13.5, 12.6, 12.3, 10.2, &quot;Kentucky&quot;, 12.9, 12.7, 11.6, 12.7, 13.7, 13.9, 11.7, 15.2, 13.4, 15.7, 15.9, 14.8, 14.4, 15.7, 14.3, 8.5, &quot;Louisiana&quot;, 20.9, 16.8, 17.8, 17.2, 19, 17, 16.9, 21.1, 18, 19.5, 14.5, 19.8, 20.8, 18.3, 16.6, 8.5, &quot;Maine&quot;, 9.2, 10.4, 10.2, 10.4, 9.6, 9.3, 9.8, 8.9, 8.5, 10.2, 10, 9.3, 10, 9.5, 11.2, 10.1, &quot;Maryland&quot;, 10, 9, 11, 11.7, 12.2, 11.9, 13.1, 13.2, 12.7, 11.4, 13.3, 12.8, 13.8, 12.4, 10.2, 7.9, &quot;Massachusetts&quot;, 7.8, 7.1, 6.9, 9.5, 10.1, 9.8, 8.6, 9.6, 4.9, 5, 4.3, 5.5, 3.4, 4.1, 3.7, 3.3, &quot;Michigan&quot;, 9, 7.8, 9, 9.8, 9.3, 10.2, 9.5, 10.1, 10.8, 11.5, 13, 13, 12.5, 10.9, 11, 8.5, &quot;Minnesota&quot;, 6.6, 8, 6.9, 7.9, 8.7, 8.3, 7.6, 8.9, 8, 8.2, 8, 9.7, 9.2, 8.3, 8.2, 5.9, &quot;Mississippi&quot;, 15.7, 13.2, 17, 16.2, 17.5, 16.9, 16.5, 20.3, 18.4, 17.7, 17.3, 21, 16.2, 15.3, 17.1, 14.5, &quot;Missouri&quot;, 6.6, 8.6, 9.7, 10.8, 9.9, 11, 11.4, 13.1, 12.2, 12.4, 14.6, 13.9, 14.9, 13.3, 13, 11.7, &quot;Montana&quot;, 17.3, 16.1, 13.8, 14.3, 18.9, 17.5, 15.5, 16.9, 15, 15.7, 15.1, 18.2, 18.3, 18.1, 16.5, 14.2, &quot;Nebraska&quot;, 9, 7.9, 7.9, 9.3, 10.1, 10.3, 9.8, 12, 13, 11.1, 11.1, 13.2, 12.3, 13.3, 11.3, 9.7, &quot;Nevada&quot;, 18.3, 15.7, 14.5, 18.4, 17.6, 18.2, 16.5, 18.6, 16.9, 18.1, 20.6, 21.4, 22.6, 23.5, 20.7, 15.2, &quot;New Hampshire&quot;, 7.7, 7.9, 9.7, 8.8, 9.3, 8.7, 9.1, 10.8, 9.9, 10.1, 9.8, 10.1, 12.5, 12, 10.7, 9.2, &quot;New Jersey&quot;, 11.1, 10.2, 11.6, 12, 12.8, 12.6, 13.7, 14.8, 14.6, 13.2, 14.5, 15.6, 15.4, 14, 13.2, 10.9, &quot;New Mexico&quot;, 24, 23, 19.6, 20, 21.3, 19.3, 20.2, 22.7, 21.8, 22.8, 20.9, 21.4, 19.6, 21.9, 18.6, 14.5, &quot;New York&quot;, 14.4, 14.5, 13.9, 14, 14.3, 11.8, 12.1, 13.4, 12.3, 13.4, 14.1, 15.1, 12.2, 11.3, 10.7, 8.7, &quot;North Carolina&quot;, 12.5, 12.1, 13.3, 15.9, 16.7, 14.2, 14.5, 17.4, 16.2, 15.1, 17.8, 17.1, 16.3, 17.2, 15.6, 13.1, &quot;North Dakota&quot;, 10.2, 9.8, 8, 9.7, 10.3, 10, 10.8, 11.8, 9.5, 11.6, 10.3, 13.4, 9.1, 11.5, 10.4, 7.9, &quot;Ohio&quot;, 9.9, 9.8, 9.9, 10.4, 11.1, 10.3, 11, 9.6, 11.1, 11.2, 13.8, 13.6, 13.7, 12.3, 11, 8.4, &quot;Oklahoma&quot;, 15.4, 17.4, 17.2, 16.7, 19.1, 18.7, 17.7, 18.8, 17.6, 13.8, 17.9, 17.3, 16.9, 17.2, 17.7, 15.4, &quot;Oregon&quot;, 14.2, 11.6, 12.7, 14.3, 16, 15.4, 15.3, 17.5, 16.2, 15.9, 17.3, 16, 13.8, 15.4, 14.7, 9.7, &quot;Pennsylvania&quot;, 7.8, 7.6, 8.4, 10.2, 10, 10.1, 9.3, 9.4, 9.1, 9.6, 10.9, 10.9, 10.8, 12, 9.7, 8.5, &quot;Rhode Island&quot;, 5.9, 6.9, 7.7, 8.1, 10.4, 10, 10.7, 8.1, 10.5, 11, 12, 11.5, 12, 12.3, 11.6, 7.4, &quot;South Carolina&quot;, 14.8, 10.7, 11.1, 11.1, 13.1, 14.9, 16.3, 15.3, 15.9, 15.5, 16.8, 20.5, 19, 14.3, 15.8, 13.6, &quot;South Dakota&quot;, 10.1, 10.8, 8.3, 10.8, 10.6, 11, 11.5, 11.5, 9.9, 12.2, 13.1, 13.1, 13, 14.4, 11.3, 9.8, &quot;Tennessee&quot;, 9.3, 10.7, 10.1, 9.8, 12.2, 12.4, 13.4, 13.2, 14, 14.5, 15, 14.6, 13.3, 13.9, 13.9, 12, &quot;Texas&quot;, 21.1, 22, 22.4, 24.5, 23.6, 23.6, 22.9, 23.9, 24.7, 24.5, 25.5, 24.6, 23.8, 24.6, 22.1, 19.1, &quot;United States&quot;, 13.6, 13.1, 13.5, 13.9, 14.6, 14.3, 14.6, 15.2, 14.7, 14.9, 16.1, 16.3, 15.7, 15.4, 14.5, 11.7, &quot;Utah&quot;, 11.9, 10.8, 13.8, 12.1, 11.5, 12.8, 15.5, 16.7, 12.2, 12, 14.1, 13.8, 14.6, 14.4, 14, 12.5, &quot;Vermont&quot;, 10.1, 7.4, 8.8, 8.9, 8.4, 9.8, 11.2, 9.8, 10.1, 9.3, 9.4, 9.3, 8.6, 7, 7.2, 5, &quot;Virginia&quot;, 11.3, 9.6, 9.8, 11.8, 11.5, 13, 12.3, 12.5, 14.2, 11.8, 12.6, 14, 13.4, 12.5, 12.3, 10.9, &quot;Washington&quot;, 12.2, 13.1, 13.3, 12.3, 14.8, 12.5, 12.5, 11.5, 11, 12, 12.6, 13.9, 14.5, 13.6, 14, 9.2, &quot;West Virginia&quot;, 14.9, 13.4, 12.9, 13.8, 16.8, 15.7, 16.5, 13.3, 13.7, 14.5, 13.7, 13.4, 14.9, 14.6, 14, 8.6, &quot;Wisconsin&quot;, 9.7, 7.1, 7.3, 8.6, 9.8, 10.3, 8.8, 8, 8, 9.2, 8.9, 9.4, 10.4, 9.7, 9.1, 7.3, &quot;Wyoming&quot;, 14.5, 14.7, 14.1, 14.8, 14.8, 12.3, 14.4, 14.2, 13.2, 13.3, 15.4, 17.2, 17.8, 15.4, 13.4, 12 ) names(ins_data) &lt;- c(&quot;state&quot;, c(1999:2014)) ins_data %&gt;% slice(2:45) %&gt;% bind_rows(slice(ins_data, 47:53)) %&gt;% filter(state != &quot;District of Columbia&quot;) -&gt; ins_data include your minimal code - just enough to reproduce the problem, and no more. Now you need to - run this minimal code in a new script window to make sure it reproduces the problem and gets the same error. - Install the reprex package #install.packages(&#39;reprex&#39;) Select all of the code in your new script window, including libraries, data, and code copy this with Ctrl-C (Windows) or Cmd-C (Mac) go to the Console, type in “reprex()” and enter your REPREX will be generated and will show up in your Viewer tab. This is now on your Clipboard. Go to RStudio Community and start a new topic. Type in an introduction to your problem, state clearly what you are trying to do, and where you are stuck. Paste in the reprex. Thank people in advance for their help. Post this topic. Wait for helpful answers. "],["the-basics-of-base-r.html", "Chapter 9 The Basics of Base R 9.1 Dimensions of Data Rectangles 9.2 Naming columns 9.3 Concatenation 9.4 Sequences 9.5 Constants 9.6 Fancier Sequences 9.7 Mathematical functions 9.8 Handling missing data (NAs) 9.9 Cutting Continuous data into Levels", " Chapter 9 The Basics of Base R While there are many great features of the tidyverse, one should not throw out the base R with the bathwater. The functions and packages of base R are stable and slow to change (unlike the dynamic packages and functions of the tidyverse), and many are helpful and important building blocks for using R. Some of the functions in base R tend to fail silently, and have unhelpful error messages, but they are embedded in a lot of R scripts. When you search for help with R on websites like RStudio Community and Stack Overflow, you will often find base R code, and you will need to know how to interpret it. There are many really basic and important functions in base R that are worth knowing about. Once you have a handle on these basic functions, you can say Obscure base R / video game meme meme details 9.1 Dimensions of Data Rectangles Whether you have a data.frame, a tibble, or a matrix, it can be helpful to know the dimensions of what you have. You can get at these dimensions with dim() nrow() ncol() You may want to know how many rows to loop over, or how many columns need names, but you will frequently need to access these numbers. The dim() function returns two numbers - the rows first, then the columns. Let’s try this on the licorice dataset from the {medicaldata} package. dim(licorice) ## [1] 235 19 This is great, as long as you know that the first number is the number of rows, and the 2nd number is the number of columns (standard notation is R x C, so rows first, columns second). But if you want to get the number of rows out, and store it in a variable, you need to use the brackets [n] notation. Brackets allow you to pull out the nth item in a vector or a list. Let’s pull out the first item (the number of rows), and the second item (the number of columns) separately. We will store these in the 2 variables rows and columns, then print them out. rows &lt;- dim(licorice)[1] rows ## [1] 235 columns &lt;- dim(licorice)[2] columns ## [1] 19 You can also do this more directly with the nrow() and ncol() functions. rows &lt;- nrow(licorice) rows ## [1] 235 columns &lt;- ncol(licorice) columns ## [1] 19 A similar approach can give you the length of a vector with the length() function. Here we will check the length of the treat vector in the licorice tibble. length(licorice$treat) ## [1] 235 The length() function works a bit differently on dataframes or tibbles - it returns the number of variables/columns. This can be surprising if you don’t expect it, and you are expecting the number of rows. length(licorice) ## [1] 19 9.2 Naming columns Sometimes you want to take a quick look at the names of all of you columns in a dataframe. The names() function is a quick solution. names(licorice) ## [1] &quot;preOp_gender&quot; &quot;preOp_asa&quot; ## [3] &quot;preOp_calcBMI&quot; &quot;preOp_age&quot; ## [5] &quot;preOp_mallampati&quot; &quot;preOp_smoking&quot; ## [7] &quot;preOp_pain&quot; &quot;treat&quot; ## [9] &quot;intraOp_surgerySize&quot; &quot;extubation_cough&quot; ## [11] &quot;pacu30min_cough&quot; &quot;pacu30min_throatPain&quot; ## [13] &quot;pacu30min_swallowPain&quot; &quot;pacu90min_cough&quot; ## [15] &quot;pacu90min_throatPain&quot; &quot;postOp4hour_cough&quot; ## [17] &quot;postOp4hour_throatPain&quot; &quot;pod1am_cough&quot; ## [19] &quot;pod1am_throatPain&quot; You can also use names() to re-set the names if you want to change a bunch of column names, by assigning a vector of names (of the same length). names(licorice) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;, &quot;K&quot;, &quot;L&quot;, &quot;M&quot;, &quot;N&quot;, &quot;O&quot;, &quot;P&quot;, &quot;Q&quot;, &quot;R&quot;, &quot;S&quot;) licorice[1:10, ] ## A B C D E F G H I J K L M N O P Q R S ## 1 0 3 32.98 67 2 1 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 2 0 2 23.66 76 2 2 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 3 0 2 26.83 58 2 1 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 4 0 2 28.39 59 2 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 5 0 1 30.45 73 1 2 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 6 0 2 35.49 61 3 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 7 0 3 25.50 66 1 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 8 0 2 31.10 61 2 1 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 9 0 3 21.22 83 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 10 0 3 27.16 69 2 3 0 1 2 0 0 0 0 0 0 0 0 0 0 Note that you can use the set_names() function in the {purrr} package to conveniently change variable/column names within a data pipeline, and the rename() function in the dplyr package to change particular variable/column names. licorice %&gt;% purrr::set_names(1:19) %&gt;% dplyr::rename(&quot;purple&quot; = 2) %&gt;% # note rename(new_name = old_name) tibble() ## # A tibble: 235 x 19 ## `1` purple `3` `4` `5` `6` `7` `8` `9` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 3 33.0 67 2 1 0 1 2 ## 2 0 2 23.7 76 2 2 0 1 1 ## 3 0 2 26.8 58 2 1 0 1 2 ## 4 0 2 28.4 59 2 1 0 1 3 ## 5 0 1 30.4 73 1 2 0 1 2 ## 6 0 2 35.5 61 3 1 0 1 3 ## 7 0 3 25.5 66 1 1 0 1 3 ## 8 0 2 31.1 61 2 1 0 1 1 ## 9 0 3 21.2 83 1 1 0 1 1 ## 10 0 3 27.2 69 2 3 0 1 2 ## # … with 225 more rows, and 10 more variables: 10 &lt;dbl&gt;, ## # 11 &lt;dbl&gt;, 12 &lt;dbl&gt;, 13 &lt;dbl&gt;, 14 &lt;dbl&gt;, 15 &lt;dbl&gt;, ## # 16 &lt;dbl&gt;, 17 &lt;dbl&gt;, 18 &lt;dbl&gt;, 19 &lt;dbl&gt; licorice[1:10, ] ## A B C D E F G H I J K L M N O P Q R S ## 1 0 3 32.98 67 2 1 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 2 0 2 23.66 76 2 2 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 3 0 2 26.83 58 2 1 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 4 0 2 28.39 59 2 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 5 0 1 30.45 73 1 2 0 1 2 0 0 0 0 0 0 0 0 0 0 ## 6 0 2 35.49 61 3 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 7 0 3 25.50 66 1 1 0 1 3 0 0 0 0 0 0 0 0 0 0 ## 8 0 2 31.10 61 2 1 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 9 0 3 21.22 83 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 ## 10 0 3 27.16 69 2 3 0 1 2 0 0 0 0 0 0 0 0 0 0 Note also that we used the bracket notation above to print just the first 10 rows of the renamed version of the licorice dataframe. This was done with brackets that define which [rows, columns] we want to use (in this case for printing). By using the sequence 1:10, we choose the first 10 rows. By putting nothing after the comma, we select all columns. You might be wondering why the column names reverted to alphabetical letters after we used set_names to change them to numbers. This is because we did set the names, and printed the result out to the console, but did not assign the result back to the licorice object with an assignment arrow, so it is transient, rather than a lasting change to the licorice object. This is a common pitfall for beginners. We can use also use brackets to choose exactly which rows and columns we want. licorice[4:7, c(2,5,10)] ## B E J ## 4 2 2 0 ## 5 1 1 0 ## 6 2 3 0 ## 7 3 1 0 Here we have selected 4 particular rows with a sequence (4:7), and 3 particular columns (by concatenating these into a vector with the c() function). 9.3 Concatenation One of the simplest, but most common early functions in R is c(). The c() function concatenates items together into a vector. This can be helpful for building a vector of items to iterate over, or to build a vector which will become a variable in a dataframe, or even a vector of options for a function. You simply write the items, separated by commas, in order inside the parentheses of c(). Remember that strings need to be enclosed in matching quotes. fib_numbers &lt;- c(1, 1, 2, 3, 5, 8, 13, 21, 34) fruit_vec &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;coconut&quot;, &quot;dragonfruit&quot;, &quot;elderberry&quot;) fib_numbers ## [1] 1 1 2 3 5 8 13 21 34 fruit_vec ## [1] &quot;apple&quot; &quot;banana&quot; &quot;coconut&quot; &quot;dragonfruit&quot; ## [5] &quot;elderberry&quot; 9.4 Sequences There are times when you want to create a sequence of numbers (i.e. 1 to 10, or 1 to 100), without manually concatenating a vector. The easiest way to do this is with the colon (:). You can assign 1:12 to an object, or 77:83, if you prefer. 1:12 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 77:83 ## [1] 77 78 79 80 81 82 83 9.5 Constants Note that base R has some handy constants that may help in making vectors - LETTERS (vector of letters (string) from A to Z) - letters (vector of letters (string) from a to z) - month.abb (vector of 3 letter (English) month abbreviations from Jan to Dec) - month.name (vector of 3 letter (English) month abbreviations from January to December) - pi (the irrational number for relating diameter to circumference) You can select subsets of these with the bracket notation, i.e letters[1:13]. You can also format number for printing as strings with sprintf() (for print formatting) to include the desired number of decimals. LETTERS[7:12] ## [1] &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; &quot;K&quot; &quot;L&quot; letters[5:10] ## [1] &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; month.abb[10:12] ## [1] &quot;Oct&quot; &quot;Nov&quot; &quot;Dec&quot; pi %&gt;% sprintf(fmt = &quot;%1.5f&quot;, .) ## [1] &quot;3.14159&quot; 9.6 Fancier Sequences You can make more complex sequences with the seq() function. The main arguments (parameters) of seq() are from (default =1) to (default =1) by (default = (to-from)/length) length You will generally need at least 3 of these to describe a sequence, or seq() will use the default by value of 1. Note that if the by increment does not match the to argument, the sequence will stop at the last number before the to number. seq_len(n) is a shortcut that gives you a sequence from 1 to n, while seq_along(vector) is a shortcut that gives you a sequence from 1 to the length of a the vector. See the examples below # leaving out &quot;length&quot; seq(from = 2, to = 18, by = 2) ## [1] 2 4 6 8 10 12 14 16 18 # leaving out argument names seq(3, 18, length=6) ## [1] 3 6 9 12 15 18 # &#39;length&#39; and &#39;to&#39; do not match seq(from = 24, to = 4, by = -6) ## [1] 24 18 12 6 # leaving out &quot;to&quot; seq(from = 5, by = 5, length = 6) ## [1] 5 10 15 20 25 30 # leaving out &quot;by&quot; seq(from = 16, to = 128, length = 8) ## [1] 16 32 48 64 80 96 112 128 seq(from = 51, by = -3, length = 17) ## [1] 51 48 45 42 39 36 33 30 27 24 21 18 15 12 9 6 3 # using the seq_len() shortcut with n seq_len(14) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # using the seq_along() shortcut with a vector seq_along(7:23) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 seq_along(licorice$C) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 ## [14] 14 15 16 17 18 19 20 21 22 23 24 25 26 ## [27] 27 28 29 30 31 32 33 34 35 36 37 38 39 ## [40] 40 41 42 43 44 45 46 47 48 49 50 51 52 ## [53] 53 54 55 56 57 58 59 60 61 62 63 64 65 ## [66] 66 67 68 69 70 71 72 73 74 75 76 77 78 ## [79] 79 80 81 82 83 84 85 86 87 88 89 90 91 ## [92] 92 93 94 95 96 97 98 99 100 101 102 103 104 ## [105] 105 106 107 108 109 110 111 112 113 114 115 116 117 ## [118] 118 119 120 121 122 123 124 125 126 127 128 129 130 ## [131] 131 132 133 134 135 136 137 138 139 140 141 142 143 ## [144] 144 145 146 147 148 149 150 151 152 153 154 155 156 ## [157] 157 158 159 160 161 162 163 164 165 166 167 168 169 ## [170] 170 171 172 173 174 175 176 177 178 179 180 181 182 ## [183] 183 184 185 186 187 188 189 190 191 192 193 194 195 ## [196] 196 197 198 199 200 201 202 203 204 205 206 207 208 ## [209] 209 210 211 212 213 214 215 216 217 218 219 220 221 ## [222] 222 223 224 225 226 227 228 229 230 231 232 233 234 ## [235] 235 9.7 Mathematical functions R has many mathematical functions, which can be used in a variety of calculations. These can be run on a vector, or on a variable in a dataframe. These include (and there are many more): mean median var sd min max range rank sum Examples are shown below mean(1:20) ## [1] 10.5 median(licorice$C) ## [1] 25.91 var(licorice$C) ## [1] 18.24933 sd(licorice$C) ## [1] 4.271923 min(licorice$C) ## [1] 15.6 max(licorice$C) ## [1] 36.33 range(licorice$C)[2] # selects 2nd value in range (max) ## [1] 36.33 rank(licorice$C)[1] # ranks first 10 values ## [1] 225 sum(licorice$C) # sum of values ## [1] 6013.99 9.8 Handling missing data (NAs) R designates missing values as the symbol NA (not available). NAs propagate through calculations, so that if you have a vector with at least one NA, and you try to calculate the mean, it will return NA. mean(licorice$J) ## [1] NA You can handle this within many functions (including mean, median, sd, and var) with the argument na.rm = TRUE. The default for these is na.rm = FALSE, so that if you are trying to do an operation on missing data, R will tell you. na.rm is an argument in a number of mathematical functions, in which na comes first, followed by the verb rm (remove). Testing whether a value or values are missing (NA) is in the reverse order. You use the is.na() function, in which the verb comes first, and then followed by NA. You might reasonably think that you can just use a normal equality test for NA values, like licorice$J == NA ## [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [19] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [37] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [55] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [73] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [91] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [109] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [127] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [145] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [163] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [181] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [199] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [217] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [235] NA but, because NAs propagate, you get just NAs, rather than TRUE or FALSE. You can use is.na() for this. licorice$J %&gt;% is.na() ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [10] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [19] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [28] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [46] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [55] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [64] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [82] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [91] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [100] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [109] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [118] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [127] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [136] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [154] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [163] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [172] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [190] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [199] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [208] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [226] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [235] FALSE There are a few TRUEs in there (value is NA), but they can be hard to count. The sum() function can help, when combined with is.na(). The which() function can help you figure out which values are missing. The anyNA() function can tell you if there are any missing values in a vector (or a variable/column in a dataframe). licorice$J %&gt;% is.na() %&gt;% sum() ## [1] 2 licorice$J %&gt;% is.na() %&gt;% which() ## [1] 113 123 licorice$C %&gt;% anyNA() ## [1] FALSE licorice$J %&gt;% anyNA() ## [1] TRUE There are two missing values, in rows 113 and 123. The na.omit() function can remove all of the rows(cases, observations) from a dataframe that have at least one missing value in any column. This can be helpful for modeling, in which cases with missing data can cause problems. It is helpful to keep track of the number of rows before and after na.omit(), to know how many cases/observations/rows you are discarding. nrow(licorice) ## [1] 235 licorice %&gt;% na.omit() %&gt;% # modeling would happen here if not too many cases discarded nrow() ## [1] 233 Note that this can also be done in the tidyverse with drop_na() in the {tidyr} package. You can include a particular column or columns as an argument in drop_na() to only drop observations if there are missing values in these particular columns. licorice %&gt;% drop_na(H:J) %&gt;% nrow() ## [1] 233 The code above takes the licorice dataset, looks for NA values in rows of (only) columns H through J, and drops 2 rows based on missing data, reducing the number of rows from 235 to 233. 9.9 Cutting Continuous data into Levels While there are good arguments for why not to do this (dichotomania, loss of granularity in data), it is common to cut continuous data into levels, like (mild, moderate, severe), or (normal weight, overweight, obese). This can, when there are already established standard levels, make the data easier to interpret. The cut() function in base R makes this easy to do. C_factor3 &lt;- cut(licorice$C, breaks = 3) table(C_factor3) ## C_factor3 ## (15.6,22.5] (22.5,29.4] (29.4,36.4] ## 59 130 46 str(C_factor3) ## Factor w/ 3 levels &quot;(15.6,22.5]&quot;,..: 3 2 2 2 3 3 2 3 1 2 ... This creates a new variable (C_factor), which is a factor with 3 levels. The levels are stored as 1, 2, 3, and range from 15.6-22.5 for 1, above 22.5-29.4 for 2, and above 29.4 to 36.4 for 3. The interval notation uses the square bracket for including the listed number, and parentheses for starting just after the listed number. It is a good practice to develop a standard way of naming these created variables, which are related to the original variable, are factors, and have a certain number of levels. One helpful shorthand is to take the original variable name, and to add the suffix “_f4\" for a factor with 4 levels. A dichotomized variable would be \"varname_f2” "],["updating-r-rstudio-and-your-packages.html", "Chapter 10 Updating R, RStudio, and Your Packages 10.1 Installing Packages 10.2 Loading Packages with Library 10.3 Updating R 10.4 Updating RStudio 10.5 Updating Your Packages", " Chapter 10 Updating R, RStudio, and Your Packages 10.1 Installing Packages The most important way to update R is to add packages. Each package adds new functions and/or data to R, enabling you to do much more in the R and RStudio environment. When you open R, or start a new session, you have only the base version of R available, and it is pretty spartan. You can see how many packages you have available to you by starting RStudio and going to the menu Session/New Session, or Session/Restart R. Each of these will give you a clean workspace to start in. Once you have started a new session, or restarted R, run the following code: print(.packages()) You will find that you only have 9 packages available, including base, utils, methods, stats, graphics, grDevices, datasets, devtools, and usethis. This is what is called “base R” and is essentially the bare minimum needed to use R. In order to use more of the power of R and RStudio, you will need to install packages (a one-time task), and load them (in each session) before use with a library(package_name) function. If you Google a bit for ways to do things in R, you will find many packages that can be helpful. The most strictly validated packages are hosted on CRAN - a mirrored server. There are now over 20,000 packages on CRAN to do various specialized things in R. These were all useful for someone, so they have shared them on CRAN. To install packages from CRAN, you use the function: install.packages(\"package_name\") Notice that the package_name has to be in quotes. These can be single or double quotes. The package_name and install.packages() are case_sensitive like all objects and functions in R, so that something like Install.Packages will not work. Once the package is installed, you keep that in your R library associated with your current major version of R. You will need to update &amp; reinstall packages each time you update a major version of R. R versions are designated with R version #.#.# A change in the third number indicates a minor version change. A change in the first or 2nd number (from R 3.6.2 to 4.0.0, or 4.0.2 to 4.1.0) is a major version upgrade which will require re-installation of packages. Let’s practice installing a package. Run the code below to install the tidyverse package. install.packages(&quot;tidyverse&quot;) 10.1.1 Installing Packages from Github Some packages are still in development. These are often in repositories on github, rather than on the CRAN servers. To install these packages, you need to know path to the repository. You can install the medicaldata package from Github. Run the code below to install this package. devtools::install_github(&quot;higgi13425/medicaldata&quot;) In contrast to install.packages, the library() function can work with quotes around the package_name, but they are not required. This is because these packages are already installed in your R library, and are known quantities. In general, known objects in your R environment do not require quotes, and novel things like packages do require quotes. If you re-run print(.packages) at this point, you will not have any more packages. This is because you have installed new packages, but not loaded them. 10.1.2 Problems with Installing Packages 10.1.2.1 R Version Issues Sometimes you may run into a problem installing a package which was developed for a previous version of R. Especially if you have recently upgraded your R version recently, the CRAN version of a package may be a bit behind. This can often be fixed by googling for “github” and “package_name”. This will usually lead you to the github repository for that package, which will have a pathname of “github_username/package_name”. Once you know this, you can use devtools::install_github('github_username/package_name') to install the newest version of the package, which will usually be compatible with the latest version of R. 10.1.2.2 Installing from Source vs Binaries 10.1.2.3 Dependencies Some packages are dependent on specific versions of other packages, and will ask you to update the other packages during installation. As a general rule, you should say ‘yes’ to all packages. If you are worried about over-writing an existing package in a way that would break your code in a different project, then that project needs its own project-specific library, which you can create with the {renv} package. 10.1.2.4 Extra-R Dependencies Sometimes packages require (depend upon) software that is not part of the R ecosystem. These will generally give you messages during the install process asking you to install this helper software. Common helper software includes things like Fortran and RJava. Sometimes you will need to go to websites, or use software like Homebrew (on the Mac) to install these extra helper pieces of software. A bit of googling will usually help with the specific package requirements. 10.2 Loading Packages with Library Run the code chunk below to load both {tidyverse} and {medicaldata}. Note that the {tidyverse} package is actually a meta-package that contains 8 packages, and each one has its own version number. library(tidyverse) library(medicaldata) Notice that loading tidyverse led to some conflict messages. The dplyr::filter function masks the stats::filter() function. These two packages, {dplyr} and {stats}, both have a function named filter(). The more recently loaded package is assumed to be the default, so if you call a filter() command, R will use dplyr::filter(). If you want to call the stats::filter() command, you have to explicitly use the package::function() format. If you are not sure which package you loaded last, it can be wise to use the explicit format when calling functions in R. The other masked function is lag(). The function dplyr::lag() is masking stats::lag(), as {dplyr} was loaded after {stats}. Most of the time this is not a big difference, but every once in a while a conflict between package functions can get very confusing. When in doubt, use the explicit format, in which you call package::function() to make clear what you mean, as in dplyr::lag() vs. stats::lag(). Note that it is good practice to load all of your packages needed for an R script or an Rmarkdown (.Rmd) document at the beginning of the script or .Rmd. This allows someone else using your script or Rmd to check whether they have the needed packages installed, and install them if needed. In an Rmarkdown document, this is done in a special setup code chunk near the top of the document. If some of these packages are not on CRAN, it is good practice to add a comment (a statement after a hashtag) on how to install this package. For example, in a setup chunk that loads {tidyverse} and {medicaldata}, it is a good idea to add a comment on how to install {medicaldata}, which is not yet on CRAN. See the example below library(tidyverse) library(medicaldata) # the {medicaldata} package can be installed with remotes::install_github(&#39;higgi13425/medicaldata&#39;) 10.3 Updating R Every once in a while, you will want to update your version of R. Usually this occurs with a major version upgrade, when something important changes. You may not rush into this, as it means re-installing all of your packages, but eventually it is worth it to be up to date. In order to update R, you have to find your installed version of R and run it on its own, outside of RStudio. This is easy if you have an R desktop shortcut, but not too hard if you hunt around a bit in your Applications folder. Double click the R icon to start up R. It will open the R Console and a menu. Click on the R menu at top left, and select Check for R Updates. If you are up to date, the R Console will report “Your version of R is up to date”. If not, this process will provide windows and buttons to click to upgrade to the latest version of R. When done, quit R and start RStudio to make sure the update has carried over. You should see the new version number when RStudio starts. 10.4 Updating RStudio To update RStudio, just run RStudio, and go to the Help menu in the top menu bar (not the Help tab in the lower right quadrant). In the Help menu, select Check for Updates. It will tell you if you are using the latest version of RStudio, or will direct you to the website to download the latest version. 10.5 Updating Your Packages To update your packages, you go to the Tools menu in RStudio, and select Check for package updates. You will usually get a list of the packages that have been updated since you installed them. Generally, select Update All, and allow one restart of your R session. RStudio may ask you if you want to restart more than once, but always say no after the first session restart. You may be asked about installing some packages from source, and you should generally select Yes. In general, your Console pane will be a bit chatty as it documents all the steps in package installation, but should generally end with something like: “The downloaded source packages are in ‘/private/var/folders/93/s18zkv2d4f556fxbjvb8yglc0000gp/T/RtmpHnsvlh/downloaded_packages’” and return to your &gt; prompt. If you then re-check for Package Updates, you will get the message that all of your packages are up to date. This process is a bit different after a major version upgrade of R, which we will cover in a later chapter. You have to retreive a list of all your packages, decide which to keep, and then install these fresh in the new version of R (and its new, major-version-specific package library). "],["major-r-updates-where-are-my-packages.html", "Chapter 11 Major R Updates (Where Are My Packages?)", " Chapter 11 Major R Updates (Where Are My Packages?) This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. "],["comparing-two-measures-of-centrality.html", "Chapter 12 Comparing Two Measures of Centrality 12.1 Common Problem 12.2 One Sample T test 12.3 Insert flipbook for ttest here 12.4 Fine, but what about 2 groups? 12.5 3 Assumptions of Student’s t test 12.6 Getting results out of t.test 12.7 Reporting the results from t.test using inline code", " Chapter 12 Comparing Two Measures of Centrality A common question in medical research is whether one group had a better outcome than another group. These outcomes can be measured with dichotomous outcomes like death or hospitalization, but continuous outcomes like systolic blood pressure, endoscopic score, or ejection fraction are more commonly available, and provide more statistical power, and usually require a smaller sample size. There is a tendency in clinical research to focus on dichotomous outcomes, even to the point of converting continuous measures to dichotomous ones (aka “dichotomania”, see Frank Harrell comments here), for fear of detecting and acting upon a small change in a continuous outcome that is not clinically meaningful. While this can be a concern, especially in very large, over-powered studies, it can be addressed by aiming for a continuous difference that is at least as large as one that many clinicians agree (a priori) is clinically important (the MCID, or Minimum Clinically Important Difference). The most common comparison of two groups with a continuous outcome is to look at the means or medians, and determine whether the available evidence suggests that these are equal (the null hypothesis). This can be done for means with Student’s t-test. Let’s start by looking at the cytomegalovirus data set. This includes data on 64 patients who received bone marrow stem cell transplant, and looks at their time to activation of CMV (cytomegalovirus). In the code chunk below, we group the data by donor cmv status (donor.cmv), and look at the mean time to CMV activation (time.to.cmv variable). Run the code (using the green arrow at the top right of the code chunk below) to see the difference in time to CMV activation in months between groups. Try out some other grouping variables in the group_by statement, in place of donor.cmv. Consider variables like race, sex, and recipient.cmv. Edit the code and run it again with the green arrow at the top right. # insert libraries in each chunk as if independent library(tidyverse) library(medicaldata) cytomegalovirus %&gt;% group_by(sex) %&gt;% summarize(mean_time2cmv = mean(time.to.cmv)) -&gt; summ summ ## # A tibble: 2 x 2 ## sex mean_time2cmv ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 13.7 ## 2 1 12.7 That seems like a big difference for donor.cmv, between 13.7303333 months and 12.7441176 months. And it makes theoretical sense that having a CMV positive donor is more likely to be associated with early activation of CMV in the recipient. But is it a significant difference, one that would be very unlikely to happen by chance? That depends on things like the number of people in each group, and the standard deviation in each group. That is the kind of question you can answer with a t-test, or for particularly skewed data like hospital length of stay or medical charges, a Wilcoxon test. 12.1 Common Problem Comparing two groups Mean or median vs. expected Two arms of study - independent Pre and post / spouse and partner / left vs right arm – paired groups Are the means significantly different? Or the medians (if not normally distributed)? 12.1.1 How Skewed is Too Skewed? Formal test of normality = Shapiro-Wilk test Use base data set called ToothGrowth library(tidyverse) library(medicaldata) data &lt;- cytomegalovirus head(data) ## ID age sex race diagnosis ## 1 1 61 1 0 acute myeloid leukemia ## 2 2 62 1 1 non-Hodgkin lymphoma ## 3 3 63 0 1 non-Hodgkin lymphoma ## 4 4 33 0 1 Hodgkin lymphoma ## 5 5 54 0 1 acute lymphoblastic leukemia ## 6 6 55 1 1 myelofibrosis ## diagnosis.type time.to.transplant prior.radiation ## 1 1 5.16 0 ## 2 0 79.05 1 ## 3 0 35.58 0 ## 4 0 33.02 1 ## 5 0 11.40 0 ## 6 1 2.43 0 ## prior.chemo prior.transplant recipient.cmv donor.cmv ## 1 2 0 1 0 ## 2 3 0 0 0 ## 3 4 0 1 1 ## 4 4 0 1 0 ## 5 5 0 1 1 ## 6 0 0 1 1 ## donor.sex TNC.dose CD34.dose CD3.dose CD8.dose TBI.dose ## 1 0 18.31 2.29 3.21 0.95 200 ## 2 1 4.26 2.04 NA NA 200 ## 3 0 8.09 6.97 2.19 0.59 200 ## 4 1 21.02 6.09 4.87 2.32 200 ## 5 0 14.70 2.36 6.55 2.40 400 ## 6 1 4.29 6.91 2.53 0.86 200 ## C1/C2 aKIRs cmv time.to.cmv agvhd time.to.agvhd cgvhd ## 1 0 1 1 3.91 1 3.55 0 ## 2 1 5 0 65.12 0 65.12 0 ## 3 0 3 0 3.75 0 3.75 0 ## 4 0 2 0 48.49 1 28.55 1 ## 5 0 6 0 4.37 1 2.79 0 ## 6 0 2 1 4.53 1 3.88 0 ## time.to.cgvhd ## 1 6.28 ## 2 65.12 ## 3 3.75 ## 4 10.45 ## 5 4.37 ## 6 6.87 12.1.2 Visualize the Distribution of data variables in ggplot Use geom_histogram or geom_density (pick one or the other) look at the distribution of CD3.dose or time.to.cmv Bonus points: facet by sex or race or donor.cmv Your turn to try it library(tidyverse) library(medicaldata) data %&gt;% ggplot(mapping = aes(time.to.cmv)) + geom_density() + facet_wrap(~sex) + theme_linedraw() library(tidyverse) library(medicaldata) data %&gt;% ggplot(mapping = aes(time.to.cmv)) + geom_histogram() + facet_wrap(~race) 12.1.3 Visualize the Distribution of data$len in ggplot The OJ group is left skewed May be problematic for using means formally test with Shapiro-Wilk library(tidyverse) library(medicaldata) data$time.to.cmv %&gt;% shapiro.test() ## ## Shapiro-Wilk normality test ## ## data: . ## W = 0.68261, p-value = 0.0000000001762 12.1.4 Results of Shapiro-Wilk p-value = 0.1091 p not &lt; 0.05 Acceptably close to normal OK to compare means rather than medians can use t test rather than wilcoxon test if p is &lt; 0.05, use wilcoxon test also known as Mann-Whitney test a rank-based (non-parametric) test 12.1.5 Try it yourself use df &lt;- msleep library(tidyverse) library(medicaldata) df &lt;- msleep head(df$sleep_total) ## [1] 12.1 17.0 14.4 14.9 4.0 14.4 test the normality of total sleep hours in mammals 12.1.6 Mammal sleep hours library(tidyverse) library(medicaldata) shapiro.test(df$sleep_total) ## ## Shapiro-Wilk normality test ## ## data: df$sleep_total ## W = 0.97973, p-value = 0.2143 meets criteria - acceptable to consider normally distributed now consider - is the mean roughly 8 hours of sleep per day? 12.2 One Sample T test univariate test Ho: mean is 8 hours Ha: mean is not 8 hours can use t test because shapiro.test is NS 12.2.1 How to do One Sample T test library(tidyverse) library(medicaldata) t.test(df$sleep_total, alternative = &quot;two.sided&quot;, mu = 8) Try it out, see if you can interpret results 12.2.2 Interpreting the One Sample T test ## ## One Sample t-test ## ## data: df$sleep_total ## t = 4.9822, df = 82, p-value = 0.000003437 ## alternative hypothesis: true mean is not equal to 8 ## 95 percent confidence interval: ## 9.461972 11.405497 ## sample estimates: ## mean of x ## 10.43373 p is highly significant can reject the null, accept alternative sample mean 10.43, CI 9.46-11.41 12.2.3 What are the arguments of the t.test function? x = vector of continuous numerical data y= NULL - optional 2nd vector of continuous numerical data alternative = c(“two.sided”, “less”, “greater”), mu = 0 paired = FALSE var.equal = FALSE conf.level = 0.95 documentation 12.3 Insert flipbook for ttest here Below is a flipbook. It illustrates a bit of how to do a t-test. click on it and you can use the arrow keys to proceed forward and back through the slides, as you add lines of code and more results occur. Let’s start with a flipbook slide show. When the title slide appears, you can step through each line of the code to see what it does. The right/left and/or up/down arrows will let you move forward and backward in the code. You can use the arrow keys to go through it one step at a time (forward or backward, depending on which arrow key you use), to see what each line of code actually does. Give it a try below. See if you can figure out what each line of code is doing. 12.3.1 Flipbook Time! This is t-testing in action. 12.4 Fine, but what about 2 groups? consider df$vore library(tidyverse) library(medicaldata) prostate &lt;- medicaldata::blood_storage tabyl(prostate$AA) ## prostate$AA n percent ## 0 261 0.8259494 ## 1 55 0.1740506 hypothesis - herbivores need more time to get food, sleep less than carnivores how to test this? normal, so can use t test for 2 groups 12.4.1 Setting up 2 group t test formula interface: outcome ~ groupvar library(tidyverse) library(medicaldata) df %&gt;% filter(vore %in% c(&quot;herbi&quot;, &quot;carni&quot;)) %&gt;% t.test(formula = sleep_total ~ vore, data = .) Try it yourself What do the results mean? 12.4.2 Results of the 2 group t test ## ## Welch Two Sample t-test ## ## data: sleep_total by vore ## t = 0.63232, df = 39.31, p-value = 0.5308 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.911365 3.650509 ## sample estimates: ## mean in group carni mean in group herbi ## 10.378947 9.509375 12.4.3 Interpreting the 2 group t test Welch t-test (not Student) Welch does NOT assume equal variances in each group p value NS accept null hypothesis Ho: means of groups roughly equal Ha: means are different 95% CI crosses 0 Carnivores sleep a little more, but not a lot 12.4.4 2 group t test with wide data You want to compare column A with column B (data are not tidy) Do mammals spend more time awake than asleep? library(tidyverse) library(medicaldata) t.test(x = df$sleep_total, y = df$awake, data = msleep) 12.4.5 Results of 2 group t test with wide data library(tidyverse) library(medicaldata) t.test(x = df$sleep_total, y = df$awake, data = msleep) ## ## Welch Two Sample t-test ## ## data: df$sleep_total and df$awake ## t = -4.5353, df = 164, p-value = 0.00001106 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.498066 -1.769404 ## sample estimates: ## mean of x mean of y ## 10.43373 13.56747 12.5 3 Assumptions of Student’s t test Sample is normally distributed (test with Shapiro) Variances are homogeneous (homoskedasticity) (test with Levene) Observations are independent not paired like left vs. right colon not paired like spouse and partner not paired like measurements pre and post Rx 12.5.1 Testing Assumptions of Student’s t test Normality - test with Shapiro If not normal, Wilcoxon &gt; t test Equal Variances - test with Levene If not equal, Welch t &gt; Student’s t Observations are independent Think about data collection are some observations correlated with some others? If correlated, use paired t test 12.6 Getting results out of t.test Use the tidy function from the broom package Do carnivores have bigger brains than insectivores? library(tidyverse) library(medicaldata) library(broom) df %&gt;% filter(vore %in% c(&quot;carni&quot;, &quot;insecti&quot;)) %&gt;% t.test(formula = brainwt ~ vore, data = .) %&gt;% tidy() -&gt; result result 12.6.1 Getting results out of t.test ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0577 0.0793 0.0216 1.20 0.253 12 ## # … with 4 more variables: conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, ## # method &lt;chr&gt;, alternative &lt;chr&gt; 12.7 Reporting the results from t.test using inline code use backticks before and after, start with r i.e. My result is [backtick]r code here[backtick]. The mean brain weight for carnivores was 0.0792556 The mean brain weight for herbivores was 0.02155 The difference was 0.0577056 The t statistic for this Two Sample t-test was 1.1995501 The p value was 0.2534631 The confidence interval was from -0.05 to 0.16 12.7.1 For Next Time Skewness and Kurtosis Review Normality When to use Wilcoxon Levene test for equal variances When to use Welch t vs. Student’s t Paired t and Wilcoxon tests "],["sample-size-calculations-with-pwr.html", "Chapter 13 Sample Size Calculations with {pwr} 13.1 Sample Size for a Continuous Endpoint (t-test) 13.2 One Sample t-test for Lowering Creatinine 13.3 Paired t-tests (before vs after, or truly paired) 13.4 2 Sample t tests with Unequal Study Arm Sizes 13.5 Testing Multiple Options and Plotting Results 13.6 Your Turn 13.7 Sample Sizes for Proportions 13.8 Sample size for two proportions, equal n 13.9 Sample size for two proportions, unequal arms 13.10 Your Turn 13.11 add chi square 13.12 add correlation test 13.13 add anova 13.14 add linear model 13.15 add note on guessing effect sizes - cohen small, medium, large 13.16 Explore More", " Chapter 13 Sample Size Calculations with {pwr} When designing clinical studies, it is often important to calculate a reasonable estimate of the needed sample size. This is critical for planning, as you may find out very quickly that a reasonable study budget and timeline will be futile. Grant funding agencies will be very interested in whether you have a good rationale for your proposed sample size and timeline, so that they can avoid wasting their money. Fortunately, the {pwr} package helps with many of the needed calculations. Let’s first install this package from CRAN, and load it with a library() function. Remember that you can copy each code chunk to the clipboard, then paste it into your RStudio console (or a script) to edit and run it. Just hover your mouse pointer over the top right corner of each code chunk until a copy icon appears, then click on it to copy the code. install.packages(&#39;pwr&#39;) library(pwr) The {pwr} package has a number of functions for calculating sample size and power. Functions in the {pwr} package Test Sample Size Function one-sample, two-sample, and paired t-tests pwr.t.test() two-sample t-tests (unequal sample sizes) pwr.t2n.test() two-sample proportion test (unequal sample sizes) pwr.2p2n.test() one-sample proportion test pwr.p.test() two-sample proportion test pwr.2p.test() two-sample proportion test (unequal sample sizes) pwr.2p2n.test() one-way balanced ANOVA pwr.anova.test() correlation test pwr.r.test() chi-squared test (goodness of fit and association) pwr.chisq.test() test for the general linear model pwr.f2.test() Now that you have {pwr} up and running, we can start with a simple example. 13.1 Sample Size for a Continuous Endpoint (t-test) Let’s propose a study of a new drug to reduce hemoglobin A1c in type 2 diabetes over a 1 year study period. You estimate that your recruited participants will have a mean baseline A1c of 9.0, which will be unchanged by your placebo, but reduced (on average) to 7.0 by the study drug. You need to calculate an effect size (aka Cohen’s d) in order to estimate your sample size. This effect size is equal to the difference between the means at the endpoint, divided by the pooled standard deviation. Many clinicians can estimate the means and the difference, but the pooled standard deviation is not very intutitive. Sometimes you have an estimate from pilot data (though these tend to have wide confidence intervals, as pilot studies are small). In other circumstances, you can estimate a standard deviation for Hgb A1c from values from a large data warehouse. When you don’t have either of these, it can be helpful to start by estimating the range of values. This is something that is intutitive, and experienced clinicians can do fairly easily. Just get a few clinicians in a room, and ask them for the highest and lowest values of HgbA1c that they have ever seen. You will quickly find a minimum and maximum that you can estimate as the range (in this case, let’s say 5.0 and 17.0 for min and max of Hgb A1c). This range divided by 4 is a reasonable rough estimate of the standard deviation. Remember that a normally distributed continuous value will have a 95% confidence interval that is plus or minus 1.96 standard deviations from the sample mean. Round this up to 2 for the full range, and you can see why we divide the range by 4 to get an estimate of the standard deviation. In our case, the difference is 2 and the range/4 (estimate of SD) is 3. So our effect size (Cohen’s d) is 0.66. Plug in 0.66 for d in the code chunk below, and run this code chunk to get an estimate of the n in each arm of a 2 armed study with a two sample t-test of the primary endpoint. pwr::pwr.t.test(n = NULL, sig.level = 0.05, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;, power = 0.80, d = __) We come up with 37.02 participants in each group to provide 80% power to detect a difference of 2 in HgbA1c, assuming a standard deviation of 3, using a two-sided alpha of 0.05. To conduct this study, assuming a 20% dropout rate in each arm, would require 37+8 subjects per arm, or 90 overall. At an enrollment rate of 10 per month, it will require 9 months to enroll all the participants, and 21 months (9 + 12 month intervention) to complete the data collection. It is a very common mistake to look at the result for n, and assume that this is your total sample size needed. The n provided by {pwr} is the number per arm. You need to multiply this n by the number of arms (or in a paired analysis, by the number of pairs) to get your total n. Another common mistake is to assume no dropout of participants. It is important to have a reasonable estimate (10-20% for short studies, 30-50% for long or demanding studies) and inflate your intended sample size by this amount. It is even better if you know from similar studies what the actual dropout rate was, and use this as an estimate (if there are similar previous studies). As a general rule, it is better to be conservative, and estimate a larger sample size, than to end up with p = 0.07. Once you define your test type (the options are “two.sample”, “one.sample”, and “paired”), and the alternative (“two.sided”, “greater”, or “less”), four variables remain in a sample size and power calculation. These are the remaining four arguments of the pwr.t.test() function. These are: n the significance level (sig.level) the power the effect size (Cohen’s d) If you know any three of these, you can calculate the fourth. In order to do this, you set the one of these four arguments that you want to calculate equal to NULL, and specify the other 3. Imagine that we only have enough funds to run this study on 50 participants. What would our power be to detect a difference of 2 in Hgb A1c? You can set the power to NULL, and the n to 25 (remember that n is per arm), and run the chunk below. pwr::pwr.t.test(n = __, # note that n is per arm sig.level = 0.05, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;, power = __, d = 0.66) We end up with 62.8% power, assuming no participant dropout (which is an extremely unlikely assumption). You can do the same thing, changing the NULL, to calculate an effect size or a significance level, if you have any need to. In most cases, you are calculating a sample size, then realizing that you might not have that much money/resources. Then many calculate the power you would obtain given the resources you actually have. Let’s show a few more examples. 13.2 One Sample t-test for Lowering Creatinine Eddie Enema, holistic healer, has proposed an unblinded pilot study of thrice-daily 2 liter enemas with “Eddie’s Dialysis Cleanser,” a proprietary mix of vitamins and minerals, which he believes will lower the serum creatinine of patients on the kidney transplant waiting list by more than 1.0 g/dL in 24 hours. The creatinine SD in this group of patients is 2. The null hypothesis is &lt; 1.0 g/dL. The alternative hypothesis is &gt;= 1.0 g/dL. Cohen’s d is 1.0 (the proposed change, or delta)/2 (the SD) = 0.5. How many participants would Eddie Enema have to recruit to have 80% power to test this one-sample, one-sided hypothesis, with an alpha of 0.05? Check each of the argument values and run the chunk below to find out. pwr::pwr.t.test(n = NULL, # note that n is per arm sig.level = 0.05, type = &quot;one.sample&quot;, alternative = &quot;greater&quot;, power = 0.8, d = 0.5) Fast Eddie would need to recruit slightly more than 26 participants (you always have to round up to get whole human participants) to have 80% power, assuming no dropout between the first and third enema, or before the blood draw 24 hours after baseline. Note that since this is an unblinded, one-sample study, The n in the results is multiplied by the number of arms (there is only 1 arm) to give you a sample size of 27. A note about alpha and beta alpha is described as the type I error, or the probability of declaring significant a difference that is not truly significant. We often use a two-sided alpha, which cuts the region of significance in half, and distributes it to both tails of the distribution, allowing for both significant positive and negative differences. Alpha is commonly set at 0.05, which works out to 0.025 on each tail of the distribution with a two-tailed alpha. beta is the power, or (1- the risk of type II error). Type II error is the probability of missing a significant result and declaring it nonsignificant after hypothesis testing. Power is often set at 80%, or 0.8, but can be 90%, 95%, or 99%, depending on how important it is not to miss a significant result, and how much money and time you have to spend (both of which tend to increase N and power). There is often an important tradeoff between type I and type II error. Things that decrease type II error (increase power) like spending more time and money for a larger N, will increase your risk of type I error. Conversely, reducing your risk of type I error will generally increase your risk of type II error. You may be in situations in which you have to decide which type of error is more important to avoid for your clinical situation to maximize benefit and minimize harms for patients. 13.3 Paired t-tests (before vs after, or truly paired) As you can see from the above example, you can use a before-after design to measure differences from baseline, and essentially convert a two-sample paired design (each participant’s baseline measurement is paired with their post-intervention measurement) to a single sample design based on the difference between the before and after values. The before-after (or baseline-postintervention) design is probably the most common paired design, but occasionally we have truly paired designs, like when we test an ointment for psoriasis on one arm, and use a placebo or sham ointment on the other arm. When this is possible, through bilateral symmetry (this also works for eyedrops in eyes, or dental treatments), it is much more efficient (in the recruiting sense) than recruiting separate groups for the treatment and control arms. To see the difference between two-sample and paired designs, run the code chunk below, for a two-sample study with a Cohen’s d of 0.8 and 80% power. Then change the type to “paired”, and see the effect on sample size. pwr::pwr.t.test(n = NULL, # note that n is per arm sig.level = 0.05, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;, power = 0.8, d = 0.8) Note that this changes the needed sample size from 52 subjects (26 per arm) to 15 subjects (as there is only one participant needed each paired application of 2 study treatments, and n in this case indicates the number of pairs), though it would be wise to randomize patients to having the treatment on the right vs. left arm (to maintain the blind). This is a large gain in recruiting efficiency. Use paired designs whenever you can. 13.4 2 Sample t tests with Unequal Study Arm Sizes Occasionally investigators want unbalanced arms, as they feel that patients are more likely to participate if they have a greater chance of receiving the study drug than the placebo. It is fairly common to use 2:1 or 3:1 ratios. Larger ratios, like 4:1 or 5:1, are thought to risk increasing the placebo response rate, as participants assume that they are on the active drug. This is somewhat less efficient in recruiting terms, but it may improve the recruiting rate enough to compensate for the loss in efficiency. This requires a slightly different function, the pwr.t2n.test() function. Let’s look at an example below. Instead of n, we have n1 and n2, and we have to specify one of these, and leave the other as NULL. Or we can try a variety of ratios of n1 and n2, leaving the power set to NULL, and test numbers to produce the desired power. We are proposing a study in which the expected reduction in systolic blood pressure is 10 mm Hg, with a standard deviation of 20 mm Hg. We choose an n1 of 40, and a power of 80%, then let the function determine n2. pwr::pwr.t2n.test(n1 = 40, n2 = NULL, sig.level = 0.05, alternative = &quot;two.sided&quot;, power = 0.8, d = 0.5) In this case, n2 works out to slightly over 153 in the drug arm, or nearly 4:1. Calculating effect size, or Cohen’s d. You can calculate the d value yourself. Or, you can make your life easier to let the {pwr} package do this for you. You can leave the calculation of the delta/SD to the program, by setting d = (20-10)/20, and the program will calculate the d of 0.5 for you. We can also round up the ratio to 4:1 (160:40) and determine the resulting power. pwr::pwr.t2n.test(n1 = c(40), n2 = c(160), sig.level = 0.05, alternative = &quot;two.sided&quot;, power = NULL, d = 0.5) This provides a power of 80.3%. 13.5 Testing Multiple Options and Plotting Results It can be helpful to compare multiple scenarios, varying the n or the estimated effect size, to examine trade-offs and potential scenarios when planning a trial. You can test multiple particular scenarios by listing the variables in a concatenated vector, as shown below for n1 and n2. pwr::pwr.t2n.test(n1 = c(40, 60, 80), n2 = c(80, 120, 160), sig.level = 0.05, alternative = &quot;two.sided&quot;, power = NULL, d = 0.5) This provides 3 distinct scenarios, with 3 pairs of n1/n2 values, and the calculated power for each scenario. You can also examine many scenarios, with the sequence function, seq(). For the sequence function, three arguments are needed: from, the number the sequence starts from to, the number the sequence ends at (inclusive) by, the number to increment by Note that the length of the sequences produced by seq() must match (or be a multiple of the other) if you are sequencing multiple arguments, so that there is a number for each scenario. If the lengths of the sequences are multiples of each other (8 and 4 in the example below), the shorter sequence (n2) will be silently “recycled” (used again in the same order) to produce a vector of matching length (8). pwr::pwr.t2n.test(n1 = seq(from = 40, to = 75, by = 5), n2 = seq(60, 120, 20), sig.level = 0.05, alternative = &quot;two.sided&quot;, power = NULL, d = 0.5) Sometimes it is helpful to look at multiple scenarios and plot the results. You can do this by leaving n = NULL, and plotting the results, as seen below. The null value will be varied across a reasonable range, and the results plotted, with an optimal value identified. The plot function will use ggplot2 if this package is loaded, or base R plotting if ggplot2 is not available. As you can see below, you can modify the ggplot2 plot of the results with standard ggplot2 functions. results &lt;- pwr::pwr.t2n.test(n1 = c(40), n2 = NULL, sig.level = 0.05, alternative = &quot;two.sided&quot;, power = 0.80, d = 0.5) plot(results) + ggplot2::theme_minimal(base_size = 14) + labs(title = &#39;Optimizing Sample Size for my 2-Sided t test&#39;, subtitle = &quot;Always Round up for Whole Participants, N = 194&quot;) Note that the results object is a list, and you can access individual pieces with the dollar sign operator, so that `results$n1` equals 40, and `results$n2` equals 153. You can examine the components of the results object in the Environment pane in RStudio. You can use these in inline R expressions in an Rmarkdown document to write up your results. Remember that each inline R expression is wrapped in backwards apostrophes, like `r code` (using the character to the left of the 1 key on the standard US keyboard), and starts with an r to let the computer know that the incoming code is written in R. This helps you write up a sentence like the below for a grant application: Using an estimated effect size of 0.5, with a two-sided alpha of 0.05, we calculated that for 40 participants in group 1, 153.0968718 participants would be needed in group 2 to produce a power of 0.8. When you knit an Rmarkdown file with these inline R expressions, each will be automatically converted to the result number and appear as standard text. 13.6 Your Turn Try calculating the sample size or power needed in the continuous outcome scenarios below. See if you can plot the results as directed by editing the code chunks. 13.6.1 Scenario 1: FEV1 in COPD You want to increase the FEV1 (forced expiratory volume in 1 second) of patients with COPD (chronic obstructive pulmonary disease) by 10% of predicted from baseline using weekly inhaled stem cells vs. placebo. Unfortunately, the standard deviation of FEV1 measurements is 20%. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with equal n in each of the two arms. Fill in the blanks in the code chunk below to calculate the sample size needed (n x number of arms). Remember that the effect size (Cohen’s d) = change in endpoint (delta)/SD of the endpoint. pwr::pwr.t.test(n = __, # note that n is per arm sig.level = __, type = &quot;__&quot;, alternative = &quot;__&quot;, power = __, d = __) You should get 128 participants (assuming no dropout) from 64 per arm. Cohen’s d is 10/20 = 0.5. It can be tricky to keep the type of “two.sample” and the alternative of “two.sided” straight. But you can do this! 13.6.2 Scenario 2: BNP in CHF You want to decrease the BNP (brain natriuretic protein) of patients with CHF (congestive heart failure) by 300 pg/mL from baseline with a new oral intropic agent vs. placebo. BNP levels go up during worsening of heart failure, and a variety of effective treatments lower BNP, which can function as surrogate marker in clinical trials. The standard deviation of BNP measurements is estimated at 350 pg/mL. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with equal n in each of the two arms. Also consider an alternative scenario with a change in BNP of only 150 pg/mL. Remember that the effect size (Cohen’s d) = change in endpoint (delta)/SD of the endpoint. Fill in the blanks in the code chunk below (2 scenarios) to calculate the sample size needed (n x number of arms) for both alternatives. pwr::pwr.t.test(n = __, # note that n is per arm sig.level = __, type = &quot;two.__&quot;, alternative = &quot;two.__&quot;, power = __, d = __/__) pwr::pwr.t.test(n = __, # note that n is per arm sig.level = __, type = &quot;two.__&quot;, alternative = &quot;two.__&quot;, power = __, d = __/__) You should get 46 participants (assuming no dropout) from 23 per arm x 2 arms, or 174 participants (87x2) with the alternative effect size. The effect size (Cohen’s d) is 300/350 = 0.86 in the original, and 150/350 (0.43) in the alternative effect. Note that you can let R calculate the Cohen’s d - just type in 300/350 and 150/350, and R will use these as values of d. 13.6.3 Scenario 3: Barthel Index in Stroke You want to increase the Barthel Activities of Daily Living Index of patients with stroke by 25 points from baseline with an intensive in-home PT and OPT intervention vs. usual care (which usually increases BADLI by only 5 points). You roughly estimate the standard deviation of Barthel index measurements as 38. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with equal n in each of the two arms. You want to consider multiple possible options for n, and plot these for a nice figure in your grant application. Fill in the blanks in the code chunk below to calculate and plot the sample size needed (n x number of arms). results &lt;- pwr::pwr.t.test(n = __, # note that n is per arm sig.level = __, type = &quot;two.__&quot;, alternative = &quot;two.sided&quot;, power = __, d = __ ) plot(results) You should get an optimal sample size of 116 participants (assuming no dropout) from 58 per arm x 2 arms, with a nice plot to show this in your grant proposal. The effect size (Cohen’s d) is (25-5)/38 = 0.526. 13.7 Sample Sizes for Proportions Let’s assume that patients discharged from your hospital after a myocardial infarction have historically received a prescription for aspirin 80% of the time. A nursing quality improvement project on the cardiac floor has tried to increase this rate to 95%. How many patients do you need to track after the QI intervention to determine if the proportion has truly increased? the null hypothesis is that the proportion is 0.8 the alternative hypothesis is that the proportion is 0.95. For this, we need the pwr.p.test() function for one proportion. We will also use a built-in function of {pwr}, the ES.h() function, to help us calculate the effect size. This function takes our two hypothesized proportions and calculates an effect size with an arcsine transformation. pwr.p.test(h = ES.h(p1 = 0.95, p2 = 0.80), n = NULL, sig.level = 0.05, power = 0.80, alternative = &quot;greater&quot;) ## ## proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.4762684 ## n = 27.25616 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater We need to evaluate at least the next 28 patients discharged with MIs to have 80% power to test this one-sided hypothesis. A note about test sided-ness and publication. Frequently in common use, you may only be focused on an increase or decrease in a proportion or a continuous outcome, and a one-sided test seems reasonable. This is fine for internal use or local quality improvement work. However, for FDA approval of a drug, for grant applications, or for journal publications, the standard is to always use two-sided tests, being open to the possibility of both improvement or worsening of the outcome you are studying. This is important to know before you submit a grant application, a manuscript for publication, or a dossier for FDA approval of a drug or device. 13.8 Sample size for two proportions, equal n For this, we need the pwr.2p.test() function for two proportions. You want to calculate the sample size for a study of a cardiac plexus parasympathetic nerve stimulator for pulmonary hypertension. You expect the baseline one year mortality to be 15% in high-risk patients, and expect to reduce this to 5% with this intervention. You will compare a sham (turned off) stimulator to an active stimulator in a 2 arm study. Use a 2-sided alpha of 0.05 and a power of 80%. Copy and edit the code chunk below to determine the sample size (n, rounded up) per arm, and the overall sample size (2n) fo the study. pwr.p.test(h = ES.h(p1 = __, p2 = __), n = __, sig.level = __, power = __, alternative = &quot;__&quot;) We need to enroll at least 67 per arm, or 134 overall. Dichotomous endpoints are generally regarded as having greater clinical significance than continuous endpoints, but often require more power and sample size (and more money and time). Most investigators are short on money and time, and prefer continuous outcome endpoints. 13.9 Sample size for two proportions, unequal arms For this, we need the pwr.2p2n.test() function for two proportions with unequal sample sizes. Imagine you want to enroll class IV CHF patients in a device trial in which they will be randomized 3:1 to a device (vs sham) that restores their serum sodium to 140 mmol/L and their albumin to 40 mg/dL each night. You expect to reduce 1 year mortality from 80% to 65% with this device. You want to know what your power will be if you enroll 300 in the device arm and 100 in the sham arm. pwr.2p2n.test(h = ES.h(p1 = __, p2 = __), n1 = __, n2 = __, sig.level = __, power = __, alternative = &quot;two.sided&quot;) This (300:100) enrollment will have 84.6% power to detect a change from 15% to 5% mortality, with a two-sided alpha of 0.05. 13.10 Your Turn Try calculating the sample size or power needed in the proportional outcome scenarios below. See if you can plot the results as directed by editing the code chunks. 13.10.1 Scenario 1: Mortality on Renal Dialysis You want to decrease the mortality of patients on renal dialysis, which averages 20% per year in your local dialysis center. You will randomize patients to a bundle of statin, aspirin, beta blocker, and weekly erythropoietin vs. usual care, and hope to reduce annual mortality to 10%. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with equal n in each of the two arms. Fill in the blanks in the code chunk below to calculate the sample size needed (n x number of arms). pwr.p.test(h = ES.h(p1 = __, p2 = __), n = __, sig.level = .05, power = __, alternative = &quot;two.__&quot;) You always round up first (to whole participants per arm), then multiply by the number of arms. You will need a minimum of 98 per arm, for a total of 196 participants needed to complete the trial. 13.10.2 Scenario 2: Intestinal anastomosis in Crohn’s disease You want to decrease 1-year endoscopic recurrence rate in Crohn’s disease from 90% to 70%. A local surgeon claims that his new “slipknot anastomosis” technique will accomplish this, by reducing colonic backwash and thereby, reducing endoscopic recurrence. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with equal n in each of the two arms. Also consider an alternative, more conservative scenario with a endoscopic recurrence rate of 80% with the new method. Fill in the blanks in the code chunk below to calculate the sample size needed (n x number of arms) for both alternatives. pwr.p.test(h = c(ES.h(p1 = 0.9, p2 = __)), n = NULL, sig.level = .05, power = __, alternative = &quot;two.sided&quot;) pwr.p.test(h = c(ES.h(p1 = __, p2 = __)), n = __, sig.level = __, power = .80, alternative = __) With the originally claimed recurrence proportion of 70%, you will need 30 participants per arm, or 60 for the whole study. The more conservative estimate will require 98 subjects per arm, or 196 for the whole study. 13.10.3 Scenario 3: Metformin in Donuts Your local endocrinologist has identified consumption of glazed donuts as a major risk factor for development of type 2 diabetes in your region. She proposes to randomize participants to glazed donuts spiked with metformin vs usual donuts, expecting to reduce the 1 year proportion of prediabetics with a HgbA1c &gt; 7.0 from 25% to 10%. You want to have 80% power to detect this difference, with a 2-sided alpha of 0.05, with 2 times as many participants in the metformin donut arm. You want to consider multiple possible sample sizes (n = 25, 50, 75) for the control glazed donuts, with 2n (double the sample size in each scenario) for the metformin donuts group. Fill in the blanks in the code chunk below to calculate the resulting power for each of the three sample size scenarios. pwr.2p2n.test(h = ES.h(p1 = __, p2 = __), n1 = seq(from = __, to = __, by = 25), n2 = seq(from = __, to = __, by = 50), sig.level = __, power = NULL, alternative = &quot;two.sided&quot;) You should get a power of 37.7% for the smallest n, 64.5% for n1=50/n2=100, and 81.4% for the largest n scenario. 13.11 add chi square 13.12 add correlation test 13.13 add anova 13.14 add linear model 13.15 add note on guessing effect sizes - cohen small, medium, large 13.16 Explore More You can explore other examples here in the official {pwr} vignette. Power calculations for more complex endpoints and study designs can be found in R packages listed in the Clinical Trials CRAN Task View here. Consider the packages {samplesize}, {TrialSize}, {clusterpower}, {CRTsize}, {cosa}, {PowerTOST}, {PowerUpR}, and which may be relevant for your particular analysis. Two other helpful references are books: Cohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences (2nd ed.). LEA. Ryan, T.P. (2013) Sample Size Determination and Power. Wiley. "],["randomization-for-clinical-trials-with-r.html", "Chapter 14 Randomization for Clinical Trials with R 14.1 Printing these on Cards 14.2 Now, try this yourself 14.3 Now Freestyle", " Chapter 14 Randomization for Clinical Trials with R There are a number of packages for doing key functions for clinical trials in R. You can find many of these on the CRAN Task View for Clinical Trials, at https://cran.r-project.org/web/views/ClinicalTrials.html. This is a curated list of packages that anyone might find useful in designing, monitoring, or analyzing clinical trials, and is often a good place to start in looking for packages that might be relevant for clinical trials. If you use Ctrl-F to search the web page for “rand”, several packages address randomization, including blockrand randomizeR pwr experiment clusterPower CRTSize cosa PowerupR Several of these are specifically for more complex designs, including cluster and multilevel randomization (clusterPower, cosa, CRTSize). For today, we will focus on the straightforward randomization packages including {blockrand} and {randomizer}. The {blockrand} package creates randomizations for clinical trials with can include stratified enrollment and permuted block randomization, and can produce a PDF file of randomization cards. Let’s start with an example in {blockrand}. Details on the package can be found at https://cran.r-project.org/web/packages/blockrand/blockrand.pdf or by running help(blockrand) in your Console. You want to randomize 180 inpatients with severe ulcerative colitis to one of 3 arms: corticosteroids alone (control), corticosteroids + tofacitinib, or corticosteroids + upadacitinib. You want to stratify the participants by (1) prior biologic failure and (2) Albumin level above or below 3.0. To be prepared for dropouts and imbalanced enrollment, you want to have a randomization list with at least 60 assignments available for each arm and stratum. To avoid a recognizable pattern in the randomization, you want to have a permuted block design with blocks of sizes 3, 6, and 9. Below, you will see how to do this for the biologic failure - low albumin stratum. bfla &lt;- blockrand(n = 60, num.levels = 3, # three treatments levels = c(&quot;CS&quot;, &quot;CS/Tofa&quot;, &quot;CS/Upa&quot;), # arm names stratum = &quot;Bfail.LowAlb&quot;, # stratum name id.prefix = &quot;BfLA&quot;, # stratum abbrev block.sizes = c(1,2,3), # times arms = 3,6,9 block.prefix = &quot;BfLA&quot;) # stratum abbrev bfla ## id stratum block.id block.size treatment ## 1 BfLA01 Bfail.LowAlb BfLA01 9 CS/Upa ## 2 BfLA02 Bfail.LowAlb BfLA01 9 CS/Tofa ## 3 BfLA03 Bfail.LowAlb BfLA01 9 CS ## 4 BfLA04 Bfail.LowAlb BfLA01 9 CS/Upa ## 5 BfLA05 Bfail.LowAlb BfLA01 9 CS ## 6 BfLA06 Bfail.LowAlb BfLA01 9 CS/Tofa ## 7 BfLA07 Bfail.LowAlb BfLA01 9 CS/Upa ## 8 BfLA08 Bfail.LowAlb BfLA01 9 CS/Tofa ## 9 BfLA09 Bfail.LowAlb BfLA01 9 CS ## 10 BfLA10 Bfail.LowAlb BfLA02 6 CS/Upa ## 11 BfLA11 Bfail.LowAlb BfLA02 6 CS/Upa ## 12 BfLA12 Bfail.LowAlb BfLA02 6 CS/Tofa ## 13 BfLA13 Bfail.LowAlb BfLA02 6 CS ## 14 BfLA14 Bfail.LowAlb BfLA02 6 CS ## 15 BfLA15 Bfail.LowAlb BfLA02 6 CS/Tofa ## 16 BfLA16 Bfail.LowAlb BfLA03 6 CS/Tofa ## 17 BfLA17 Bfail.LowAlb BfLA03 6 CS/Upa ## 18 BfLA18 Bfail.LowAlb BfLA03 6 CS ## 19 BfLA19 Bfail.LowAlb BfLA03 6 CS/Upa ## 20 BfLA20 Bfail.LowAlb BfLA03 6 CS ## 21 BfLA21 Bfail.LowAlb BfLA03 6 CS/Tofa ## 22 BfLA22 Bfail.LowAlb BfLA04 6 CS ## 23 BfLA23 Bfail.LowAlb BfLA04 6 CS/Upa ## 24 BfLA24 Bfail.LowAlb BfLA04 6 CS/Tofa ## 25 BfLA25 Bfail.LowAlb BfLA04 6 CS/Upa ## 26 BfLA26 Bfail.LowAlb BfLA04 6 CS/Tofa ## 27 BfLA27 Bfail.LowAlb BfLA04 6 CS ## 28 BfLA28 Bfail.LowAlb BfLA05 6 CS/Tofa ## 29 BfLA29 Bfail.LowAlb BfLA05 6 CS ## 30 BfLA30 Bfail.LowAlb BfLA05 6 CS/Upa ## 31 BfLA31 Bfail.LowAlb BfLA05 6 CS/Tofa ## 32 BfLA32 Bfail.LowAlb BfLA05 6 CS/Upa ## 33 BfLA33 Bfail.LowAlb BfLA05 6 CS ## 34 BfLA34 Bfail.LowAlb BfLA06 3 CS/Upa ## 35 BfLA35 Bfail.LowAlb BfLA06 3 CS ## 36 BfLA36 Bfail.LowAlb BfLA06 3 CS/Tofa ## 37 BfLA37 Bfail.LowAlb BfLA07 6 CS/Upa ## 38 BfLA38 Bfail.LowAlb BfLA07 6 CS/Tofa ## 39 BfLA39 Bfail.LowAlb BfLA07 6 CS/Tofa ## 40 BfLA40 Bfail.LowAlb BfLA07 6 CS/Upa ## 41 BfLA41 Bfail.LowAlb BfLA07 6 CS ## 42 BfLA42 Bfail.LowAlb BfLA07 6 CS ## 43 BfLA43 Bfail.LowAlb BfLA08 3 CS/Tofa ## 44 BfLA44 Bfail.LowAlb BfLA08 3 CS/Upa ## 45 BfLA45 Bfail.LowAlb BfLA08 3 CS ## 46 BfLA46 Bfail.LowAlb BfLA09 9 CS/Upa ## 47 BfLA47 Bfail.LowAlb BfLA09 9 CS/Tofa ## 48 BfLA48 Bfail.LowAlb BfLA09 9 CS ## 49 BfLA49 Bfail.LowAlb BfLA09 9 CS ## 50 BfLA50 Bfail.LowAlb BfLA09 9 CS/Upa ## 51 BfLA51 Bfail.LowAlb BfLA09 9 CS ## 52 BfLA52 Bfail.LowAlb BfLA09 9 CS/Upa ## 53 BfLA53 Bfail.LowAlb BfLA09 9 CS/Tofa ## 54 BfLA54 Bfail.LowAlb BfLA09 9 CS/Tofa ## 55 BfLA55 Bfail.LowAlb BfLA10 3 CS ## 56 BfLA56 Bfail.LowAlb BfLA10 3 CS/Upa ## 57 BfLA57 Bfail.LowAlb BfLA10 3 CS/Tofa ## 58 BfLA58 Bfail.LowAlb BfLA11 6 CS/Tofa ## 59 BfLA59 Bfail.LowAlb BfLA11 6 CS ## 60 BfLA60 Bfail.LowAlb BfLA11 6 CS/Upa ## 61 BfLA61 Bfail.LowAlb BfLA11 6 CS ## 62 BfLA62 Bfail.LowAlb BfLA11 6 CS/Tofa ## 63 BfLA63 Bfail.LowAlb BfLA11 6 CS/Upa You can see the id for each participant, their stratum, the block.id for their permuted block, the block.size, and their assigned treatment. You can imagine this as a randomization list, or as assignments that you could print out on cards and seal in security envelopes for the time of randomization. Of course, this is only one of our four strata. We should do the same for the 3 other strata. bfha &lt;- blockrand(n = 60, num.levels = 3, # three treatments levels = c(&quot;CS&quot;, &quot;CS/Tofa&quot;, &quot;CS/Upa&quot;), # arm names stratum = &quot;Bfail.HiAlb&quot;, # stratum name id.prefix = &quot;BfHA&quot;, # stratum abbrev block.sizes = c(1,2,3), # times arms = 3,6,9 block.prefix = &quot;BfHA&quot;) # stratum abbrev bfha ## id stratum block.id block.size treatment ## 1 BfHA01 Bfail.HiAlb BfHA01 6 CS/Upa ## 2 BfHA02 Bfail.HiAlb BfHA01 6 CS/Tofa ## 3 BfHA03 Bfail.HiAlb BfHA01 6 CS ## 4 BfHA04 Bfail.HiAlb BfHA01 6 CS/Tofa ## 5 BfHA05 Bfail.HiAlb BfHA01 6 CS ## 6 BfHA06 Bfail.HiAlb BfHA01 6 CS/Upa ## 7 BfHA07 Bfail.HiAlb BfHA02 6 CS/Tofa ## 8 BfHA08 Bfail.HiAlb BfHA02 6 CS/Upa ## 9 BfHA09 Bfail.HiAlb BfHA02 6 CS ## 10 BfHA10 Bfail.HiAlb BfHA02 6 CS ## 11 BfHA11 Bfail.HiAlb BfHA02 6 CS/Tofa ## 12 BfHA12 Bfail.HiAlb BfHA02 6 CS/Upa ## 13 BfHA13 Bfail.HiAlb BfHA03 6 CS/Tofa ## 14 BfHA14 Bfail.HiAlb BfHA03 6 CS ## 15 BfHA15 Bfail.HiAlb BfHA03 6 CS/Tofa ## 16 BfHA16 Bfail.HiAlb BfHA03 6 CS/Upa ## 17 BfHA17 Bfail.HiAlb BfHA03 6 CS/Upa ## 18 BfHA18 Bfail.HiAlb BfHA03 6 CS ## 19 BfHA19 Bfail.HiAlb BfHA04 6 CS/Upa ## 20 BfHA20 Bfail.HiAlb BfHA04 6 CS/Tofa ## 21 BfHA21 Bfail.HiAlb BfHA04 6 CS ## 22 BfHA22 Bfail.HiAlb BfHA04 6 CS ## 23 BfHA23 Bfail.HiAlb BfHA04 6 CS/Tofa ## 24 BfHA24 Bfail.HiAlb BfHA04 6 CS/Upa ## 25 BfHA25 Bfail.HiAlb BfHA05 3 CS/Tofa ## 26 BfHA26 Bfail.HiAlb BfHA05 3 CS ## 27 BfHA27 Bfail.HiAlb BfHA05 3 CS/Upa ## 28 BfHA28 Bfail.HiAlb BfHA06 6 CS/Upa ## 29 BfHA29 Bfail.HiAlb BfHA06 6 CS/Upa ## 30 BfHA30 Bfail.HiAlb BfHA06 6 CS/Tofa ## 31 BfHA31 Bfail.HiAlb BfHA06 6 CS/Tofa ## 32 BfHA32 Bfail.HiAlb BfHA06 6 CS ## 33 BfHA33 Bfail.HiAlb BfHA06 6 CS ## 34 BfHA34 Bfail.HiAlb BfHA07 3 CS/Upa ## 35 BfHA35 Bfail.HiAlb BfHA07 3 CS/Tofa ## 36 BfHA36 Bfail.HiAlb BfHA07 3 CS ## 37 BfHA37 Bfail.HiAlb BfHA08 9 CS/Upa ## 38 BfHA38 Bfail.HiAlb BfHA08 9 CS/Tofa ## 39 BfHA39 Bfail.HiAlb BfHA08 9 CS/Tofa ## 40 BfHA40 Bfail.HiAlb BfHA08 9 CS/Tofa ## 41 BfHA41 Bfail.HiAlb BfHA08 9 CS ## 42 BfHA42 Bfail.HiAlb BfHA08 9 CS ## 43 BfHA43 Bfail.HiAlb BfHA08 9 CS/Upa ## 44 BfHA44 Bfail.HiAlb BfHA08 9 CS/Upa ## 45 BfHA45 Bfail.HiAlb BfHA08 9 CS ## 46 BfHA46 Bfail.HiAlb BfHA09 9 CS ## 47 BfHA47 Bfail.HiAlb BfHA09 9 CS/Upa ## 48 BfHA48 Bfail.HiAlb BfHA09 9 CS/Upa ## 49 BfHA49 Bfail.HiAlb BfHA09 9 CS ## 50 BfHA50 Bfail.HiAlb BfHA09 9 CS/Tofa ## 51 BfHA51 Bfail.HiAlb BfHA09 9 CS/Tofa ## 52 BfHA52 Bfail.HiAlb BfHA09 9 CS/Tofa ## 53 BfHA53 Bfail.HiAlb BfHA09 9 CS ## 54 BfHA54 Bfail.HiAlb BfHA09 9 CS/Upa ## 55 BfHA55 Bfail.HiAlb BfHA10 6 CS/Upa ## 56 BfHA56 Bfail.HiAlb BfHA10 6 CS/Tofa ## 57 BfHA57 Bfail.HiAlb BfHA10 6 CS ## 58 BfHA58 Bfail.HiAlb BfHA10 6 CS/Upa ## 59 BfHA59 Bfail.HiAlb BfHA10 6 CS ## 60 BfHA60 Bfail.HiAlb BfHA10 6 CS/Tofa bnha &lt;- blockrand(n = 60, num.levels = 3, levels = c(&quot;CS&quot;, &quot;CS/Tofa&quot;, &quot;CS/Upa&quot;), stratum = &quot;Bnaive.HiAlb&quot;, id.prefix = &quot;BnHA&quot;, block.sizes = c(1,2,3, 4), block.prefix = &quot;BnHA&quot;) bnha ## id stratum block.id block.size treatment ## 1 BnHA01 Bnaive.HiAlb BnHA1 6 CS/Upa ## 2 BnHA02 Bnaive.HiAlb BnHA1 6 CS ## 3 BnHA03 Bnaive.HiAlb BnHA1 6 CS/Upa ## 4 BnHA04 Bnaive.HiAlb BnHA1 6 CS/Tofa ## 5 BnHA05 Bnaive.HiAlb BnHA1 6 CS ## 6 BnHA06 Bnaive.HiAlb BnHA1 6 CS/Tofa ## 7 BnHA07 Bnaive.HiAlb BnHA2 6 CS ## 8 BnHA08 Bnaive.HiAlb BnHA2 6 CS/Upa ## 9 BnHA09 Bnaive.HiAlb BnHA2 6 CS ## 10 BnHA10 Bnaive.HiAlb BnHA2 6 CS/Tofa ## 11 BnHA11 Bnaive.HiAlb BnHA2 6 CS/Upa ## 12 BnHA12 Bnaive.HiAlb BnHA2 6 CS/Tofa ## 13 BnHA13 Bnaive.HiAlb BnHA3 12 CS ## 14 BnHA14 Bnaive.HiAlb BnHA3 12 CS/Tofa ## 15 BnHA15 Bnaive.HiAlb BnHA3 12 CS ## 16 BnHA16 Bnaive.HiAlb BnHA3 12 CS/Upa ## 17 BnHA17 Bnaive.HiAlb BnHA3 12 CS ## 18 BnHA18 Bnaive.HiAlb BnHA3 12 CS/Upa ## 19 BnHA19 Bnaive.HiAlb BnHA3 12 CS/Tofa ## 20 BnHA20 Bnaive.HiAlb BnHA3 12 CS/Tofa ## 21 BnHA21 Bnaive.HiAlb BnHA3 12 CS/Upa ## 22 BnHA22 Bnaive.HiAlb BnHA3 12 CS ## 23 BnHA23 Bnaive.HiAlb BnHA3 12 CS/Upa ## 24 BnHA24 Bnaive.HiAlb BnHA3 12 CS/Tofa ## 25 BnHA25 Bnaive.HiAlb BnHA4 6 CS ## 26 BnHA26 Bnaive.HiAlb BnHA4 6 CS/Tofa ## 27 BnHA27 Bnaive.HiAlb BnHA4 6 CS ## 28 BnHA28 Bnaive.HiAlb BnHA4 6 CS/Upa ## 29 BnHA29 Bnaive.HiAlb BnHA4 6 CS/Tofa ## 30 BnHA30 Bnaive.HiAlb BnHA4 6 CS/Upa ## 31 BnHA31 Bnaive.HiAlb BnHA5 3 CS ## 32 BnHA32 Bnaive.HiAlb BnHA5 3 CS/Upa ## 33 BnHA33 Bnaive.HiAlb BnHA5 3 CS/Tofa ## 34 BnHA34 Bnaive.HiAlb BnHA6 6 CS/Upa ## 35 BnHA35 Bnaive.HiAlb BnHA6 6 CS/Tofa ## 36 BnHA36 Bnaive.HiAlb BnHA6 6 CS/Tofa ## 37 BnHA37 Bnaive.HiAlb BnHA6 6 CS ## 38 BnHA38 Bnaive.HiAlb BnHA6 6 CS ## 39 BnHA39 Bnaive.HiAlb BnHA6 6 CS/Upa ## 40 BnHA40 Bnaive.HiAlb BnHA7 3 CS/Upa ## 41 BnHA41 Bnaive.HiAlb BnHA7 3 CS ## 42 BnHA42 Bnaive.HiAlb BnHA7 3 CS/Tofa ## 43 BnHA43 Bnaive.HiAlb BnHA8 6 CS/Tofa ## 44 BnHA44 Bnaive.HiAlb BnHA8 6 CS/Upa ## 45 BnHA45 Bnaive.HiAlb BnHA8 6 CS ## 46 BnHA46 Bnaive.HiAlb BnHA8 6 CS ## 47 BnHA47 Bnaive.HiAlb BnHA8 6 CS/Tofa ## 48 BnHA48 Bnaive.HiAlb BnHA8 6 CS/Upa ## 49 BnHA49 Bnaive.HiAlb BnHA9 12 CS/Upa ## 50 BnHA50 Bnaive.HiAlb BnHA9 12 CS/Tofa ## 51 BnHA51 Bnaive.HiAlb BnHA9 12 CS/Upa ## 52 BnHA52 Bnaive.HiAlb BnHA9 12 CS/Tofa ## 53 BnHA53 Bnaive.HiAlb BnHA9 12 CS/Upa ## 54 BnHA54 Bnaive.HiAlb BnHA9 12 CS ## 55 BnHA55 Bnaive.HiAlb BnHA9 12 CS ## 56 BnHA56 Bnaive.HiAlb BnHA9 12 CS/Upa ## 57 BnHA57 Bnaive.HiAlb BnHA9 12 CS/Tofa ## 58 BnHA58 Bnaive.HiAlb BnHA9 12 CS/Tofa ## 59 BnHA59 Bnaive.HiAlb BnHA9 12 CS ## 60 BnHA60 Bnaive.HiAlb BnHA9 12 CS bnla &lt;- blockrand(n = 60, num.levels = 3, levels = c(&quot;CS&quot;, &quot;CS/Tofa&quot;, &quot;CS/Upa&quot;), stratum = &quot;Bnaive.LoAlb&quot;, id.prefix = &quot;BnLA&quot;, block.sizes = c(1,2,3), block.prefix = &quot;BnLA&quot;) bnla ## id stratum block.id block.size treatment ## 1 BnLA01 Bnaive.LoAlb BnLA01 3 CS/Upa ## 2 BnLA02 Bnaive.LoAlb BnLA01 3 CS ## 3 BnLA03 Bnaive.LoAlb BnLA01 3 CS/Tofa ## 4 BnLA04 Bnaive.LoAlb BnLA02 9 CS ## 5 BnLA05 Bnaive.LoAlb BnLA02 9 CS ## 6 BnLA06 Bnaive.LoAlb BnLA02 9 CS/Tofa ## 7 BnLA07 Bnaive.LoAlb BnLA02 9 CS/Tofa ## 8 BnLA08 Bnaive.LoAlb BnLA02 9 CS/Tofa ## 9 BnLA09 Bnaive.LoAlb BnLA02 9 CS/Upa ## 10 BnLA10 Bnaive.LoAlb BnLA02 9 CS/Upa ## 11 BnLA11 Bnaive.LoAlb BnLA02 9 CS ## 12 BnLA12 Bnaive.LoAlb BnLA02 9 CS/Upa ## 13 BnLA13 Bnaive.LoAlb BnLA03 6 CS/Upa ## 14 BnLA14 Bnaive.LoAlb BnLA03 6 CS ## 15 BnLA15 Bnaive.LoAlb BnLA03 6 CS ## 16 BnLA16 Bnaive.LoAlb BnLA03 6 CS/Upa ## 17 BnLA17 Bnaive.LoAlb BnLA03 6 CS/Tofa ## 18 BnLA18 Bnaive.LoAlb BnLA03 6 CS/Tofa ## 19 BnLA19 Bnaive.LoAlb BnLA04 3 CS/Upa ## 20 BnLA20 Bnaive.LoAlb BnLA04 3 CS/Tofa ## 21 BnLA21 Bnaive.LoAlb BnLA04 3 CS ## 22 BnLA22 Bnaive.LoAlb BnLA05 9 CS ## 23 BnLA23 Bnaive.LoAlb BnLA05 9 CS/Tofa ## 24 BnLA24 Bnaive.LoAlb BnLA05 9 CS ## 25 BnLA25 Bnaive.LoAlb BnLA05 9 CS ## 26 BnLA26 Bnaive.LoAlb BnLA05 9 CS/Tofa ## 27 BnLA27 Bnaive.LoAlb BnLA05 9 CS/Upa ## 28 BnLA28 Bnaive.LoAlb BnLA05 9 CS/Upa ## 29 BnLA29 Bnaive.LoAlb BnLA05 9 CS/Upa ## 30 BnLA30 Bnaive.LoAlb BnLA05 9 CS/Tofa ## 31 BnLA31 Bnaive.LoAlb BnLA06 9 CS ## 32 BnLA32 Bnaive.LoAlb BnLA06 9 CS/Tofa ## 33 BnLA33 Bnaive.LoAlb BnLA06 9 CS ## 34 BnLA34 Bnaive.LoAlb BnLA06 9 CS/Tofa ## 35 BnLA35 Bnaive.LoAlb BnLA06 9 CS ## 36 BnLA36 Bnaive.LoAlb BnLA06 9 CS/Tofa ## 37 BnLA37 Bnaive.LoAlb BnLA06 9 CS/Upa ## 38 BnLA38 Bnaive.LoAlb BnLA06 9 CS/Upa ## 39 BnLA39 Bnaive.LoAlb BnLA06 9 CS/Upa ## 40 BnLA40 Bnaive.LoAlb BnLA07 3 CS/Upa ## 41 BnLA41 Bnaive.LoAlb BnLA07 3 CS ## 42 BnLA42 Bnaive.LoAlb BnLA07 3 CS/Tofa ## 43 BnLA43 Bnaive.LoAlb BnLA08 6 CS ## 44 BnLA44 Bnaive.LoAlb BnLA08 6 CS/Tofa ## 45 BnLA45 Bnaive.LoAlb BnLA08 6 CS/Upa ## 46 BnLA46 Bnaive.LoAlb BnLA08 6 CS ## 47 BnLA47 Bnaive.LoAlb BnLA08 6 CS/Tofa ## 48 BnLA48 Bnaive.LoAlb BnLA08 6 CS/Upa ## 49 BnLA49 Bnaive.LoAlb BnLA09 6 CS ## 50 BnLA50 Bnaive.LoAlb BnLA09 6 CS/Tofa ## 51 BnLA51 Bnaive.LoAlb BnLA09 6 CS ## 52 BnLA52 Bnaive.LoAlb BnLA09 6 CS/Upa ## 53 BnLA53 Bnaive.LoAlb BnLA09 6 CS/Tofa ## 54 BnLA54 Bnaive.LoAlb BnLA09 6 CS/Upa ## 55 BnLA55 Bnaive.LoAlb BnLA10 6 CS/Upa ## 56 BnLA56 Bnaive.LoAlb BnLA10 6 CS ## 57 BnLA57 Bnaive.LoAlb BnLA10 6 CS/Tofa ## 58 BnLA58 Bnaive.LoAlb BnLA10 6 CS ## 59 BnLA59 Bnaive.LoAlb BnLA10 6 CS/Upa ## 60 BnLA60 Bnaive.LoAlb BnLA10 6 CS/Tofa 14.1 Printing these on Cards Ideally, you will print out each randomization on a card, and seal it in a security envelope, with the outside of the envelope labeled with the id. You can do this with the plotblockrand() function. This function creates a pdf file of randomization cards for printing. These are designed so that the middle text will show in a standard letter sized envelope with a window, and the top text (the assignment) can be folded over to increase security (against anyone trying to peek through the security envelope to guess the assignment). uc_study &lt;- bind_rows(bfha, bfla, bnha, bnla) # bind together the four strata into one dataframe blockrand::plotblockrand(uc_study, # input dataframe file = &quot;uc_study.pdf&quot;, # output pdf # top hidden text with assignment top=list(text=c(&#39;My Study&#39;,&#39;Patient: %ID%&#39;,&#39;Treatment: %TREAT%&#39;), col=c(&#39;black&#39;,&#39;black&#39;,&#39;red&#39;),font=c(1,1,4)), # middle text to show through window of # 10 envelope middle=list(text=c(&quot;My Study&quot;,&quot;Stratum: %STRAT%&quot;,&quot;Patient: %ID%&quot;), col=c(&#39;black&#39;,&#39;blue&#39;,&#39;orange&#39;),font=c(1,2,3)), # bottom text- any instructions to study coordinator bottom=&quot;Call 123-4567 to report patient entry&quot;, cut.marks=TRUE) # add cut marks - 4 per page Open up the file uc_study.pdf in your Files tab to see the output pdf file, with assorted fonts and colors. Just for fun, change (and then re-run) the text “My Study” to something more interesting change “Patient” to “Participant” change “Treatment” to “Arm” or “Assignment” change some of the colors to standard R colors change some of the fonts (within 1-4) Sometimes with equal blocks, and clear treatment effects or side effects, nurses or study coordinators can guess the randomization pattern. If you want to get fancy, and make it even harder to guess treatment assignments, you can add one of the unequal blocks options, to make it hard to find patterns in treatment or in side effects. Set uneq.beg = TRUE for an unequal block in the beginning, or uneq.mid = TRUE for an unequal block in the middle. 14.2 Now, try this yourself You want to randomize 80 outpatients with Crohn’s disease to one of 8 arms, as part of a 2^3 factorial design to increase patient activation. These arms involve using (A,B, C) or not using (a,b,c) 3 intervention components. The 8 arms then become: abc abC aBc aBC Abc AbC ABc ABC Then we want to stratify the participants by baseline PAM score (a measure of patient activation) with levels of low, medium, and high PAM. To be prepared for dropouts and imbalanced enrollment, you want to have a randomization list with at least 32 assignments available for each arm and stratum. To avoid a recognizable pattern in the randomization, you want to have a permuted block design with blocks of sizes 8 and 16. You can hover over top right corner of the code chunk below, and a copy icon will appear - click this to copy the code to your clipboard. You can then paste it into your local version of RStudio, edit it, and run it. In the code block below, fill in the blanks to complete the code to make a dataframe for the low_pam stratum. low_pam &lt;- blockrand(n = __, num.levels = __, #eight treatments levels = c(&quot;abc&quot;, &quot;abC&quot;, &quot;aBc&quot;, &quot;aBC&quot;, &quot;Abc&quot;, &quot;AbC&quot;, &quot;ABc&quot;, &quot;ABC&quot;), # arm names stratum = &quot;__&quot;, # stratum name id.prefix = &quot;lp&quot;, # stratum abbrev block.sizes = c(1,2,3), # times arms block.prefix = &quot;LP&quot;) # stratum abbrev low_pam Now that you have one stratum sorted, edit the code block below to create the med_pam and high_pam strata. med_pam &lt;- blockrand(n = __, num.levels = __, #eight treatments levels = c(&quot;abc&quot;, &quot;abC&quot;, &quot;aBc&quot;, &quot;aBC&quot;, &quot;Abc&quot;, &quot;AbC&quot;, &quot;ABc&quot;, &quot;ABC&quot;), # arm names stratum = &quot;__&quot;, # stratum name id.prefix = &quot;__&quot;, # stratum abbrev block.sizes = c(__), # times arms block.prefix = &quot;__&quot;) # stratum abbrev med_pam high_pam &lt;- blockrand(n = __, num.levels = __, #eight treatments levels = c(&quot;abc&quot;, &quot;abC&quot;, &quot;aBc&quot;, &quot;aBC&quot;, &quot;Abc&quot;, &quot;AbC&quot;, &quot;ABc&quot;, &quot;ABC&quot;), # arm names stratum = &quot;__&quot;, # stratum name id.prefix = &quot;__&quot;, # stratum abbrev block.sizes = c(__), # times arms block.prefix = &quot;__&quot;) # stratum abbrev high_pam Great! Now try to bind these 3 strata into one dataframe print these as cards to a pdf file Edit the code chunk below to produce the pdf file cd_study &lt;- bind_rows(__,__,__) # bind together the 3 strata into one dataframe blockrand::plotblockrand(__, # input dataframe file = &quot;cd_study.pdf&quot;, # output pdf # top hidden text with assignment top=list(text=c(&#39;CD Study&#39;,&#39;Patient: %ID%&#39;,&#39;Treatment: %__%&#39;), col=c(&#39;orange&#39;,&#39;blue&#39;,&#39;red&#39;),font=c(1,1,4)), # middle text to show through window of # 10 envelope middle=list(text=c(&quot;CD Study&quot;,&quot;Stratum: %STRAT%&quot;,&quot;Patient: %__%&quot;), col=c(&#39;black&#39;,&#39;red&#39;,&#39;cadetblue&#39;),font=c(1,2,3)), # bottom text- any instructions to study coordinator bottom=&quot;Call 123-4567 to report patient entry&quot;, cut.marks=TRUE) # add cut marks - 4 per page 14.3 Now Freestyle Your turn. Create randomization tables and a pdf file of cards for a study of 2 microbiome interventions to reduce the formation of colon adenomas. your 3 study arms will be - placebo, Streptococcus thermophilus, and S.thermo plus lactose (a preferred sugar for S.t, making this arm a synbiotic, while arm 2 is a probiotic) - aka 3 arms called: pbo, probiotic, synbiotic. Your stratifications will be by prior polyps being MSI_hi or MSI_lo (for microsatellite instability mutations) BMI above or below 35. BMI_hi, BMI_low block sizes of 4,8,12,16 160 per arm Edit the code block below for the first stratum mhbh &lt;- blockrand(n = __, # treatment arms num.levels = __, # of treatments levels = c(&quot;placebo&quot;, &quot;probiotic&quot;, &quot;synbiotic&quot;), # arm names stratum = &quot;__,__&quot;, # stratum name id.prefix = &quot;mhbh&quot;, # stratum abbrev block.sizes = c(__,__,__,__), # times arms block.prefix = &quot;__&quot;) # stratum abbrev mhbh Edit the code block below for the remaining strata mhbl &lt;- blockrand(n = __, # treatment arms num.levels = 3, # of treatments levels = c(&quot;placebo&quot;, &quot;probiotic&quot;, &quot;__&quot;), # arm names stratum = &quot;msi_hi.bmi_lo&quot;, # stratum name id.prefix = &quot;__&quot;, # stratum abbrev block.sizes = c(1,2,__,__), # times arms block.prefix = &quot;MHBL&quot;) # stratum abbrev mhbl mlbl &lt;- blockrand(n = 160, # treatment arms num.levels = __, # of treatments levels = c(&quot;placebo&quot;, &quot;__&quot;, &quot;synbiotic&quot;), # arm names stratum = &quot;__&quot;, # stratum name id.prefix = &quot;mlbl&quot;, # stratum abbrev block.sizes = c(__,__,3,4), # times arms block.prefix = &quot;MLBL&quot;) # stratum abbrev mlbl mlbh &lt;- blockrand(n = __, # treatment arms num.levels = 3, # of treatments levels = c(&quot;__&quot;, &quot;probiotic&quot;, &quot;synbiotic&quot;), # arm names stratum = &quot;msi_lo.bmi_hi&quot;, # stratum name id.prefix = &quot;__&quot;, # stratum abbrev block.sizes = c(1,2,3,4), # times arms block.prefix = &quot;MLBH&quot;) # stratum abbrev mlbh Edit the code block below to bind the strata together and print the cards adenoma_study &lt;- bind_rows(mlbl, mlbh, mhbh, mhbl) # bind together the strata into one dataframe blockrand::plotblockrand(__, # input dataframe file = &quot;adenoma_cards.pdf&quot;, # output pdf # top hidden text with assignment top=list(text=c(&#39;Adenoma Study&#39;,&#39;Patient: %__%&#39;,&#39;Treatment: %TREAT%&#39;), col=c(&#39;orange&#39;,&#39;blue&#39;,&#39;red&#39;),font=c(1,1,4)), # middle text to show through window of # 10 envelope middle=list(text=c(&quot;Adenoma Study&quot;,&quot;Stratum: %__%&quot;,&quot;Patient: %ID%&quot;), col=c(&#39;black&#39;,&#39;red&#39;,&#39;cadetblue&#39;),font=c(1,2,3)), # bottom text- any instructions to study coordinator bottom=&quot;Call 123-4567 to report patient entry. \\nInstruct participant to avoid antibiotics and stop aspirin&quot;, cut.marks=TRUE) # add cut marks - 4 per page "],["univariate-ggplots-to-visualize-distributions.html", "Chapter 15 Univariate ggplots to Visualize Distributions 15.1 HIstograms 15.2 Density Plots 15.3 Comparing Distributions Across Categories 15.4 Boxplots 15.5 Violin Plots 15.6 Ridgeline Plots", " Chapter 15 Univariate ggplots to Visualize Distributions When you first encounter a new dataset, it is often helpful to start with Data Exploration and Validation (DEV) . Note that DEV is different from EDA (Exploratory Data Analysis), which involves rummaging through your data (often with plots) and generating new hypotheses, performing multiple tests, and essentially data mining. DEV has specific goals, which include identifying variables with problematic values (NA, outliers), unexpected distributions (sometimes bimodal), and less-than-helpful variable names or data types. In this chapter, we will look at investigating single continuous variables, looking for outliers, multi-modal distributions, and making comparisons across categories. 15.1 HIstograms One of the most helpful ways to get started is to explore your continuous variables with the humble histogram or dotplot. These geoms in ggplot2 allow you to see a distribution of a single variable, and are a good way to get started with ggplot. Let’s start by looking at the distribution of the number of colon polyps found in participants in a clinical trial. You just need to map a variable (in this case, number12m) to the x aesthetic, and you are good to go. medicaldata::polyps %&gt;% ggplot() + aes(x = number12m) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). This gives us a very basic histogram, with some participants with zero polyps, but many with much larger numbers. We also learn that geom_histogram is a bit grumpy, and complains that we have not picked a binwidth (or a number of bins). Since the distribution goes out to 60+ polyps, let’s pick a binwidth of 5 (or about 13 bins). polyps %&gt;% ggplot() + aes(x = number12m) + geom_histogram(binwidth = 5) ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). Even with 13 bins, most bins contain only one or two participants. It would be more helpful to visualize each participant as a single dot, which you can do with a dotplot (geom_dotplot). medicaldata::polyps %&gt;% ggplot() + aes(x = number12m, fill = treatment) + geom_dotplot(binwidth = 1) + scale_y_continuous(NULL, breaks = NULL) + # Make this ratio = (tallest column * binwidth * 1.5) coord_fixed(ratio = 6) + labs(title = &quot;Distribution of Participants binned by number of Polyps&quot;, subtitle = &quot;Colored by Treatment&quot;) + theme(legend.position = &quot;top&quot;) ## Warning: Removed 2 rows containing non-finite values ## (stat_bindot). One of the nice things about geom_dotplot() is that it represents each participant as a dot, which is closer to reality than columns which look like they are portraying values, rather than counts. The y-axis is less helpful. It is showing the proportion of the total sample, which is sort of helpful, but this leaves a lot of empty space with no data most of the time. 15.1.1 Comparisons of Distributions with Histograms When you have a continuous variable that could vary across several categories of a categorical variable, you may want to compare the distributions across the categories. You can compare a small number of categories with histograms, usually 5 categories or less is manageable. There are several ways to make this kind of comparison. You can set the fill aesthetic to the categorical variable. In this case, we will use the treatment variable. This is OK, as you can see the sulindac-treated patients are shifted to the left, but it is not great, as the counts are stacked, and this can make comparisons hard. Note that color = outline of bars color, as opposed to fill. medicaldata::polyps %&gt;% ggplot() + aes(x = number12m, fill = treatment) + geom_histogram(bins=15, color = &quot;purple&quot;) ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). A mirror histogram can work well for 2 categories. You filter the data by value within the geom, and set the y value to ..density.. for one category value, and to negative -..density.. for the other category value, as seen below. The ..density.. variable is a value ggplot calculates in the background with a stat function. medicaldata::polyps %&gt;% ggplot(aes(x = number12m)) + geom_histogram(fill = &quot;red&quot;, aes(y = ..density..), data = . %&gt;% filter(treatment == &#39;placebo&#39;), bins = 15) + geom_label(aes(x = 55, y = 0.03, label = &quot;Placebo&quot;), color = &quot;red&quot;) + geom_histogram(fill = &quot;#404080&quot;, aes(y = -..density..), data = . %&gt;% filter(treatment == &#39;sulindac&#39;), bins = 15) + geom_label(aes(x = 20, y = -0.03, label = &quot;Sulindac&quot;), color = &quot;#404080&quot;) ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). You can do small multiples with facet_wrap medicaldata::polyps %&gt;% ggplot() + aes(x = number12m, fill = treatment) + geom_histogram(bins =15) + facet_grid(. ~ treatment) ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). Note for facet_wrap and facet_grid, the formula notation or the arguments is y \\~ x, so that by putting treatment after the tilde, the treatments are compared on the x axis, or side by side. If you put the treatment categorical value in the first (y) position, they would be shown as top and bottom. medicaldata::polyps %&gt;% ggplot() + aes(x = number12m, fill = treatment) + geom_histogram(bins =15) + facet_grid(treatment ~ .) ## Warning: Removed 2 rows containing non-finite values ## (stat_bin). While the viewer can generally make the comparisons with 2 categories, it can get more complicated with increasing numbers of categories. 15.1.2 Histograms and Categories You can also look at distributions of different categories, by color: mockstudy %&gt;% ggplot() + aes(x = age, color = sex, fill = sex) + geom_histogram(alpha=0.3, position=&quot;identity&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. Or with vertical facets: mockstudy %&gt;% ggplot() + aes(x = fu.time, color = sex) + geom_histogram(fill=&quot;white&quot;, alpha=0.5, position=&quot;identity&quot;)+ facet_grid(sex ~ .) ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. or horizontal facets: mockstudy %&gt;% ggplot() + aes(x = age, color = sex) + geom_histogram(fill=&quot;white&quot;, alpha=0.5, position=&quot;identity&quot;)+ facet_grid(. ~ sex) ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. 15.2 Density Plots Density Plots are essentially smoothed versions of histograms. Density plots are helpful to show the distribution of a numeric variable. The defaults for smoothing are quite reasonable. The only required aesthetic (like histogram) is an x variable. Optional aesthetics include alpha - setting the transparency, from 0 (invisible) to 1 (the default). Used when there are a lot of overlapping points, usually in the range from 0.05-0.5. group - for grouping into separate curves by a categorical variable. color - color of the distribution line, like color = “orchid”. When this is set to a categorical variable, geom_density will group by this variable and assign different colors to the groups. If you want to control the color assignments, you can use a palette with scale_color_brewer() or scale_color_manual (with the manual option you can set your own values or palette). Some options can be found here. fill color. You can use a single color, or set this to a categorical variable and use scale_fill_brewer or scale_fill_manual. You can also use scale_fill_viridis. size (line thickness) - the default is 1. linetype (6 types 1-6). You can also use the names of these 6 standard line types in quotes which are “solid”, “dashed”, “dotted”, “dotdash”, “longdash”, or “twodash”. You can even define a cutsom line type, providing the lengths of (up to 8) consecutive on-off segments. The string “3313” specifies a line with 3 units on, 3 units off, 1 unit on, 3 units off (dash-dot). The standard dotted and dashed line types (types 2-6) correspond to “44”, “13”. “1343”, “73” and “2262”. mockstudy %&gt;% ggplot() + aes(x = age) + geom_density() + geom_density(fill=&quot;orchid&quot;, size = 2, position=&quot;identity&quot;, alpha = 0.4, linetype = &quot;dashed&quot;)+ facet_grid(. ~ sex) + hrbrthemes::theme_ipsum() 15.2.1 Comparisons with Density plots You can compare a small number of categories with density plots, usually 5 or less. https://www.r-graph-gallery.com/density-plot You can set color and fill to the same variable. But you may have a lot of overlap. It may help to make these semi-transparent. It can be problematic if there is a lot of overlap and multiple categories, as they can hide each other A mirror density chart can work well for 2 categories. You can do small multiples with facet_wrap You can even do a stacked density chart. This is not great, as after the first layer, the baseline changes, and it ban be hard to compare. Can work for large differences between categories, but not good for small differences. 15.3 Comparing Distributions Across Categories Several geoms are designed for comparisons of distributions across multiple categories, and are more useful for this than histograms or density plots, particularly when the number of categories is large. These include Boxplots Ridgeline plots Violin plots All of these can be combined with plotting of individual data points to give both a summary of the distribution and visualization of the actual data. This can be important, especially when the number data points varies across categories, or of the number of data points are quite low in one part of the distribution. 15.4 Boxplots Boxplots are quick summaries of a distribution. They quickly give you a median line, and 25th and 75th percentiles (the ends of the box). The upper whisker is found at the last observation less than or equal to the 75th percentile plus 1.5* the IQR (interquartile range, or 75th-25th percentile). The lower whisker is found at the last observation less than or equal to the 25th percentile minus 1.5* the IQR. Points beyond the whiskers are considered “outliers”. Boxplots can be useful to compare distributions of a continuous variable across several categories. However, boxplots have several weakesses. Boxplots assume that your data are unimodal, and hide the distribution of your data points, which can be problematic if they are truly bimodal or trimodal (or even more local modes! (as in a Likert scale). Boxplots can also hide the number of observations, which may be quite different between categories. When this is an issue, it can be helpful to add data points, with geoms like jitter, beeswarm, or sina, or in ridgeline/raincloud plots. If there are a lot of overlapping points, you may need to set the geom_point alpha argument to a low value (0.05-0.5), rather than the default transparency of one. It can help the viewer of box plots to arrange your categories by order of their median, to make the interpretation easier. If your categories have a natural order (like months of the year), you will generally be better off keeping this natural order than ordering by median. 15.5 Violin Plots https://www.r-graph-gallery.com/violin_and_boxplot_ggplot2.html Violin Plots are essentially distribution plots that are symmetrical around their baseline. They are better than boxplots in that they don’t hide the distribution of your data points, and they don’t assume that the data are unimodal. You can see multiple local clusters if they are present in your data. A bimodal distribution will give you a shape that is reminiscent of a violin body. Violin plots can be helpful for comparing the distribution of a continuous variable across many categories. Violin plots can hide the number of observations, which may be quite different between categories. When this is an issue, it can be helpful to add points, with geoms like jitter, beeswarm, or sina, or in ridgeline/raincloud plots. If there are a lot of overlapping points, you may need to set the geom_point alpha argument to a low value (0.05-0.5), rather than the default transparency of one. Violin plots are helpful when you are comparing many groups. 15.6 Ridgeline Plots https://www.r-graph-gallery.com/ridgeline-plot Ridgeline plots are a nice way to show the distribution of a continuous variable, and compare distributions across several categories. Ridgeline plots are great for showing off striking differences between categories. Because they can have some overlap, they are not best for small or subtle differences. Note that you can add the N in each category to the category label when the number in each category varies. cmv %&gt;% ggplot(aes(x = time.to.transplant, y = diagnosis)) + geom_density_ridges(scale = 0.9) ## Picking joint bandwidth of 8.96 ## Warning: Removed 1 rows containing non-finite values ## (stat_density_ridges). 15.6.1 Including Plots You can also embed plots, for example: cmv %&gt;% ggplot(aes(x = time.to.agvhd, y = diagnosis, fill= stat(x))) + geom_density_ridges_gradient(scale = 0.9) + scale_fill_viridis_c(name = &quot;Days&quot;, option = &quot;A&quot;) + labs(title = &quot;Time to AGvHD in Days&quot;) ## Picking joint bandwidth of 8.13 15.6.2 Including Points You can also add data points, for example: cmv %&gt;% ggplot(aes(x = time.to.cmv, y = diagnosis, fill= stat(x))) + geom_density_ridges_gradient(scale = 0.5, jittered_points = TRUE, position = &quot;raincloud&quot;, alpha = 0.5) + scale_fill_viridis_c(name = &quot;Days&quot;, option = &quot;B&quot;) + labs(title = &quot;Time to CMV in Days&quot;) + theme_ridges() ## Picking joint bandwidth of 8.96 15.6.3 Including Points You can also add data points, for example: cmv %&gt;% ggplot(aes(x = CD8.dose, y = diagnosis, fill= stat(x))) + geom_density_ridges_gradient(scale = 0.9) + scale_fill_viridis_c(name = &quot;Days&quot;, option = &quot;C&quot;) + labs(title = &quot;CD8 Dose&quot;) + theme_ridges() ## Picking joint bandwidth of 0.249 15.6.4 Including Points You can also add data points, for example: cmv %&gt;% ggplot(aes(x = CD3.dose, y = diagnosis, fill= stat(x))) + geom_density_ridges_gradient(scale = 0.7, jittered_points = TRUE, position = &quot;raincloud&quot;) + scale_fill_viridis_c(name = &quot;Days&quot;, option = &quot;D&quot;) + labs(title = &quot;CD3 Dose&quot;) + theme_ridges() ## Picking joint bandwidth of 0.761 15.6.5 Including Points You can also add data points, for example: cmv %&gt;% ggplot(aes(x = TNC.dose, y = diagnosis, fill= stat(x))) + geom_density_ridges_gradient(scale = 2, jittered_points = TRUE, position = &quot;points_sina&quot;, alpha = 0.7) + scale_fill_viridis_c(name = &quot;Days&quot;, option = &quot;E&quot;) + labs(title = &quot;TNC Dose&quot;) + theme_ridges() ## Picking joint bandwidth of 2.19 "],["helping-out-with-ggplot.html", "Chapter 16 Helping out with ggplot 16.1 ggx::gghelp() 16.2 Getting more help with theming with ggThemeAssist 16.3 Website helpers for ggplot 16.4 Getting Even more help with esquisse", " Chapter 16 Helping out with ggplot The {ggplot} package is extremely powerful, and has many extension packages that augment that power for data visualization. But the number of options, particularly for theming, can get overwhelming. This is especially true if you are not using {ggplot} every day. Medical day (and night) jobs can get in the way of great data visualizations. But there are several helper packages to make your life easier, including {ggx}, {ggThemeAssist}, and {esquisse}. 16.1 ggx::gghelp() Let’s start with {ggx}. The gghelp() function in this simple package converts natural language queries (in quotes) into a ggplot command string. It can be helpful for styling axes, labels, font size, title, and legends. It is limited by its library of commands and range of styling, but can be helpful in a pinch. Try a few questions below, and make your own. ggx::gghelp(&quot;how do I remove the legend&quot;) ## Registered S3 method overwritten by &#39;sets&#39;: ## method from ## print.element ggplot2 ## theme(legend.position = &quot;none&quot;) ggx::gghelp(&quot;how do I increase the font size of the title&quot;) ## theme(axis.title.x=element_text(size=rel(2))) ggx::gghelp(&quot;change the x axis label to &#39;systolic blood pressure&#39;&quot;) ## xlab(&#39;systolic blood pressure&#39;) 16.2 Getting more help with theming with ggThemeAssist ggThemeAssist is an RStudio add-in that you install when you install the {ggThemeAssist} package. It is used after you have your basic plot in place, with the right geom, and the x and y variables in place. Let’s start you out with a simple plot from the {medicaldata} package. This plots the fentanyl requirements for anesthesia in the supraclavicular dataset comparing high and low BMI, age, and gender effects. Run this code block to see what the basic plot looks like. plot1 &lt;- medicaldata::supraclavicular %&gt;% filter(!is.na(bmi)) %&gt;% mutate(gender_cat = case_when(gender == 1 ~ &quot;Male&quot;, gender == 0 ~ &quot;Female&quot;)) %&gt;% mutate(bmi_cat = case_when(bmi &lt;30 ~ &quot;low&quot;, bmi &gt;= 30 ~ &quot;high&quot;)) %&gt;% ggplot(aes(x=age, y = fentanyl, col = gender_cat)) + geom_point() + facet_wrap(. ~ bmi_cat) plot1 + theme(axis.text.y = element_text(family = &quot;serif&quot;), panel.background = element_rect(fill = &quot;antiquewhite&quot;), plot.background = element_rect(fill = &quot;white&quot;)) + labs(title = &quot;Fentanyl Requirements&quot;, x = &quot;Age&quot;, y = &quot;Fentanyl in mcg&quot;, colour = &quot;Gender&quot;, subtitle = &quot;by BMI and Age&quot;) ## Warning: Removed 1 rows containing missing values ## (geom_point). Now select the code that generates the plot. You can also assign the plot (with an assignment arrow) to an imaginative name, like plot1. You can then select this object as well. Once you have the plot selected in your code, you can activate ggThemeAssist in one of two ways: Go to your Addins dropdown menu, and select ggThemeAssist type in the Console: ggthemeAssistGaget(plot1) Either approach will open up an interactive window, with 6 tabs and a bunch of options. These tabs include: Settings Panel and Background Axis Title and Label Legend Subtitle and Caption Open these up and experiment. You will see the results as you change options. When you are happy with the result, click on the Done button in the top right. This will add all of your thematic changes as valid R code to the existing plot object. 16.3 Website helpers for ggplot Several websites provide quick help for ggplot needs, and are worth bookmarking. The Aesthetics Helper, at https://ggplot2tor.com/aesthetics/, provides a quick guide to which aesthetics are required (green), and which other aesthetics are available (optional, in orange) to map variables in your dataset to components of a plot for each geom. The Guide to Scales, at https://ggplot2tor.com/scales, helps you find the proper names of scales in ggplot2, by selecting your variable type and (if needed) the aesthetic for the scale. Then by clicking on one of the available scales, you can get example code with appropriate syntax for multiple arguments. This can be very helpful in preventing frustration with not having the scales quite right. The R Graph Gallery, at https://www.r-graph-gallery.com, is a popular overview of available graph types. You can quickly scan a bunch of plots based on distributions, correlations, rankings, parts of a whole, change over time, maps, and flow and select one that looks interesting. When you find one you like, you can click on it and get the underlying ggplot code. Another popular web gallery with code is the Top 50 ggplot2 visualizations, at http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html Cedric Scherer has a lengthy free tutorial with lots of great examples at https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/ There is also a gallery of all the many extension packages people have built as add-ons to ggplot, often for specific and specialized plotting needs. Take a loot at https://exts.ggplot2.tidyverse.org/gallery/ Packages ggridges, ggalluvial, ggmosaic, ggdist, gghalves, and ComplexUpset have proven quite popular. Take a look at all the options for extending ggplot. 16.4 Getting Even more help with esquisse {esquisse} is an RStudio add-in that you install when you install the {esquisse} package. It is used to create plots from scratch in ggplot. A nice website on how to get started can be found at: https://cran.r-project.org/web/packages/esquisse/vignettes/get-started.html But the two ways to get started are to either: run esquisser(dataframe) in the Console, with the dataframe that you want to make a plot from, or In the Addins menu, select ggplot2_builder under ESQUISSE Either approach opens up a new window. If you did not specify a dataframe, it will ask you to select one, either from the Global Environment, or from a data package. For our purposes, select the {medicaldata} package, and the dataframe blood_storage. This will give you a window with many options, and a sandbox full of variables. To look at the data, click on the Show Data icon at top left You can drag variables into different aesthetic mappings. Start with PreopPSA for x TimeToRecurrence for y Age mapped to color FamHx or AA mapped to Size You can drag variables out of mappings and back to the variable sandbox if needed You can click on the Settings Gear at the top right to activate more aesthetics You can click on the geom icon at top left to change the plot type You can edit the Title and Labels with this tab You can edit Plot Options You can edit the plot appearance When you are happy with it, you can click on the Code tab, and copy it to your script. {esquisse} can get you started on good plots, and remind you of ggplot options you may have forgotten (or never knew) about. There are a number of fancy plots and extensions it can not do, and it will not clean your data for you, or reorder factors. But it can be really helpful if you are not plotting very often or if you are just getting started with ggplot. 16.4.1 Acknowledgement This chapter is inspired by a lesson from Claus Wilke at https://wilkelab.org/SDS375/slides/functional-programming "],["functions.html", "Chapter 17 Functions 17.1 Don’t repeat yourself 17.2 Your Turn 17.3 Freestyle 17.4 Read More", " Chapter 17 Functions Nearly everything in R and the tidyverse is built on functions. Every command you write that ends with parentheses is a function. The parentheses contain arguments (unless you just use the defaults), which the function acts upon. If you type a function into the console and forget the parentheses, you may be surprised at what you get back when you press the Enter key. Try this with the sd function by running the chunk below sd At first this looks like computer gobbledygook, but if you look at it closely, the first line sets up the structure of the function and its arguments, and the second line defines the function as the square root of the variance of a vector that contains numbers. You are seeing the inner workings of the sd function. Other functions may be written in other languages (like C++) for more speed, but you can use R to write more R functions. When you try this for another function, like filter in the dplyr package, it is less informative. As you can see when you unpack the function below. dplyr::filter 17.1 Don’t repeat yourself Functions are especially helpful when you do something repeatedly, or do the same thing on several variables or several datasets. Let’s start with a simple plot of Ct (COVID viral load) vs age for inpatients. Run the code chunk below. covid %&gt;% filter(patient_class == &quot;inpatient&quot;) %&gt;% ggplot() + aes(x = age, y = ct_result) + geom_point() + theme_linedraw() + labs(y = &quot;viral load (in Ct)&quot;, title = &quot;Patient class: inpatient&quot;) This is great, but you would like a similar plot for ER patients, observation patients, outpatients, etc. How do you do this without multiple copy-paste and careful edits of the key variables (often with errors)? First, you need to avoid hard-coding the specific values that change from plot to plot. In our next version of this plot, we will set an envronment variable (patient_class_choice_env) to “inpatient”, then use that variable to: filter the data set the title (using the glue function) patient_class_choice_env = &quot;inpatient&quot; covid %&gt;% filter(.data$patient_class == .env$patient_class_choice_env) %&gt;% ggplot() + aes(x = age, y = ct_result) + geom_point() + theme_linedraw() + labs(y = &quot;viral load (in Ct)&quot;, title = glue(&quot;Patient class: {patient_class_choice_env}&quot;)) The use of variable pronouns is very important in the filter step. .data$patient_class means the variable “patient_class” in the currently in-use dataframe. .env$patient_class_choice_env means the variable “patient_class_choice_env” in the current environment (check the Environment pane). This can be especially helpful when these variables have the same names - the .data pronoun helps you refer specifically to variables in the dataframe, and the .env pronoun helps you refer specifically to variables in the environment. It is usually better to give these slightly different names. In this case, I have added the suffix, “choice_env” for the variable in the environment in which I have specified the value chosen. Let’s try another example. In the code chunk below, edit the patient_class_choice_env variable to the value “emergency”, and then run the code chunk. patient_class_choice_env = &quot;emergency&quot; covid %&gt;% filter(.data$patient_class == .env$patient_class_choice_env) %&gt;% ggplot() + aes(x = age, y = ct_result) + geom_point() + theme_linedraw() + labs(y = &quot;viral load (in Ct)&quot;, title = glue(&quot;Patient class: {patient_class_choice_env}&quot;)) Fairly slick, right? You can edit and run through each value for patient_class. But what if you want to automate this, and just produce all the plots for all the possible values? To do this, you have to turn your plot into a function. To make a function, you first need a chunk of code with no hard-coded values. Then, you wrap the code in curly braces, and preface it with “function(arguments)” Then you assign it to a function name. Examine, then run the example below. make_covid_plot &lt;- function(patient_class_choice_env) { covid %&gt;% filter(.data$patient_class == .env$patient_class_choice_env) %&gt;% ggplot() + aes(x = age, y = ct_result) + geom_point() + theme_linedraw() + labs(y = &quot;viral load (in Ct)&quot;, title = glue(&quot;Patient class: {patient_class_choice_env}&quot;)) } make_covid_plot(&quot;recurring outpatient&quot;) Note that this creates a function object named make_covid_plot in your Environment pane. And you still have to call the function with the argument “recurring outpatient” to get one plot. Generally, you should write functions when you find yourself copy-pasting the same code several times. Each function should be no more than 20-30 lines. If longer, break it up into a couple of functions. You can do this repeatedly, over several values for patient_class. Try this below by editing the code chunk to make plots for “emergency” and “not applicable” patient classes. make_covid_plot(&quot;outpatient&quot;) make_covid_plot(&quot;observation&quot;) This is better, but it is still not completely automated. Let’s automate calling the function over several values with the map function. map takes each element of the vector patient_class and uses it as input for make_covid_plot(). map returns a list of created plots. We can return these with bracket references to items 1,2, and 3 in the list. patient_class &lt;- c(&quot;emergency&quot;, &quot;inpatient&quot;, &quot;observation&quot;) plots &lt;- map(patient_class, make_covid_plot) plots[[1]] plots[[2]] plots[[3]] We can make this simpler with the walk function. The walk function is like map, but it does not return values, only side effects (like printing). In this case, it is applying the print function to each element of the plots list. walk(plots, print) We can automate this further, and not need to supply the vector of patient_class values. We can automate it by applying the make_covid_plot function to all available patient_class values. To demonstrate this, we need a more general function, with 2 arguments: one for the dataset, and one for for patient_class. We will test this on the “inpatient” class. The filtering operation will be moved outside of the function make_covid_plot_2 &lt;- function(data, patient_class) { data %&gt;% # note data argument # filtering now done outside of function ggplot() + aes(x = age, y = ct_result) + geom_point() + theme_linedraw() + labs(y = &quot;viral load (in Ct)&quot;, title = glue(&quot;Patient class: {patient_class}&quot;)) } data_inpatient &lt;- covid %&gt;% filter(patient_class == &quot;inpatient&quot;) make_covid_plot_2(data_inpatient, patient_class = &quot;inpatient&quot;) Now we can try this in a tidy pipeline. We will first subset the data by nesting it with the nest function. Take a look at what this does covid %&gt;% nest(data = -patient_class) ## # A tibble: 5 x 2 ## patient_class data ## &lt;chr&gt; &lt;list&gt; ## 1 observation &lt;tibble [28 × 16]&gt; ## 2 outpatient &lt;tibble [65 × 16]&gt; ## 3 emergency &lt;tibble [124 × 16]&gt; ## 4 recurring outpatient &lt;tibble [11 × 16]&gt; ## 5 inpatient &lt;tibble [111 × 16]&gt; We have essentially grouped the data in to 5 distinct tibbles (dataframes) in the list-column named data. These are dataframes in a column nested inside of a larger dataframe. Now we can make a plot from each of these smaller dataframes, using the mutate function and the map function, and our original make_covid_plot function. covid %&gt;% nest(data = -patient_class) %&gt;% mutate(plots = map(patient_class, make_covid_plot)) ## # A tibble: 5 x 3 ## patient_class data plots ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 observation &lt;tibble [28 × 16]&gt; &lt;gg&gt; ## 2 outpatient &lt;tibble [65 × 16]&gt; &lt;gg&gt; ## 3 emergency &lt;tibble [124 × 16]&gt; &lt;gg&gt; ## 4 recurring outpatient &lt;tibble [11 × 16]&gt; &lt;gg&gt; ## 5 inpatient &lt;tibble [111 × 16]&gt; &lt;gg&gt; Now we have a column that contains the plots. We can pull these out of the dataframe into a vector with the pull function (because walk works on vectors), then print them to the Plots tab with the walk function. covid %&gt;% nest(data = -patient_class) %&gt;% mutate(plots = map(patient_class, make_covid_plot)) %&gt;% pull(plots) %&gt;% walk(print) Note that we could also walk the ggsave function if we wanted to save these plots as tiffs or pdfs or png files. We can use our more generalizable function make_covid_plot_2 in this pipeline if we use map2, which is like map, but for functions with 2 arguments. The arguments of the map2 function include the two arguments (data, patient_class) for making plots, and then the function (make_covid_plot_2). covid %&gt;% nest(data = -patient_class) %&gt;% mutate(plots = map2(data, patient_class, make_covid_plot_2)) %&gt;% pull(plots) %&gt;% walk(print) This version of the pipeline will automatically process all patient classes in the dataset, whatever they are called, without us having to specify them. 17.2 Your Turn Let’s look at how to do this with the prostate dataset. We will start with a simple violin and jitter plot of time to recurrence by baseline Gleason score (bgs), in the subgroup where rbc_age_group = 1. rbc_age_group = 1 prostate %&gt;% filter(rbc_age_group == 1) %&gt;% ggplot() + aes(x = bgs, y = time_to_recurrence, fill = bgs) + geom_violin() + geom_jitter(width = 0.2) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = &quot;Baseline Gleason Score&quot;, y = &#39;Time to Recurrence&#39;, title = glue(&quot;Preop PSA by baseline Gleason Score for RBC Age: {rbc_age_group}&quot;)) We would like to make this plot for each level of rbc age (rbc_age_group). Let’s walk through the process of how to build this into a function, and how to map it across all the values of rbc_age_group. First, let’s avoid hard-coding the value for rbc_age_group. Use .data$var for a variable in a column, and .env$var for an environment variable. Edit both sides of logic test in the filter statement in the code chunk below to eliminate hard coding the filter variable and the filter value. Then, check that this works for other values by changing the rbc_age_group environmental variable value to 2 or 3. rbc_age_group = 1 prostate %&gt;% filter(rbc_age_group == 1) %&gt;% ggplot() + aes(x = bgs, y = time_to_recurrence, fill = bgs) + geom_violin() + geom_jitter(width = 0.2) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = &quot;Baseline Gleason Score&quot;, y = &#39;Preoperative PSA&#39;, title = glue(&quot;Preop PSA by baseline Gleason Score for RBC Age: {rbc_age_group}&quot;)) Now make this into a function, named make_plot. Edit the chunk below to turn it into a function by wrapping the code in curly braces on the preceding and following line prefacing the first curly brace with function(argument) - in this case, the argument will be rbc_age_group. assigning the function to the name make_plot Then run make_plot(1) rbc_age_group = 1 { prostate %&gt;% filter(.data$rbc_age_group == .env$rbc_age_group) %&gt;% ggplot() + aes(x = bgs, y = time_to_recurrence, fill = bgs) + geom_violin() + geom_jitter(width = 0.2) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = &quot;Baseline Gleason Score&quot;, y = &#39;Preoperative PSA&#39;, title = glue(&quot;Preop PSA by baseline Gleason Score for RBC Age: {rbc_age_group}&quot;)) } Now, let’s automate calling the function, with a vector of the values of rbc_age_group, and running the function over each of these with map(vector, function). Edit the vector and function in the code chunk below to use the map function to create a list of plots. Hint - the vector is provided, and your new function name is make_plot. When it is working, simplify the plots1-3 by replacing them with walk(plots, print) rbc_age_group = 1:3 plots &lt;- map(vector, func) plots[[1]] plots[[2]] plots[[3]] Now let’s make this a more generalized function with 2 arguments - data, and rbc_age_group. Edit the code chunk below to create a new function, make_plot2. Remember to: wrap the code in curly braces on the preceding and following line preface the first curly brace with function(arguments) - in this case, the arguments will be data, rbc_age_group. assigning the function to the name make_plot2 Remove the filter step Replace the filter step with a separate (outside the function) filter step to filter for rbc_age_group ==1, that saves the result to data_prostate_1 Then run make_plot2 rbc_age_group = 1 make_plot2 &lt;- function(arg1, arg2){ prostate %&gt;% filter(.data$rbc_age_group == .env$rbc_age_group) %&gt;% ggplot() + aes(x = bgs, y = time_to_recurrence, fill = bgs) + geom_violin() + geom_jitter(width = 0.2) + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(x = &quot;Baseline Gleason Score&quot;, y = &#39;Preoperative PSA&#39;, title = glue(&quot;Preop PSA by baseline Gleason Score for RBC Age: {rbc_age_group}&quot;)) } data_prostate_1 &lt;- prostate %&gt;% filter(rbc_age_group == 1) make_plot2(data_prostate_1, rbc_age_group = 1) Now let’s try to use make_plot in a tidy pipeline in the chunk below. In the chunk below, you want to nest the data by rbc_age_group, and map the make_plot function over the values of rbc_age_group. Edit both of the var values in the code chunk below to rbc_age_group. Run it to see the nested tibbles and nested plots in your new dataframe. prostate %&gt;% nest(data = -var) %&gt;% mutate(plots = map(var, make_plot)) Now pull out the plots and print them with pull and walk by editing the code chunk below. Pipe this code into the pull and walk functions by adding two lines of code. Remember that the argument for pull is the plotscolumn. Remember that the argument for walk is the print function. prostate %&gt;% nest(data = -rbc_age_group) %&gt;% mutate(plots = map(rbc_age_group, make_plot)) Now let’s make this even more generalizable with the make_plot2 functiion, which has 2 arguments, for data and rbc_age_group. Edit the code chunk below to replace arg1 with the data column, and func with make_plot2. See if this runs prostate %&gt;% nest(data = -rbc_age_group) %&gt;% mutate(plots = map2(arg1, rbc_age_group, func)) %&gt;% pull(plots) %&gt;% walk(print) This version of the pipeline should run for all values of rbc_age_group. 17.3 Freestyle Now try this on your own - and use the outline mode to jump back and forth to previous code chunks to make this easier. Make a plot of age (on the y axis) vs by parity (x axis) in the infert dataset with violin and jitter plots, as in the prostate example prostate-plot-1. This should be filtered for education == “0-5yrs”. Edit to make appropriate axis labels as appropriate. Make a non-hard-coded version, by editing the filter statement with .data and .env variables, and edit the title with glue as in prostate-plot-2. Make this into a function, make_infert_plots, as in prostate-plot-3. Try this with different values for education. Set the levels of education with a vector as in prostate-plot 4, make plots with the map function, and print these out, with the walk function Now make a more generalizable function, make_infert_plots2, by adding a data argument, as in prostate-plot-5. Now nest the data by education, and mutate up some plots with make_infert_plots, as in prostate-plot-6 Then pull these plots and print them out with walk, as in prostate-plot-7 Now make a nested and generalizable version for all values of education, using your make_infert_plots2 function, as in prostate-plot-8 17.4 Read More More on these topics can be found in R for Data Science - Chapter 19: Functions R for Data Science - Chapter 21.5: Map Functions Purrr::map documentation A blog post on how to use purrr:map to make plots for all 50 states and put them into a powerpoint presentation. "],["cmd-line.html", "Chapter 18 Running R from the UNIX Command Line 18.1 What is the UNIX Command line? 18.2 Why run R from the command line? 18.3 How do you get started? 18.4 The Yawning Blackness of the Terminal Window 18.5 Where Are We? 18.6 Cleaning Up 18.7 Other helpful file commands 18.8 What about R? 18.9 What about just a few lines of R? 18.10 Running an R Script from the Terminal 18.11 Rendering an Rmarkdown file from the Terminal", " Chapter 18 Running R from the UNIX Command Line 18.1 What is the UNIX Command line? The command line is a simple Terminal window with a prompt at which you can type commands, And do primitive but powerful things to your files. The UNIX computing environment was developed in the 1960s, and is still beloved and fetishized by brogrammers, who believe you are not truly a programmmer if you can’t code from the command line. This is silly. The major attraction to UNIX in the 1960s is that it was much better than punch cards. Which isn’t saying much. We have had 60 years of software advancement and user interface improvements, so we really should not have to put up with the inherent user hostility of the UNIX environment. UNIX is an early operating system, which is built around a ‘kernel’ which executes operating system commands, and a ‘shell’ which interprets your commands and sends them to the kernel for execution. The most common shell these days is named ‘bash’, which is a silly recursive brogrammer joke. You will sometimes see references to shell scripts or shell or bash programming. These are the same thing as command line programming. UNIX is a common under-the-hood language across many computers today, as the Apple iOS is built on top of UNIX, and the various versions of the LinuxOS are built on a UNIX-like kernel, with a similar command shell. The command line is often the least common denominator between different pieces of open-source software that were not designed to work together. It can occasionally be helpful to build a data pipeline from mismatched parts. However, there is a lot of low-quality user-hostile command line work involved to get it done, often referred to as “command-line bullshittery”. This is a common bottleneck that slows scientific productivity, and there is a vigorous discussion of it on the interwebs here and here (counterpoint). Essentially, some argue that it is largely a waste of time and effort, while others see it as a valuable learning experience, like doing least squares regression by hand with a pencil. Running R from the command line is a bit like spending a day tuning your car’s engine by yourself. There is a case to be made that this will improve the efficiency and performance of your car, but it is also usually more efficient to pay someone else to do it, unless you are a car expert with a lot of free time. 18.2 Why run R from the command line? You can run R from the command line. It has none of the bells and whistles, nor any of the user conveniences of the RStudio Interactive Developer Environment (IDE). But it is how R was originally expected to be used when it was developed back in 2000 in New Zealand. Running R from the command line allows you to do powerful things, like process multiple files at once, which can be handy when you have multiple files of sequencing data from distinct observations, or you have a multistep data wrangling pipeline with several slow steps. For many years, this was the only way to easily apply code across multiple files to build a complex data pipeline. This is much less true today, with tools to handle file paths like the {here} and {fs} packages, run Python scripts from R with the {reticulate} package, run C++ scripts with Rcpp, and run bash, python, SQL, D3, and Stan scripts from Rmarkdown. You can use the {drake} package to manage multi-step data pipelines in different languages (similar to make). But some labs have been doing things at the command line for years, and find it hard to change. 18.3 How do you get started? First, you need to open a terminal window. And to do that, you need to find it. This is akin to getting under the hood of a car, and computer makers don’t exactly encourage it. 18.3.1 On a Mac Go to Finder/Applications/Utilities/Terminal 18.3.2 On a Windows PC Go to Applications/Terminal 18.4 The Yawning Blackness of the Terminal Window So, you have managed to open a terminal window, which has a standard UNIX prompt, ending in something like % or $. Not terribly helpful, is it? The bash shell is waiting for you to enter a command. No user interface for you! Let’s start with a simple one, which can’t do any harm. Run the command below: whoami whoami ## peterhiggins Remember that UNIX started out as an operating system for terminals, and knowing who was logged in was a helpful thing. You can string together two commands with a semicolon between them. Try the following: whoami;date ## peterhiggins ## Thu Jul 22 18:52:52 EDT 2021 OK, fine. This is sort of helpful. It was really important when you were on a terminal and paying by the minute for time on a mainframe back in 1969. And, on occasion, if you will need to use an entire computer cluster to run a script (or scripts) on a lot of data, you will likely have to use some of this command line knowledge. You can even schedule jobs (scripts) to run when your time is scheduled on the cluster with cron and crontab. At this point, it would be helpful to open a window with your Documents folder, and keep it side by side with the window in which you are reading this e-book. We will start working with files and directories, and it is helpful to see changes in your file/folder structure in real time. As we run commands in the bash shell, check them against what you see in the folder window. You may find that some files (dotfiles, starting with a period) are hidden from the user to prevent problems that occur when these are deleted. 18.5 Where Are We? OK, let’s start looking at files and directories. Start with the pwd command, which does not stand for password, but for print working directory. Run the code below in your Terminal window. pwd ## /Users/peterhiggins/Documents/RCode/rmrwr-book You can see the full path to your current directory. This can be a bit obscure if you are just looking at your folder structure, particularly at the beginning of the path. Fortunately, the {here} package handles a lot of this for you when you are working in Rstudio projects. We think of the directory as a tree, with a root - in this case, Users, and various branches as you build out folders and subfolders. We can move up and down the folders of the directory paths with the cd command, for change directory. Try this command in your Terminal Window, and see if you can figure out what it does. cd .. It changes the directory up one level closer to the root directory. It is straightforward to go up the directory tree, as each folder only has one parent. But it is tricky to go down the directory tree, as there are many possible branches/children, and you do not inherently know the names of these branches. We need to list the contents of your current directory with ls to know what is there. Try the ls command in your Terminal window cd /Users/peterhiggins/Documents/; ls ## 1FQ_Crohn&#39;s Disease_23Oct2020 (002).doc ## 2020-Jun-05 AGA IMIBD meeting notest.docx ## 2020_Higgins_ClinResIBD_biosketch.doc ## 2021 AGA Invited Speaker Session Basic Hybrid Example.pdf ## 2021-07-13_Higgins_WH_signed_letter.docx ## 2021.Biobanking Program_InVivo_DRAFT-6.14.2021.docx ## 2021.Biobanking Program_InVivo_PDRH-6.21.docx ## 2021.Higgins AGA Distinguished Clinician.CO.docx ## 203ClareScenes080119 copy.pdf ## A is for Allspice.2.0.docx ## A is for Allspice.docx ## ABT263_HIO_report_toWord.docx ## ACG U-ACHIEVE and U-ACCOMPLISH.docx ## ACLS eCard Peter Higgins.pdf ## AGA DDW 2021 ## AGA IMIBD ## AGA IMIBD Councilor Career Discussion Guide.docx ## AGA IMIBD Webinar Outline.docx ## AIBD CAM Higgins.pdf ## AIBD CAM Higgins.pptx ## AIBD SoMe Higgins.pdf ## AIBD SoMe Higgins.pptx ## AIBD agreement.docx ## AIBD20Template.pptx ## AMAG DDW Clear draft_PDRH comments.docx ## APG1244_Milestone_report.docx ## ARead_RAC-Review_PHiggins.docx ## ASUC_UC_protocol_comments_2020.docx ## A_Woodward_Score Sheet_PDRH.docx ## AbbVie_Contract_2021_K000013379_-_Peter_Higgins.pdf ## AbbVie_Contract_K000013378_-_Peter_Higgins_2021_adhoc.pdf ## Abbvie_DocuSign_Dr_Higgins_Invoice.docx.pdf ## Accounts and Access (1) (1).docx ## Advice for participants in webinars.docx ## Animation of NSAID.pptx ## Applicant Research Design Task T32.docx ## BKochar_Frailty.pdf ## BM recommendation.docx ## Beginners_GuideToR.pdf ## Best Practices Perils Excel ## Biosketch for K.pptx ## Biosketch_2020_Higgins_ClinResIBD_biosketch.doc ## Brazil.ItineraryNov2015.docx ## Butter BCS Chicken.docx ## CAS.K.candidate.background_SB_PDRH.docx ## CAS.T32.Project.Description-JS.docx ## CAS.career.goals.obj.development.training_PDRH.docx ## CB6 and JAK_stat.pptx ## CC360_The Risk of SARS.R1.docx ## CC360_The Risk of SARS.docx ## CCF IBD Webcast 2020 Draft Deck_For Review.pptx ## CCFA EIC Candidate Interview Questions (candidates) jobin[1].doc ## CDC_proposal1.1.docx ## CLARE STOCKS.docx ## COVID Trials Feasibility ## CaltechCampus Tour &amp; Information Session.webarchive ## Cancel Appt Epic.ppt ## Causal.png ## CellDeath_DDW_2021_ISS.pdf ## Chu RPG Review_PDRH.docx ## Clare Investment Summary.docx ## Clinical Coordination and Intense Proactive Monitoring to Improve Utilization of Resources and Reduce Expenditures in High.docx ## Council Conversations Author Chat Guide.docx ## Coursera_Programming in R Notes.docx ## CoverLetterPlus.pptx ## Crash&amp;Burn_ScriptV2_100318 copy.pdf ## DDW JAK for UC.pptx ## DDW2021 CB6 powerpoint.pptx ## DDW2021_CB6_Antifibrotic_Higgins.pdf ## DDW21_JAK_Higgins.pdf ## DataCamp Courses by Topic.docx ## DeEscalationACG2016.pptx ## Demographics.pdf ## Documents.Rproj ## DrHiggins IBD Data Request.xlsx ## Draft Postop IBD Surgery Care Protocols v2_SERedit.docx ## ECCO 2016 Amsterdam Schedule.docx ## ECCO 2019 UC PRO SS Abstract D1f_JP_UA_YO_AM_PDRH.docx ## ECCO2016Lycera30937.pptx ## Effect of medications on the recurrence of cancer in IBD patients.docx ## Electrical engineering interview questions.docx ## FCP Sensor proposal draft.docx ## FDAtofaResponse.docx ## FFMI Kickstart-FinalReport 5-20-16-LJ.docx ## FITBITProtocol_28NOV2016_AbbVie.docx ## FITBITProtocol_4DEC2016_AbbVie.docx ## FMT_DDW_2021_ISS.pdf ## Feasibility and Pilot Studies.pptx ## Feb2021_ibdTrials.pptx ## FellowshipRec_Janson Jacob_Higgins_JB.docx ## FellowshipRec_Janson Jacob_Higgins_JB.pdf ## FibrosisIBDCedars2016.pptx ## Figures-KC-JAMA.pptx ## Finance and Retirement Plans.docx ## Financial Priorities.docx ## GCPcitiCompletionReport8018282.pdf ## Garmin Notes.docx ## General Social Media Tips.docx ## General thoughts about query letters.docx ## Getting Started with REDCap.docx ## Git for MDs_2.pptx ## GitHub ## Github for MDs_1.pptx ## Github for MDs_3.pptx ## Glover_RPG_Review_PDRH.docx ## GoToMeeting Chats ## GradPartyHigginsInvites.xlsx ## HPI-5016 IBD Patient Contact Info.xlsx ## HS movie.docx ## Higgins AGA Webinar Slides.pptx ## Higgins Bio.docx ## Higgins New IBD.pptx ## Higgins Other Support 2021-2.docx ## Higgins Other Support 2021.docx ## Higgins Refractory Proctitis.pptx ## Higgins biosketch2015KRao.doc ## Higgins biosketch2016KRao.doc ## Higgins-peter.jpg ## HigginsACGMidwest2019_PerioperativeIBD.pptx ## Higgins_LOS_IBDBiobank_Shah_Nusrat_2019.docx ## Higgins_UM_CME_Pregnancy in IBD.pptx ## Higginslab server.pptx ## How To Log in to IBD Server.docx ## How To Log in to RStudio Server for HigginsLab.docx ## How To Log in to RStudio Server for Shiny.docx ## IBD 2020 - Honorarium reimbursement Form.docx ## IBD Biobank Cryostor.pptx ## IBD Clinical Trials for MDsDearborn2017.pptx ## IBD Insurance Pilot Results.docx ## IBD Insurance Survey for CCFA Partners Existing.docx ## IBD Journal Club 13Feb2017.docx ## IBD Journal Club July 11.docx ## IBD Plexus meeting 21 Sep 2015 notes.docx ## IBD School 322 Script.docx ## IBD School 324 Script.docx ## IBD School 325 Script.docx ## IBD and biologics tweets.docx ## IBD inbox coverage.docx ## IBDInsuranceSurvey3.docx ## IBDMentoringConferenceCall4AbstractsPH.docx ## IBD_Deescalation_Apr_2019_PDRH.docx ## IBDforLansing2017.pptx ## IMG_0006.jpg ## IMG_0008.jpg ## IMG_1523st.jpg ## IMIBD Councilors 2020-21.docx ## IMIBD Partners insurance 2020DDW.pptx ## IMIBD Plenary Intro.pptx ## IMIBD_expanded_descriptors.xlsx ## Introduction to Application Supplement Photoacoustic.docx ## JAK_DDW_2021_ISS.pdf ## JAMA Review on CD.docx ## JAMA.CD.Highlights_PDRH.docx ## JAMA_KC_Second JAMA.docx ## JAMA_Review_on_CD_Revisions_Tracked_Changes with edits_PDRH.docx ## JB_V1 Career Goals and Objectives 7.8.2020_PDRH.docx ## JB_V2 Candidate’s Background 7.7.2020_PDRH.docx ## JDix_Study_update.docx ## Jessica Sheehan Rec Letter Fellowship.docx ## Jessica Sheehan Rec Letter Fellowship.pdf ## Jun2021_ibdTrials.pptx ## K Award Institutional Letter of Commitment.pptx ## K Candidate Section.pptx ## K105_Melmed_PROs in Practice_MB_bb_JLS.pptx ## K23 Aims - Shirley Cohen-Mekelburg 11.14.19.docx ## K23_morph_measurements_MockupManuscript_21JAN2019.docx ## Learning R discussion Jeremy Louissaint.docx ## Letter to Frank Hamilton.docx ## Library ## Lin_Reviewer Score_PDRH.docx ## Log in to IBD Server.docx ## Low Enrollers ACD.xlsx ## MCTSU QC Time to Activation (002).pptx ## MEI_2020_PH_W9.pdf ## MEI_2021_PH_W9.pdf ## MEI_2021_W9.pdf ## MEI_ACH_Wire Transfer Form.docx ## MIM-TESRIC PROTOCOL_Higgins_14Apr2020.docx ## MIM-TESRIC PROTOCOL_Higgins_26Aug2020.docx ## Managment of CD.pptx ## Manuscript v1.docx ## Manuscript v2.PDRH.docx ## McDonald, Nancy.pdf ## Megan McLeod Rec Letter Residency.docx ## MentoringAgendaDraftPH.docx ## Meta analysis TB vs CD version 3.5.docx ## Michigan Medicine Gastroenterology Social Media Initiative.docx ## Michigan Medicine Model for COVID-19 Clinical Trial Oversight DRAFT (KSB 04.17.20)-AL-PDRH.docx ## Microsoft User Data ## MultidisciplinaryIBDClinicPHv2.docx ## NordicTrackTC9iTreadmillManual.pdf ## Oct2019payPDRH.PDF ## Odd college lists.docx ## P Singh K grant aims 8-25_PDRH.docx ## P2PEP slide 2020 ## P2PEP slide 2020.pptx ## PHcv2019.docx ## PHcv2020.docx ## PRO agenda videos VINDICO.docx ## PRO letter.docx ## PS_K grant aims 6-25_PDRH.docx ## PTM LOS From PDRH.docx ## PTM LOS From PDRH.pdf ## Pearson 5 Notes.docx ## Perils of Excel.pptx ## Personal statement version 3!.docx ## Peter Higgins 2021 Vision Statment for the NSAC.docx ## Peter Higgins_Annual_Review.docx ## Peter_Higgins_photo_headshot.jpg ## Pfizer_Contract_for_Peter_Higgins_-_RD-20-D11.pdf ## Pitch Letter - S is for Saffron.docx ## Pitching Notes.docx ## Poppy Eulogy backup.docx ## Poppy Eulogy.docx ## Possible Eastern College Tour.docx ## Powerpoint ## Prashant Rec Letter.docx ## Prashant Rec Letter.pdf ## PredictingIBD_DDW_2021_ISS.html ## PredictingIBD_DDW_2021_ISS.pdf ## Proposal for MCTSU Study Accrual Monitoring.docx ## Purdue Disclosure Form_Higgins.docx ## Question 16.docx ## Quiros SRA- Higgins LoS draft_PDRH.docx ## Quiros SRA- Higgins LoS draft_PDRH.pdf ## RCode ## Ramp up clinical research_PH.xlsx ## Ramping up human subject research - MM 6-1-20 _KDA_PDRH_suggestions.docx ## Recordings ## Reply_JAMA_Thiopurines.docx ## Review Criteria for COVID Clinical Trials.docx ## Review guidelines_2017.docx ## Roasted Salted Cashews.docx ## S is for Saffron 3.0.docx ## S is for Saffron 3.1.docx ## S is for Saffron 3.2.docx ## S is for Saffron.2.0.docx ## SCM Mentor Letters.docx ## SEAN STOCKS.docx ## SIG_Template_IBD Program_FINAL.docx ## SPECIFIC AIMS 2_PDRH.docx ## Scoring Sheet_Janda_PDRHiggins.docx ## Screenwriting Contests.docx ## Sean Common App academic honors list.docx ## Sean Common App activities list.docx ## Sean Higgins Bordogni.mp4 ## Sean Higgins Brag Sheet.docx ## Sean Investment Summary.docx ## Sean Resume Tabular VBorder.docx ## Sean Resume Tabular.docx ## Sean Resume.docx ## Sean Summer Priorities 2016.docx ## SecureIBD.pptx ## ShareRmd.html ## Sherman Prize Nominee Questions.docx ## Shoreline West Tour Information.docx ## Short PA slides.pptx ## Shotwave thread.docx ## Signing Clinical Research Infusion Orders.pdf ## SingleCell_DDW_2021_ISS.pdf ## SkinCancer.IBD.Gentics_Yanhua_PDRH.docx ## SoMe_use_2020.png ## Social Media for GI.pptx ## Source Code PT1.docx ## Specific Aims.pdf ## Stelara paper.docx ## Stelara paper_revised_PDRH_KCC.docx ## Structure of Aim 3.docx ## T32_current_text_14June2019.docx ## TOPPIC ML draft v5SCM_YL_AKW_PDRH.docx ## TabaCrohn IBD J club.docx ## Tables.docx ## Takeda_IBD School Videos_Submission.pdf ## Task List 2020-2.docx ## Task List 2020-5.docx ## Task List 2020.docx ## Task List 2021.docx ## Testing signatures with Adobe.pdf ## The Risk of SARS.R1.Markup.docx ## Tidymodels.docx ## Timelines for K submission.pptx ## Tofa in ICI Figure Legends_Final Draft_V2.docx ## Tofa inpatient induction Protocol_02NOV2018_PHforEdits.docx ## Tofa_Presentation_2_10_2021.pptx ## Toffee Separation Tips.docx ## UC CD Impact Manuscript Tables__19Feb2021_PDRH.docx ## UC and CD Impact Manuscript_Draft1_19Feb2021_PDRH.docx ## UCRx_DDW_2021_ISS.pdf ## UC_protocol_comments_2020.docx ## UM IBD Clinical Trials IBD referral form.docx ## UPA_U_ACHIEVE 1st draft_PDRH.docx ## Upa ASUC Concept Page.docx ## VINDICO_PRO.pptx ## VideoVisitSchedulingQuickApptsforProviders.pdf ## VincentChen_K specific aims 2020-10-25.docx ## VirtualPtEdMar2020.v2.pdf ## WebEx ## Why not excel.docx ## Zoom ## Zwift ## Zwift-Gift-Card.pdf ## aga institute council july 2020 meeting.pdf ## algorithms_thiopurine.pdf ## base-r-cheatsheet.pdf ## biomakers_fibrosisPDRH.docx ## blue_down_arrow - Gravit Designer.html ## bmj_imputation.pdf ## cgh_factors_utilization.pdf ## cycling core exercises.docx ## draft_tokenization letter Risa_Uste.docx ## early-career-faculty_Dec-2020.xlsx ## epic cancel_reschedule appointments.ppt ## epic schedule viewing_close.ppt ## escalator.html ## exercise1.xlsx ## exercise2.xlsx ## fellow graduation 2020.docx ## hexStickers.jpg ## higgins2x3.jpg ## iBike Rides ## introduce_clare.docx ## jama_cushing_crohn_review_2021.pdf ## learnr app diagram.jpg ## learnr app diagram.pptx ## letter Lowrimore.docx ## mockstudy manuscript draft.docx ## nejm1966_beecher_ethics.pdf ## nejm_indomethacin.pdf ## nejm_statins.pdf ## orange_green_down_arrow - Gravit Designer.html ## orange_green_down_arrow- Gravit Designer.pdf ## pdrh_IBD_email.xlsx ## personal statement fellowship_PDRH.docx ## peterhiggins.jpg ## pink_down_arrow - Gravit Designer.html ## pink_up_arrow - Gravit Designer.pdf ## seq-6.pdf ## signature.docx ## signature.fld ## signature.html ## signature.pdf ## signature.png ## stiff_bcl.R ## submitJanssen_IBD School Videos_12Jul2018.pdf ## tidyr_pivot.png ## tidyr_pivot.xcf ## tofa_checkpoint.pdf ## ucla1.jpg ## untidy_sheets.pptx ## user_testing_learnr tutorials.pptx ## wga_min20.pdf ## zwift_training_pacepartner.xlsx ## {&quot;Attachments&quot;-[{&quot;__type&quot;-&quot;F.textClipping ## ~$T Review Higgins.docx ## ~$sk List 2020-5.docx ## ~$sk List 2020.docx ## ~$sk List 2021.docx You will see a listing of all files and folders in the current directory. You can get more details by adding the option (sometimes called a flag) -l cd /Users/peterhiggins/Documents/; ls -l The full listing will give you more details, including read &amp; write permissions, file size, date last saved, etc. Many commands have options, or flags, that modify what they do. Find a folder inside of your Documents folder. We will now go down a level in the directory tree. In my case, I will use the Powerpoint folder. In your Terminal window: change the directory to the Powerpoint directory list the contents of this folder cd /Users/peterhiggins/Documents/Powerpoint; ls ## 2016IBDClinTrialsforMDsDearborn.pptx ## 2016IntegratedDeckorMDsGB.pptx ## 2019 SCSG GI Symposium IBD SoA - Read-Only.pptx ## Annual Research Career Review 2021PH.pptx ## BE LGD Dearborn 2016.04.12.pptx ## CCF_Clinical_Trials.pptx ## Feasibility and Pilot Studies.pptx ## Getting Started in RStudio.pptx ## Higgins Microbiota for IBD Patient Ed.pptx ## HigginsDec2018AJG_SmokingStatus.pptx ## IBDUpdate.pptx ## Integrated Slide Deck Dearborn 2016.04.12.pptx ## MER Stress Management Dearborn 4-14.pptx ## MichiganMedicine-IBDTemplate.potx ## PDRH RCR 2020.pptx ## PennThioMTX2017Higgins.pptx ## Pregnancy in IBD.pptx ## Regenbogen CRS for GI CME Course2016.pptx ## Senior Slide Show.pptx ## Social Media for GI.pptx ## ThomsonRectalStumpComplicationsIBD2_13.pptx ## UEGweek2020.pptx ## UMHS Talk- Moving Beyond AntiTNF 4-2016 FINAL v2.pptx ## UMich COVID-19 IBD.pptx ## Vertebrate Animals for K.pptx ## VirtualPtEdMar2020.v2.pptx ## Writers Room.pptx ## ibd_meds_surgery_metan.pptx Great! You moved to a new directory and listed it. Now we will get fancy, and make a new directory within this directory with the mkdir command. Try this in your Terminal window: pwd; mkdir new_files; ls You have now made a new directory (folder) within the previous directory, named new_files. Verify this in your Documents folder. You can now change to this directory and list the contents (it should be empty). Try this out in your Terminal Window (note edit the cd command to your own directory path). cd /Users/peterhiggins/Documents/Powerpoint/new_files; ls Note that you can abbreviate the current directory with ., so that you could have also used cd ./new_files You can create a new (empty) file in this directory with the touch command. Sometimes you need to create a new file, then write data to it. Try this out touch file_name; ls You can also create a file with data inside it with the cat &gt; command. Type in the following lines into your Terminal window. When complete, type control-D to be done and return to the Terminal prompt. cat stands for concatenate. cat &gt; file2.txt cat1 cat2 cat3 Now you can list the contents of this file with the cat command below. Give this a try cat file2.txt You can also list the directory of your new_files folder with ls to see the new folder contents. Try this ls Note that you don’t need to use the Terminal to run bash commands. You can do this from an Rmarkdown file. Take a moment to run pwd in your Terminal, to get the current directory. Now open Rstudio, and a new Rmarkdown document. Copy the path to the current directory from the Terminal. Switch back to the Rmarkdown document. Select one of the R code chunks (note the {r} at the top) and delete it. Now click on the Insert dropdown at the top of the document, and insert a Bash chunk. Now add UNIX commands (separated by a semicolon), like cd (paste in path here); pwd; ls; cat file2.txt Then run this chunk. Now you can run terminal commands directly from Rmarkdown! 18.6 Cleaning Up OK, now we are done with the file file2.txt and the directory new_files. Let’s get rid of them with rm (for removing files) and rmdir for removing directories. In order, we will - Make sure we are in the right directory - remove the file with rm file2.txt - go up one level of the directory with cd .. - remove the directory with rmdir new_files Give this a try pwd; rm file2.txt; cd ..; rmdir new_files Verify all of this in your Documents window. This is great. But you can imagine a situation in which you mistakenly rm a file (or directory) that you actually needed. Unlike your usual user interface, when a file is removed at the command line, it is gone. It is not in the trash folder. It is gone. There is something to be said for modern user interfaces, which are built for humans, who occasionally make mistakes. Sometimes we do want files or folders back. 18.7 Other helpful file commands Here are some file commands worth knowing cat filename - to print out whole file to your monitor less filename - to print out the first page of a file, and you can scroll through each page one at a time head filename - print first 10 lines of a file tail filename - print last 10 lines of a file cp file1 file2 - copy file1 to file2 mv file1.txt file.2.txt file3.txt new_folder - move 3 files to a new folder 18.8 What about R? So now you can get around directories, and find your files in the Terminal window, but you really want to run R. You can launch an R session from the Terminal Window (if you have R installed on your computer) by typing the letter R at the Terminal prompt Launch R R You get the usual R intro, including version number, and the R&gt; prompt. Now you can run R in interactive mode with available datasets, or your own datasets. Try a few simple commands with the mtcars dataset. Give the examples below a try. You can use q() to quit back to the terminal (and reply “n” to not save the workplace image). head(mtcars) ## mpg cyl disp hp drat wt qsec vs am ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 ## gear carb ## Mazda RX4 4 4 ## Mazda RX4 Wag 4 4 ## Datsun 710 4 1 ## Hornet 4 Drive 3 1 ## Hornet Sportabout 3 2 ## Valiant 3 1 mtcars %&gt;% filter(mpg &gt; 25) %&gt;% select(starts_with(&#39;m&#39;)|starts_with(&#39;c&#39;)) ## mpg cyl carb ## Fiat 128 32.4 4 1 ## Honda Civic 30.4 4 2 ## Toyota Corolla 33.9 4 1 ## Fiat X1-9 27.3 4 1 ## Porsche 914-2 26.0 4 2 ## Lotus Europa 30.4 4 2 18.9 What about just a few lines of R? Sometimes you will want to call R, run some code, and be done with R. You can call R, run a few lines, and quit in one go. Just add the flag -e (for evaluate) to the call to R, and put the R commands in quotes. Try the example below (note that this will not work if you are still in R - be sure you are back in the terminal with the % or $ prompt) R -e &quot;head(mtcars)&quot; or this example - note that single or double quotes does not matter - as long as they match. Try this R -e &#39;install(palmerpenguins)&#39; You can also string together several commands with the semicolon between them. Try the example below. R -e &#39;library(palmerpenguins);data(penguins);tail(penguins)&#39; 18.10 Running an R Script from the Terminal Now we are stepping up a level - you have an R script that you have carefully created and saved as the myscript.R file. How do you run this from the Terminal? This is easy - just call the Rscript command with your file name. Pick out a short R file you have written, make sure you are in the right directory where the file is, and use it as in the example below. Rscript myscript.R This launches R, runs your script, saves resulting output (if your script includes save or ggsave commands), closes R, and sends you back to the Terminal. Very simple. 18.11 Rendering an Rmarkdown file from the Terminal This is a little different, as you can’t just run an Rmarkdown file. Normally you would use the dropdown button to knit your file from Rstudio. But you can use the rmarkdown::render command to render your files to HTML, PDF, Word, Powerpoint, etc. Pick out a simple Rmd file like output_file.Rmd below, make sure you are in the right directory where the file is, and try something like the example below. Note that this is one case where nesting different types of quotes (single vs. double) can come in handy. It helps to use single quotes around your filename and double quotes around the rmarkown::render command. Try it out Rscript -e &quot;rmarkdown::render(&#39;output_file.Rmd&#39;)&quot; So there you have it! Just enough to get you started with R from the command line. For further reading, check out these helpful links: Data Science at the Command line (e-book) R in Batch mode on Linux R tutorial for a Unix Environment The Linux Command Line: A Complete Introduction - a whole book on the topic Software Carpentry Command-Line Programs Scheduling jobs with cron "],["title-holder.html", "Title holder", " Title holder "],["references-1.html", "References", " References "]]
