---
title: "Logistic Regression"
author: "Peter Higgins"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
library(tidyverse)
library(shiny)
library(medicaldata)
library(performance)
library(broom)
library(cutpointr)
library(janitor)
```

# Logistic Regression and Broom for Tidying Models

Logistic regression allows you to:

1. Estimate the effects of predictors (independent variables) on an dichotomous outcome (dependent variable), like alive/dead, remission/not in remission.

2. Make predictions about future cases (patients) with their measured predictors on this continuous outcome.

Let’s look at a simple logistic model to predict recurrence in prostate cancer.


```{r}
prostate <- medicaldata::blood_storage %>% 
  janitor::clean_names()

prostate %>% 
  glm(formula = recurrence ~ fam_hx + b_gs,
      data = .,
      family = binomial) ->
prostate_model

prostate_model
```

We use the glm() function from the base stats package, which is for *generalized linear model*. This function can use a variety of model families, including logistic, poisson, gamma, quasibinomial, gamma, etc., and the family of models needs to be specified in the family argument.

The formula for the dependent variable (outcome) ~ independent variables (predictors) is the same as with linear modeling with the *lm()* function.

We specify the dataset with the *data* argument, and when we the pipe, we set `data = .`.

When you print out the prostate model, you get the 

- Call (the glm function and arguments) 
- the coefficients for the intercept and each predictor 
- the degrees of freedom 
- how many observations were deleted due to missingness (IMPORTANT, DO NOT BLOW BY THIS),  
- two values for Deviance and 
- AIC.

Let’s walk through what these mean: 

- the **coefficients** estimate how much a change of one unit in each predictor will affect the outcome (in logit units - more about this later).

- The **degrees of freedom** are related to the number of observations, and how many predictors you have used. If you look at the mean value in the prostate dataset for recurrence, it is 0.1708861, which means that 17% of the participants experienced a recurrence of prostate cancer. If you are calculating the mean of 315 of the 316 observations, and you know the overall mean of all 315, you (mathematically) **know** the value of the last observation - recurrence or not - it has no degrees of freedom. So for 316 observations, you have n-1 or 315, degrees of freedom. For each predictor in your model you ‘use up’ one degree of freedom. The degrees of freedom affect the significance of the test statistic (T, or chi-squared, or F statistic).

- how many **observations were deleted due to missingness** - the logistic model will only work on *complete cases*, so if one of your predictors or the outcome is frequently missing, your effective dataset size will shrink rapidly. You want to know if this is an issue, as this might change which predictors you use (avoid frequently missing ones), or lead you to consider imputation of missing values.

- Null Deviance and Residual Deviance. The null deviance is measured for the null model, with only an intercept. The residual deviance is measured for your model with predictors. Your residual deviance **should** be lower than the null deviance. You can even measure whether your model is significantly better than the null model by calculating the difference between the Null Deviance and the Residual Deviance. This difference [281.9 - 246.8 = 35.1] has a chi-square distribution. You can look up the value for chi-square with 2 degrees (because you had 2 predictors) of freedom. Or you can calculate this in R with pchisq(q = 35.1, df=2, lower.tail = TRUE) which gives you a p value of 1.

- The **AIC** is Aikaike’s Information Criterion, which estimates prediction error. A lower values is better when comparing similar models.

## The Model Summary

You can get more information from a summary of the model.

```{r}
summary(prostate_model)
```



Again, you get the call (to orient you to which model). The Deviance Residuals should have a Median near zero, and be roughly symmetric around zero. If the median is close to zero, the model is not biased in one direction (the outcome is not over- nor under-estimated).

Now, in addition to estimates, we get standard errors for each estimate (which can be used to calculate confidence intervals), z statistic values for each predictor, and the resulting p value (calculated with the statistic and the degrees of freedom). As a general rule of thumb, a z value with an absolute value of > 1.96 should have a p value less than 0.05, and an absolute value > 2.576 should have a p value of less than 0.01. These values should sound familiar from the normal distribution (95% and 99% confidence interval Z values).

## Evaluating your Model Assumptions

You can do this in base R with `plot(model)`, but there is a prettier version in the {performance} package in the {easystats} meta-package. Just run the `check_model()` function on your model. You can set the argument panel = TRUE for a multipanel figure, or panel = FALSE for single figures in the Plots tab. Try it both ways to see which you prefer. If the multipanel seems too small, click on the Zoom button in the Plots tab to make it bigger.

```{r}
check_model(prostate_model, panel = FALSE)
```

This generates graphs with nice subtitles to help you interpret the output. Big deviations should make you worry about one or more of the model assumptions, and may require rescaling one of your predictors.

If all is well, you want to look at how your model predictors actually predict the outcome. You make a nicer looking regression table with the tidy() function from the {broom} package.

```{r}
prostate_model %>% 
      broom::tidy()
```

This model has 3 terms: an intercept, and two predictors. The family history predictor (fam_hx) is not significant, but trends toward an association with a decreased odds of recurrence, while the baseline Gleason score (b_gs) is significant and is associated with an 18% increased log-odds of recurrence for each extra point in the Gleason score. Note that this is expressed in logit, or log-odds, not probability, which can take some finagling to get to percentages and probabilities.

A positive estimate indicates that increasing that predictor will be associated with increasing odds of the outcome. Conversely, a negative estimate indicates that increasing that predictor will be associated with decreasing odds of the outcome.

The quantity log[p/(1-p)] is called the logarithm of the odds, also known as the log-odds or logit. Despite this being commonly written as “log”, it is not base 10 logarithms, but the natural log, with the base e (2.718…). Why would anyone use logit? It is really hard to model zeroes and ones (dichotomous outcomes). The logit is a link function, or a way to convert zeroes and ones to a continuous function that does not cross zero or one. Once you have a continuous function, you can use generalized linear models to model it. Why not just model probability? For complicated mathematical reasons, it was easier to convert probabilities to odds and then take the natural log. There will be times when using logistic regression that it will be fairly painful to convert between probabilities and odds and logit units. But R has functions to do that for us. We just have to **watch out** for when we have probabilities vs. odds vs. logit.

One way is to look at the range of the estimates. Probabilities always have a range from zero to 1. Logit units generally range from about -4 to +4, with zero meaning an equal probability of no event or the event outcome occurring. Odds ratios can range from very small (but positive) numbers to very large positive numbers.

You can see this by re-running broom::tidy() with the exp = TRUE option. This will exponentiate the logit, or log-odds estimates, to give us the estimates as odds ratios.

```{r}
prostate_model %>% 
      broom::tidy(exp = TRUE)
```

Now the estimates are all positive, with the *previously* most negative (intercept was logit -3.45) now being a small positive number of 0.0318, and the most positive (b_gs was logit 1.18) now a larger positive odds ratio of 3.27.

These odds ratios versions of the estimates are more easily interpretable than logit scores. Odds ratios of less than one means that an increase in that predictor makes the outcome **less** likely to occur, and an odds ratio of greater than one means that an increase in that predictor makes the outcome **more** likely to occur. For example, the odds ratio estimate of 0.407 means that for someone with a positive family history of prostate cancer, the odds of their having a recurrence are 59.3% ((1-0.407) x 100) lower than someone without a family history. Similarly, for each unit increase in the baseline Gleason score (b_gs), the odds of recurrence increase by 227% ((3.27-1) x 100).

:::tryit
## Converting between logit, odds ratios, and probability

Let’s step back and try a real-world example. If the percent probability of snow on january 10th is 72%, then p, the probability is 0.72. The probability of no snow (1-p) is 1-0.72 = 0.28. The odds of snow are p/(1-p) = 0.72/0.28 = 2.57. If we take the natural log of these odds, this gives us an estimate in logit terms, with ln(2.57) = 0.944. This estimate in logit units is what you get as a default estimate from logistic regression.

To convert these logit estimates back to probability, you need to do the reverse. First, exponentiate the logit estimate of 0.944, exp(0.944) = 2.57. This is the odds ratio. To convert odds to probability - calculate odds / (1 + odds) = 2.57/3.57 = 0.72, which is the probability. To get the percent probability, you can multiply the probability by 100 to get 72%.

Fortunately, R has functions to help us do this sort of conversion. You just have to be able to recognize which units (logit, odds, or probability) that you are looking at.
:::

We can look at the overall quality of the model with the glance() function in {broom}. Let’s look at 2 versions of the model, one with **fam_hx** only, and one with both predictors.

```{r}
prostate %>% 
  glm(recurrence ~ fam_hx,
      data = .,
      family = binomial) %>% 
      broom::glance()

prostate %>% 
  glm(recurrence ~ fam_hx + b_gs,
      data = .,
      family = binomial) %>% 
      broom::glance()
```

You can see that adding the baseline Gleason score improves the model, as it lowers both AIC and BIC. This is not surprising, as it was a significant predictor.

You can add predicted (fitted) values and residuals for each observation in your dataset with `broom::augment()`

```{r}
model <- prostate %>% 
  glm(recurrence ~ fam_hx + b_gs,
      data = .,
      family = binomial) 
augment(model)
```

Note that the fitted data are both positive and negative, with a range within +/- 4. This should tell you that they are in logit (log-odds) units (ln(p/1-p)), in which 0 is a 50% probability of either outcome.

We can do a variety of other things with this model in base R. Let’s look at a few.

If you just want the coefficients and a 95% confidence interval for each (in logit units), you can use the coef() and the confint() functions.

```{r}
coef(model) # estimated coefficients for predictors
```

```{r}
confint(model) # 95% confidence interval for coefficients
```

If you would prefer these as odds ratios, you can exponentiate these.

```{r}
exp(coef(model)) # show the odds ratios, rather than log-odds for coefficients
```

```{r}
exp(confint(model)) # 95% confidence interval for coefficients, now expressed as odds ratios
```

Notice that these are all now greater than zero, rather than from -4 to 4.

You can also use functions to make predictions (fitted) from your data, or even from new data. The default output is in logit units, but if you use the “response” type argument in the predict() function, you get probabilities on a zero to 1 scale.

```{r}
predict(model) %>% 
  head() # predictions for each observation 
# in the prostate dataset on the logit scale 
# from around -4 (very unlikely to have event) 
# to +4 (very likely to have event)
```

```{r}
predict(model, type = "response") %>% 
  head() # predictions for each observation in 
# the prostate dataset using 0-1 probabilities 
# (can multiply by 100 to get percent probability 
# if you prefer)
```

You can then classify these probabilities as likely (>0.5) or unlikely (<=0.5) and compare these class predictions to the true recurrence outcomes. By calculating the mean of the observations that match, you can calculate an overall accuracy of your classification model.

```{r}
probabilities <- predict(model, type = "response") 
predicted.classes <- ifelse(probabilities > 0.5, 0, 1)
# predictions as pos or neg

mean(predicted.classes == prostate$recurrence) 
# calcuated accuracy
```

You can even make predictions on new data (in this case, a random 3% sample of the original dataset)

```{r}
predict(model, newdata = slice_sample(prostate, prop = 0.03), type = "response") 
```

Let’s see how this works with another dataset, from which we will use predictors to classify diabetes cases. We will start by loading the data into dm_data, and building an “all predictors” model, by specifying the formula predictors as “.” - this means to use all other variables (except the outcome variable) as predictors. Look at the model output for problems.

```{r}
data("PimaIndiansDiabetes2", package = "mlbench")
dm_data <- PimaIndiansDiabetes2
rm(PimaIndiansDiabetes2)

# build model, with all variables
dm_mod <- glm(diabetes ~ ., 
              data = dm_data, 
              family = "binomial")
dm_mod
```

Did you notice that 376 observations were deleted due to missingness? We can use the vis_miss() function from the {visdat} package to figure out which are the problem variables.

```{r}
visdat::vis_miss(dm_data)
```

It looks like triceps and insulin measurements were missing fairly often. Would a model without these measures be better? Let’s try.

```{r}
dm_mod_miss <- glm(diabetes ~ glucose + pressure + mass + pedigree + age, 
              data = dm_data, 
              family = "binomial")
dm_mod_miss
```

Apparently not. Even with all the missing data, the AIC of the reduced model is 697.7, and the AIC of the full model was 362. This suggests that insulin and triceps measurements are pretty helpful in predicting diabetes.

Let’s look at how well the full model works, with our usual battery of model functions.

```{r}
# output
summary(dm_mod)
```

```{r}
# test model assumptions
check_model(dm_mod)
```

```{r}
# tidy version of estimates
tidy(dm_mod)
```

```{r}
# model performance
glance(dm_mod)
```

OK, let’s make some predictions, and convert these to percent probability (0-100 range).

```{r}
# augment data with fitted predictions and residuals
dm_data_plus <- augment(dm_mod) %>% 
  mutate(pct_prob = 100 * plogis(.fitted)) %>% 
  relocate(diabetes, .fitted, pct_prob) %>% 
  arrange(-.fitted)
dm_data_plus 
# arrange puts the high-probability cases first
```

```{r}
set.seed(1234)
dm_data_plus %>% 
  slice_sample(n=10) %>%  
  select(diabetes:.rownames) 
# a random sample of 10
```

You can see the relationship between the fitted values (on the logit scale) and the percent probability of diabetes. Some of the high-probability folks are still negative (rowname 609), while others with lower predicted probability alread have diabetes (rowname 486).

A fancier way to classify based on your predictions is to pick an optimal cutpoint, with the {cutpointr} package. You can do this with a number of different metrics you can choose, and different methods. The code chunk below demonstrates some of the helpful output from {cutpointr}. There are 15 different methods, and 20 differnt metric options to choose from, at this website.

Here we start with `maximize_metric` and `sum_sens_spec`.

```{r}
# select a cut point for classification
cp <- dm_data_plus %>% 
  cutpointr(pct_prob, diabetes,
            pos_class = "pos",
            method= maximize_metric,
            metric = sum_sens_spec)
cp
```

```{r}
summary(cp)
```

```{r}
plot(cp)
```

```{r}
plot_metric(cp)
```

You can then use the cutpoint to classify observations, and see how accurate your model is.

```{r}
# classify based on cut point
dm_data_plus <- dm_data_plus %>% 
  mutate(pred_dm = 
  case_when(pct_prob > cp$optimal_cutpoint ~ "pred_yes", 
  pct_prob <= cp$optimal_cutpoint ~ "pred_no")) %>% 
  mutate(pred_dm = factor(pred_dm,
    levels = c("pred_no", "pred_yes"))) %>% 
  relocate(pred_dm, .after = pct_prob)

# check confusion matrix
dm_data_plus %>% 
  tabyl(diabetes, pred_dm) %>% 
  adorn_totals("both") %>% 
  adorn_percentages() %>% 
  adorn_pct_formatting()
```

You can also check model assumptions, and model performance, even against competing models. Let’s build some competing models below.

```{r}
#check model assumptions
performance::check_model(dm_mod, panel = FALSE)
```


```{r}
# use panel = TRUE in Rmarkdown 
# to get 2x3 panels for 6 plots
# 
performance::model_performance(dm_mod)
```

```{r}
#try a simpler model
dm_mod2 <- glm(diabetes ~ glucose + mass + pedigree, 
              data = dm_data, 
              family = "binomial")
tidy(dm_mod2)
```

```{r}
glance(dm_mod2)
```

```{r}
# build a really simple (NULL) model as a baseline

dm_mod3 <- glm(diabetes ~ 1,
                 data = dm_data, 
              family = "binomial")

summary(dm_mod3)
```

```{r}
# compare models

# compare_performance(dm_mod, dm_mod2, dm_mod3, rank = TRUE)

# plot(compare_performance(dm_mod, dm_mod2, dm_mod3, rank = TRUE)) + labs(subtitle = "Larger Area is Better")

# plot(compare_performance(dm_mod, dm_mod2, rank = TRUE)) + labs(subtitle = "Larger Area is Better")


# save model to RDS for later use in predictions or web apps.
saveRDS(dm_mod, "dm_mod.RDS")
```


27.1 Choosing predictors for multivariate modeling – testing, dealing with collinearity

interactions

27.1.1 Challenges

27.2 presenting model results with RMarkdown

27.2.1 Challenges

27.3 presenting model results with a Shiny App

27.3.1 Challenges

